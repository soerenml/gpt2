{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1 - Linear layers\n",
    "\n",
    "A **linear layer** (or **fully connected layer**) in a neural network is a layer where each input neuron is connected to each output neuron with a certain weight. This layer performs a linear transformation on the input vector.\n",
    "\n",
    "Given an input vector $\\mathbf{x} \\in \\mathbb{R}^n$ and a weight matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, and a bias vector $\\mathbf{b} \\in \\mathbb{R}^m$, the output $\\mathbf{y}$ of a linear layer can be computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W}$: $\\begin{bmatrix} w_{11} & w_{12} & \\cdots & w_{1n} \\\\ w_{21} & w_{22} & \\cdots & w_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{m1} & w_{m2} & \\cdots & w_{mn} \\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](/Users/soeren/code/gpt2/images/q_k_v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create the following:\n",
    "\n",
    "`nn.Linear(3, 4)`\n",
    "\n",
    "We get a weight matrix according to the dimensionality of of the output neurons $y$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W} = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23} \\\\\n",
    "w_{31} & w_{32} & w_{33} \\\\\n",
    "w_{41} & w_{42} & w_{43}\n",
    "\\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['linear.weight', 'linear.bias'])\n",
      "tensor([[ 0.1082, -0.1936, -0.4974],\n",
      "        [-0.1222,  0.1985,  0.0237],\n",
      "        [-0.1828, -0.2036,  0.5703],\n",
      "        [-0.4793,  0.0586, -0.4492],\n",
      "        [ 0.5626,  0.5330, -0.3480],\n",
      "        [-0.1288,  0.1013,  0.1506],\n",
      "        [-0.2883, -0.3445,  0.0686],\n",
      "        [ 0.1738,  0.3542,  0.4543],\n",
      "        [ 0.5254, -0.1385, -0.2611]])\n"
     ]
    }
   ],
   "source": [
    "class MyLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    # Define flow\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input_size = 3\n",
    "output_size = 9\n",
    "\n",
    "x = torch.randn(1, input_size)  # Random input tensor\n",
    "\n",
    "model = MyLinearLayer(input_size, output_size)\n",
    "output = model(x)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Print of the state_dict\n",
    "print(state_dict.keys())\n",
    "\n",
    "# Access weights of a specific layer\n",
    "print(state_dict['linear.weight'])\n",
    "\n",
    "# When we look at the weight matrix, it's had exactly the dimension we expectec based on the math above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2 - Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), \n",
      "\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      " tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 10  # Example block size\n",
    "\n",
    "ones_matrix = torch.ones(block_size, block_size)\n",
    "lower_triangular_matrix = torch.tril(ones_matrix)\n",
    "reshaped_tensor = lower_triangular_matrix.view(1, 1, block_size, block_size)\n",
    "\n",
    "print(f\"\"\"{ones_matrix}, \\n\\n {lower_triangular_matrix} \\n\\n {reshaped_tensor}\"\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E3 - Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rand tensor \n",
      "\n",
      " tensor([[[-1.7575,  0.7193, -1.9861,  0.5738,  0.5189],\n",
      "         [ 0.9489, -2.5567, -0.5070,  0.5043, -0.2296],\n",
      "         [ 0.6315,  2.3076, -0.7179,  1.4599, -1.1427]],\n",
      "\n",
      "        [[ 1.7366, -1.2806,  0.4704, -0.5723, -0.7280],\n",
      "         [ 0.8246,  1.1727,  1.3063, -0.5635, -1.9792],\n",
      "         [-0.2773,  0.5857, -1.4455, -0.7405,  0.9141]],\n",
      "\n",
      "        [[ 0.1281,  0.0568, -0.7959, -0.4782,  0.7437],\n",
      "         [ 1.0682, -0.3148, -0.1492, -0.8080,  1.0871],\n",
      "         [-0.5311,  0.0943,  0.4168,  0.4737,  0.0230]],\n",
      "\n",
      "        [[-0.4430,  0.3532, -0.9002, -0.1892,  1.3197],\n",
      "         [-1.0167,  1.1315, -0.9219,  1.6219, -0.6803],\n",
      "         [ 1.8157,  0.6416, -0.0551,  0.8636, -1.8395]]])\n",
      "\n",
      "\n",
      "Weights linear layer \n",
      "\n",
      " Parameter containing:\n",
      "tensor([[-2.0065e-01, -1.9127e-01,  1.9535e-01, -2.4680e-01,  4.0346e-01],\n",
      "        [-1.4478e-01,  4.0237e-01,  4.8295e-02, -2.1675e-01,  3.5998e-02],\n",
      "        [-1.4243e-02,  3.1968e-01, -3.2070e-01,  3.4785e-01,  1.8699e-01],\n",
      "        [-9.4957e-02,  3.2350e-01,  4.3050e-02,  1.5257e-01, -2.9602e-01],\n",
      "        [-6.1804e-02, -8.3925e-02,  1.2471e-01, -3.7791e-01,  5.6723e-02],\n",
      "        [ 2.4555e-01, -9.2420e-03, -1.8976e-01,  2.9485e-01,  4.1852e-01],\n",
      "        [-3.1911e-01,  4.5744e-02, -2.3758e-01,  3.5253e-01, -4.1518e-01],\n",
      "        [-4.1320e-01, -4.0778e-04, -1.0118e-01, -2.5499e-02,  4.5685e-02],\n",
      "        [-2.1863e-01, -2.3227e-01, -6.5249e-02,  3.1389e-01, -2.9858e-01],\n",
      "        [-2.9611e-01, -2.5652e-01, -1.8437e-01,  3.2825e-02, -2.3559e-01],\n",
      "        [-3.4752e-01, -4.0469e-01, -3.5141e-01, -1.2452e-01,  3.2983e-01],\n",
      "        [ 2.5620e-01,  3.1032e-01, -2.1043e-02, -1.8046e-01, -2.6030e-01],\n",
      "        [-2.9847e-01, -5.3834e-02, -1.7199e-01,  1.4803e-01,  3.2088e-01],\n",
      "        [-5.6645e-02,  1.4475e-01, -2.4522e-01, -6.4961e-02,  3.9717e-01],\n",
      "        [ 3.3122e-01, -1.7460e-02, -2.9791e-01,  2.6763e-01,  3.7144e-01]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "qkv tensor \n",
      "\n",
      " tensor([[[-2.3078e-01,  2.4920e-01,  1.6170e+00,  5.5065e-01, -8.3204e-01,\n",
      "           5.9702e-01,  7.4256e-01,  7.7295e-01,  2.3251e-01,  4.1389e-01,\n",
      "           1.4611e+00, -7.6370e-01,  1.3432e+00,  4.1449e-01,  5.3934e-01],\n",
      "         [-1.4309e-01, -1.4012e+00, -1.0733e-01, -4.9145e-01, -5.5607e-01,\n",
      "           6.7736e-01, -3.3609e-01, -5.2605e-01,  5.0689e-01,  3.5425e-01,\n",
      "           1.0883e+00, -9.1066e-01,  2.0684e-01, -8.6853e-01,  7.5560e-01],\n",
      "         [-1.6553e+00,  3.5177e-01,  1.6815e+00,  1.5193e+00, -1.3839e+00,\n",
      "           4.9412e-01,  7.5379e-01, -4.4165e-01,  3.2810e-02, -5.1422e-01,\n",
      "          -1.1160e+00,  5.8718e-01, -7.5558e-02, -5.1945e-01,  5.4499e-01]],\n",
      "\n",
      "        [[-2.8969e-01, -7.3919e-01, -4.9175e-01, -1.2807e-01, -2.1139e-01,\n",
      "           1.4750e-01, -9.3390e-01, -9.4626e-01, -2.1460e-01, -3.0448e-01,\n",
      "          -7.5683e-02, -9.4005e-03, -5.8437e-01, -1.0961e+00,  2.2978e-01],\n",
      "         [-9.1966e-01,  3.7340e-01, -1.9342e-01,  1.1599e+00, -3.3098e-01,\n",
      "          -7.7876e-01, -2.0668e-01, -7.1240e-01, -2.6324e-01, -5.2282e-01,\n",
      "          -1.4591e+00,  8.2477e-01, -9.8817e-01, -1.3918e+00, -8.2651e-01],\n",
      "         [ 8.7197e-02,  3.0634e-01,  9.9657e-01,  7.2647e-02, -3.2579e-01,\n",
      "           6.3702e-01, -4.9174e-01,  1.5826e-01, -6.2587e-01, -2.2605e-01,\n",
      "           1.1048e+00, -3.0301e-01,  7.4782e-01,  4.2106e-01,  6.6593e-01]],\n",
      "\n",
      "        [[ 1.0044e-01,  3.2152e-03,  6.7274e-01, -1.8531e-02, -3.3423e-01,\n",
      "           6.2418e-01, -6.3642e-01, -8.9222e-02, -5.0082e-01, -2.8141e-01,\n",
      "           8.6080e-01, -3.7993e-01,  5.2771e-01,  7.7515e-02,  6.2279e-01],\n",
      "         [ 3.2914e-01, -1.6734e-01,  2.8266e-01, -3.5214e-01, -1.3639e-01,\n",
      "           7.8222e-01, -1.3659e+00, -5.1886e-01, -8.6829e-01, -6.7541e-01,\n",
      "           6.1155e-01, -2.9785e-01,  2.1725e-01, -3.0306e-02,  7.8729e-01],\n",
      "         [-6.3327e-02, -5.9950e-02,  5.0159e-01,  4.6701e-01, -5.4604e-01,\n",
      "           2.1090e-01, -7.7664e-02,  3.2090e-03,  6.9423e-02, -1.1842e-01,\n",
      "           2.9222e-01, -5.4683e-01,  4.2349e-01, -5.2519e-01,  2.9593e-02]],\n",
      "\n",
      "        [[ 2.9903e-01,  1.5822e-01,  1.0173e+00,  6.6900e-04, -4.1335e-01,\n",
      "           8.2730e-01, -5.5313e-01,  1.7611e-01, -5.1930e-01, -2.9534e-01,\n",
      "           1.1299e+00, -6.3413e-01,  9.2774e-01,  3.8835e-01,  7.5085e-01],\n",
      "         [-9.9289e-01,  8.8861e-02,  1.5373e+00,  1.1744e+00, -1.2438e+00,\n",
      "           3.8029e-01,  1.1396e+00,  2.7751e-01,  5.9242e-01,  2.0952e-01,\n",
      "           1.3679e-01, -3.4536e-01,  6.8717e-01, -3.7315e-01,  2.9552e-01],\n",
      "         [-1.5787e+00, -3.5388e-01,  5.8180e-01,  1.0117e+00, -1.0488e+00,\n",
      "           2.0710e-01,  2.2129e-01, -1.0140e+00,  1.3851e-01, -4.1510e-01,\n",
      "          -1.2418e+00,  6.4862e-01, -7.6518e-01, -1.2282e+00,  3.5046e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # Number of examples in the batch\n",
    "sequence_length = 3  # Length of the sequence for each example (e.g. \"I love dogs\"), assuming \"I love dogs\" would result in three tokens: \"I\", \"love\", \"dogs\"\n",
    "embedding_dimension = 5  # Dimension of the embedding space\n",
    "\n",
    "# Create a sample tensor with random values\n",
    "rand_tensor = torch.randn(batch_size, sequence_length, embedding_dimension)\n",
    "print(\"\\n\\nRand tensor \\n\\n\", rand_tensor)\n",
    "\n",
    "B, T, C = rand_tensor.size()\n",
    "\n",
    "# In a second step we are going to use a linear layer with three times the embedding dimension.\n",
    "lin_layer = nn.Linear(embedding_dimension, 3 * embedding_dimension)\n",
    "qkv = lin_layer(rand_tensor)\n",
    "print(\"\\n\\nWeights linear layer \\n\\n\", lin_layer.weight)\n",
    "\n",
    "print(\"\\n\\nqkv tensor \\n\\n\", qkv)\n",
    "q, k, v = qkv.split(embedding_dimension, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5970,  0.7426,  0.7730,  0.2325,  0.4139],\n",
       "          [ 0.6774, -0.3361, -0.5261,  0.5069,  0.3542],\n",
       "          [ 0.4941,  0.7538, -0.4417,  0.0328, -0.5142]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1475, -0.9339, -0.9463, -0.2146, -0.3045],\n",
       "          [-0.7788, -0.2067, -0.7124, -0.2632, -0.5228],\n",
       "          [ 0.6370, -0.4917,  0.1583, -0.6259, -0.2261]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6242, -0.6364, -0.0892, -0.5008, -0.2814],\n",
       "          [ 0.7822, -1.3659, -0.5189, -0.8683, -0.6754],\n",
       "          [ 0.2109, -0.0777,  0.0032,  0.0694, -0.1184]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8273, -0.5531,  0.1761, -0.5193, -0.2953],\n",
       "          [ 0.3803,  1.1396,  0.2775,  0.5924,  0.2095],\n",
       "          [ 0.2071,  0.2213, -1.0140,  0.1385, -0.4151]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_head = 1\n",
    "# B = batch\n",
    "# T = sequence length\n",
    "# head = number of heads\n",
    "# C = embedding dimension --> The tensor k is being split into head smaller chunks. This is a common step in multi-head attention, where the input is projected into multiple subspaces corresponding to different heads.\n",
    "k = k.view(B, T, n_head, C // n_head).transpose(1, 2) # transpose starts from zero. Hence, we are transposing T and head.\n",
    "q = q.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#E4 - Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3, 5]) torch.Size([4, 1, 3, 5]) torch.Size([4, 1, 3, 5])\n",
      "tensor([[[[-2.3078e-01,  2.4920e-01,  1.6170e+00,  5.5065e-01, -8.3204e-01],\n",
      "          [-1.4309e-01, -1.4012e+00, -1.0733e-01, -4.9145e-01, -5.5607e-01],\n",
      "          [-1.6553e+00,  3.5177e-01,  1.6815e+00,  1.5193e+00, -1.3839e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.8969e-01, -7.3919e-01, -4.9175e-01, -1.2807e-01, -2.1139e-01],\n",
      "          [-9.1966e-01,  3.7340e-01, -1.9342e-01,  1.1599e+00, -3.3098e-01],\n",
      "          [ 8.7197e-02,  3.0634e-01,  9.9657e-01,  7.2647e-02, -3.2579e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0044e-01,  3.2152e-03,  6.7274e-01, -1.8531e-02, -3.3423e-01],\n",
      "          [ 3.2914e-01, -1.6734e-01,  2.8266e-01, -3.5214e-01, -1.3639e-01],\n",
      "          [-6.3327e-02, -5.9950e-02,  5.0159e-01,  4.6701e-01, -5.4604e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9903e-01,  1.5822e-01,  1.0173e+00,  6.6900e-04, -4.1335e-01],\n",
      "          [-9.9289e-01,  8.8861e-02,  1.5373e+00,  1.1744e+00, -1.2438e+00],\n",
      "          [-1.5787e+00, -3.5388e-01,  5.8180e-01,  1.0117e+00, -1.0488e+00]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "tensor([[[[ 0.5970,  0.6774,  0.4941],\n",
      "          [ 0.7426, -0.3361,  0.7538],\n",
      "          [ 0.7730, -0.5261, -0.4417],\n",
      "          [ 0.2325,  0.5069,  0.0328],\n",
      "          [ 0.4139,  0.3542, -0.5142]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1475, -0.7788,  0.6370],\n",
      "          [-0.9339, -0.2067, -0.4917],\n",
      "          [-0.9463, -0.7124,  0.1583],\n",
      "          [-0.2146, -0.2632, -0.6259],\n",
      "          [-0.3045, -0.5228, -0.2261]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6242,  0.7822,  0.2109],\n",
      "          [-0.6364, -1.3659, -0.0777],\n",
      "          [-0.0892, -0.5189,  0.0032],\n",
      "          [-0.5008, -0.8683,  0.0694],\n",
      "          [-0.2814, -0.6754, -0.1184]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8273,  0.3803,  0.2071],\n",
      "          [-0.5531,  1.1396,  0.2213],\n",
      "          [ 0.1761,  0.2775, -1.0140],\n",
      "          [-0.5193,  0.5924,  0.1385],\n",
      "          [-0.2953,  0.2095, -0.4151]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the tensors: batch, sequence length, number of heads, embedding dimension (i.e. the embedding)\n",
    "print(q.shape, k.shape, v.shape)\n",
    "\n",
    "\n",
    "print(q)\n",
    "print(\"\\n\\n\")\n",
    "print(k.transpose(-2, -1))\n",
    "\n",
    "# What happens is that the transformation allows us to multiply the q and k tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# The @ operator is used for matrix multiplication.\n",
    "# k.size(-1) is the length of the embeddings.\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
