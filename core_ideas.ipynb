{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1 - Linear layers\n",
    "\n",
    "A **linear layer** (or **fully connected layer**) in a neural network is a layer where each input neuron is connected to each output neuron with a certain weight. This layer performs a linear transformation on the input vector.\n",
    "\n",
    "Given an input vector $\\mathbf{x} \\in \\mathbb{R}^n$ and a weight matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, and a bias vector $\\mathbf{b} \\in \\mathbb{R}^m$, the output $\\mathbf{y}$ of a linear layer can be computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W}$: $\\begin{bmatrix} w_{11} & w_{12} & \\cdots & w_{1n} \\\\ w_{21} & w_{22} & \\cdots & w_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{m1} & w_{m2} & \\cdots & w_{mn} \\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/q_k_v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create the following:\n",
    "\n",
    "`nn.Linear(3, 4)`\n",
    "\n",
    "We get a weight matrix according to the dimensionality of of the output neurons $y$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W} = \\begin{bmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32} \\\\\n",
    "w_{13} & w_{23} & w_{33} \\\\\n",
    "w_{14} & w_{24} & w_{34}\n",
    "\\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['linear.weight', 'linear.bias'])\n",
      "tensor([[-0.3509, -0.1814, -0.2050],\n",
      "        [ 0.3712,  0.2231,  0.5085],\n",
      "        [-0.5417,  0.5079, -0.1018],\n",
      "        [ 0.5199,  0.3205, -0.4828],\n",
      "        [-0.0970,  0.5154,  0.3764],\n",
      "        [ 0.3046,  0.5113, -0.1598],\n",
      "        [-0.4367, -0.1647,  0.3665],\n",
      "        [-0.1620,  0.2772,  0.1710],\n",
      "        [-0.1128, -0.3102, -0.1087]])\n"
     ]
    }
   ],
   "source": [
    "class MyLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    # Define flow\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input_size = 3\n",
    "output_size = 9\n",
    "\n",
    "x = torch.randn(1, input_size)  # Random input tensor\n",
    "\n",
    "model = MyLinearLayer(input_size, output_size)\n",
    "output = model(x)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Print of the state_dict\n",
    "print(state_dict.keys())\n",
    "\n",
    "# Access weights of a specific layer\n",
    "print(state_dict['linear.weight'])\n",
    "\n",
    "# When we look at the weight matrix, it's had exactly the dimension we expectec based on the math above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2 - Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), \n",
      "\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      " tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 10  # The maximum length of the input sequence\n",
    "\n",
    "ones_matrix = torch.ones(block_size, block_size)\n",
    "lower_triangular_matrix = torch.tril(ones_matrix)\n",
    "mask = lower_triangular_matrix.view(1, 1, block_size, block_size)\n",
    "\n",
    "print(f\"\"\"{ones_matrix}, \\n\\n {lower_triangular_matrix} \\n\\n {mask}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E3 - Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input tensor \n",
      "\n",
      " tensor([[[ 0.8794,  0.0162, -0.0192,  0.2003, -0.7611],\n",
      "         [ 1.0562, -0.5795, -0.5137,  1.1017,  1.6426],\n",
      "         [ 0.6179, -0.3312, -0.2809,  1.2661, -0.7663]],\n",
      "\n",
      "        [[ 2.1343,  0.2184,  0.4019,  0.7011, -0.7549],\n",
      "         [ 0.0954, -0.9134, -0.2705,  0.0145,  1.8395],\n",
      "         [ 0.1218, -0.3264,  0.8258, -0.9379,  1.2374]],\n",
      "\n",
      "        [[-0.8918, -0.0756,  0.2246, -0.9340,  0.6094],\n",
      "         [ 0.6002, -1.3578,  0.9987, -0.9037,  0.3855],\n",
      "         [-0.4717, -1.1441,  0.0813,  0.0183,  0.6153]],\n",
      "\n",
      "        [[ 1.0018,  2.1894, -0.4847,  0.9124,  1.0803],\n",
      "         [-1.0512,  0.0516,  0.9709, -0.0821, -0.7628],\n",
      "         [-1.0259, -1.8742, -0.3618, -0.0867, -1.1570]]])\n",
      "\n",
      "\n",
      "Weights linear layer \n",
      "\n",
      " Parameter containing:\n",
      "tensor([[ 0.1323, -0.4311, -0.3484, -0.3857, -0.2718],\n",
      "        [-0.2823, -0.4313, -0.2143,  0.0630, -0.2421],\n",
      "        [ 0.0306, -0.1347,  0.4031,  0.0497,  0.1724],\n",
      "        [ 0.0738,  0.0929, -0.2537, -0.3381,  0.0239],\n",
      "        [-0.2294, -0.3157,  0.1039, -0.0898,  0.0561],\n",
      "        [-0.2480, -0.2124, -0.2918,  0.3381, -0.3456],\n",
      "        [-0.3464, -0.0233, -0.2880,  0.3966,  0.3284],\n",
      "        [-0.2238, -0.4415, -0.3083,  0.2162,  0.2018],\n",
      "        [ 0.2484, -0.2777, -0.4087,  0.3304,  0.2563],\n",
      "        [ 0.2166,  0.2253,  0.3157,  0.4382, -0.4094],\n",
      "        [ 0.2178, -0.4416,  0.0459, -0.0010, -0.3850],\n",
      "        [ 0.2625, -0.0398,  0.0874, -0.0344,  0.0478],\n",
      "        [-0.3914,  0.1948,  0.3009, -0.1443,  0.3758],\n",
      "        [-0.3381, -0.0573, -0.2979, -0.3809, -0.1226],\n",
      "        [ 0.0883,  0.3488,  0.2408, -0.1695,  0.1864]], requires_grad=True)\n",
      "\n",
      "\n",
      "qkv tensor \n",
      "\n",
      " tensor([[[ 0.5620, -0.4771, -0.1193, -0.1770, -0.0912, -0.1881, -0.9082,\n",
      "          -0.3982, -0.1748,  0.4831,  0.0430,  0.4818, -0.7781, -0.1399,\n",
      "           0.3219],\n",
      "         [ 0.0133, -0.6892,  0.2262, -0.3412,  0.0587, -0.4870,  0.3338,\n",
      "           0.6576,  1.1507, -0.3581, -0.6044,  0.5926, -0.3389, -0.6563,\n",
      "           0.3061],\n",
      "         [ 0.3586, -0.1289, -0.1339, -0.5227, -0.0447,  0.3891, -0.3131,\n",
      "           0.1238,  0.3145,  0.7347,  0.1284,  0.3672, -0.9779, -0.3589,\n",
      "          -0.0669]],\n",
      "\n",
      "        [[ 0.2992, -0.9787,  0.0876, -0.3416, -0.4438, -0.4980, -1.2683,\n",
      "          -0.7886,  0.0757,  1.1504,  0.2435,  0.8231, -1.1731, -0.8928,\n",
      "           0.5210],\n",
      "         [ 0.3113, -0.4422,  0.3197, -0.1326,  0.5185, -0.6843,  0.2379,\n",
      "           0.7497,  0.5966, -1.1216, -0.7297,  0.4218,  0.2762,  0.0052,\n",
      "           0.3843],\n",
      "         [ 0.2108, -0.8520,  0.5322, -0.0466,  0.4928, -1.2493, -0.6761,\n",
      "          -0.1807, -0.4770, -0.8084, -0.7001,  0.5052,  0.6213,  0.0726,\n",
      "           0.9045]],\n",
      "\n",
      "        [[ 0.3473, -0.3930,  0.1170,  0.0381,  0.5481, -0.6575, -0.3624,\n",
      "          -0.0052, -0.7125, -0.9024, -0.8175,  0.1464,  0.6494,  0.6556,\n",
      "           0.6400],\n",
      "         [ 0.8769, -0.3709,  0.6103, -0.1829,  0.6759, -0.8934, -1.1339,\n",
      "          -0.0502, -0.3495, -0.5188,  0.1954,  0.6451, -0.0400,  0.0099,\n",
      "           0.4641],\n",
      "         [ 0.5444,  0.0385,  0.2644, -0.3156,  0.6890, -0.1730, -0.0621,\n",
      "           0.6239,  0.0634, -0.6826, -0.2640,  0.2542,  0.0985,  0.2540,\n",
      "           0.1097]],\n",
      "\n",
      "        [[-0.9717, -1.7501, -0.2431, -0.0448, -0.8145, -0.9399,  0.0199,\n",
      "          -0.7161,  0.1496,  0.4105, -1.6209,  0.4503,  0.0465, -0.6641,\n",
      "           1.2012],\n",
      "         [ 0.0558, -0.1769,  0.2017, -0.4720,  0.4686, -0.1005, -0.6379,\n",
      "          -0.3483, -1.1627,  0.2624, -0.3467,  0.0698,  0.3225,  0.3236,\n",
      "           0.4498],\n",
      "         [ 1.4625,  1.0273, -0.1435, -0.3189,  0.9108,  0.8259, -0.3492,\n",
      "           0.8266, -0.1793, -0.4273,  0.5998,  0.0179, -0.6111,  0.8724,\n",
      "          -0.6133]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # Number of examples in the batch\n",
    "sequence_length = 3  # Length of the sequence for each example (e.g. \"I love dogs\"), assuming \"I love dogs\" would result in three tokens: \"I\", \"love\", \"dogs\"\n",
    "embedding_dimension = 5  # Dimension of the embedding space\n",
    "\n",
    "# Create a sample tensor with random values\n",
    "input_tensor = torch.randn(batch_size, sequence_length, embedding_dimension)\n",
    "print(\"\\n\\nInput tensor \\n\\n\", input_tensor)\n",
    "\n",
    "B, T, C = input_tensor.size()\n",
    "\n",
    "# In a second step we are going to use a linear layer with three times the embedding dimension.\n",
    "lin_layer = nn.Linear(embedding_dimension, 3 * embedding_dimension)\n",
    "qkv = lin_layer(input_tensor)\n",
    "print(\"\\n\\nWeights linear layer \\n\\n\", lin_layer.weight)\n",
    "\n",
    "print(\"\\n\\nqkv tensor \\n\\n\", qkv)\n",
    "q, k, v = qkv.split(embedding_dimension, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1881, -0.9082, -0.3982, -0.1748,  0.4831],\n",
       "          [-0.4870,  0.3338,  0.6576,  1.1507, -0.3581],\n",
       "          [ 0.3891, -0.3131,  0.1238,  0.3145,  0.7347]]],\n",
       "\n",
       "\n",
       "        [[[-0.4980, -1.2683, -0.7886,  0.0757,  1.1504],\n",
       "          [-0.6843,  0.2379,  0.7497,  0.5966, -1.1216],\n",
       "          [-1.2493, -0.6761, -0.1807, -0.4770, -0.8084]]],\n",
       "\n",
       "\n",
       "        [[[-0.6575, -0.3624, -0.0052, -0.7125, -0.9024],\n",
       "          [-0.8934, -1.1339, -0.0502, -0.3495, -0.5188],\n",
       "          [-0.1730, -0.0621,  0.6239,  0.0634, -0.6826]]],\n",
       "\n",
       "\n",
       "        [[[-0.9399,  0.0199, -0.7161,  0.1496,  0.4105],\n",
       "          [-0.1005, -0.6379, -0.3483, -1.1627,  0.2624],\n",
       "          [ 0.8259, -0.3492,  0.8266, -0.1793, -0.4273]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_head = 1\n",
    "# B = batch\n",
    "# T = sequence length\n",
    "# head = number of heads\n",
    "# C = embedding dimension --> The tensor k is being split into head smaller chunks. This is a common step in multi-head attention, where the input is projected into multiple subspaces corresponding to different heads.\n",
    "k = k.view(B, T, n_head, C // n_head).transpose(1, 2) # transpose starts from zero. Hence, we are transposing T and head.\n",
    "q = q.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#E4 - Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3, 5]) torch.Size([4, 1, 3, 5]) torch.Size([4, 1, 3, 5])\n",
      "tensor([[[[ 0.5620, -0.4771, -0.1193, -0.1770, -0.0912],\n",
      "          [ 0.0133, -0.6892,  0.2262, -0.3412,  0.0587],\n",
      "          [ 0.3586, -0.1289, -0.1339, -0.5227, -0.0447]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2992, -0.9787,  0.0876, -0.3416, -0.4438],\n",
      "          [ 0.3113, -0.4422,  0.3197, -0.1326,  0.5185],\n",
      "          [ 0.2108, -0.8520,  0.5322, -0.0466,  0.4928]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3473, -0.3930,  0.1170,  0.0381,  0.5481],\n",
      "          [ 0.8769, -0.3709,  0.6103, -0.1829,  0.6759],\n",
      "          [ 0.5444,  0.0385,  0.2644, -0.3156,  0.6890]]],\n",
      "\n",
      "\n",
      "        [[[-0.9717, -1.7501, -0.2431, -0.0448, -0.8145],\n",
      "          [ 0.0558, -0.1769,  0.2017, -0.4720,  0.4686],\n",
      "          [ 1.4625,  1.0273, -0.1435, -0.3189,  0.9108]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "tensor([[[[-0.1881, -0.4870,  0.3891],\n",
      "          [-0.9082,  0.3338, -0.3131],\n",
      "          [-0.3982,  0.6576,  0.1238],\n",
      "          [-0.1748,  1.1507,  0.3145],\n",
      "          [ 0.4831, -0.3581,  0.7347]]],\n",
      "\n",
      "\n",
      "        [[[-0.4980, -0.6843, -1.2493],\n",
      "          [-1.2683,  0.2379, -0.6761],\n",
      "          [-0.7886,  0.7497, -0.1807],\n",
      "          [ 0.0757,  0.5966, -0.4770],\n",
      "          [ 1.1504, -1.1216, -0.8084]]],\n",
      "\n",
      "\n",
      "        [[[-0.6575, -0.8934, -0.1730],\n",
      "          [-0.3624, -1.1339, -0.0621],\n",
      "          [-0.0052, -0.0502,  0.6239],\n",
      "          [-0.7125, -0.3495,  0.0634],\n",
      "          [-0.9024, -0.5188, -0.6826]]],\n",
      "\n",
      "\n",
      "        [[[-0.9399, -0.1005,  0.8259],\n",
      "          [ 0.0199, -0.6379, -0.3492],\n",
      "          [-0.7161, -0.3483,  0.8266],\n",
      "          [ 0.1496, -1.1627, -0.1793],\n",
      "          [ 0.4105,  0.2624, -0.4273]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the tensors: batch, sequence length, number of heads, embedding dimension (i.e. the embedding)\n",
    "print(q.shape, k.shape, v.shape)\n",
    "\n",
    "\n",
    "print(q)\n",
    "print(\"\\n\\n\")\n",
    "print(k.transpose(-2, -1))\n",
    "\n",
    "# What happens is that the transformation allows us to multiply the q and k tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1881, -0.9082, -0.3982, -0.1748,  0.4831],\n",
      "         [-0.4870,  0.3338,  0.6576,  1.1507, -0.3581],\n",
      "         [ 0.3891, -0.3131,  0.1238,  0.3145,  0.7347]]],\n",
      "       grad_fn=<SelectBackward0>), \n",
      "\n",
      " tensor([[[-0.1881, -0.4870,  0.3891],\n",
      "         [-0.9082,  0.3338, -0.3131],\n",
      "         [-0.3982,  0.6576,  0.1238],\n",
      "         [-0.1748,  1.1507,  0.3145],\n",
      "         [ 0.4831, -0.3581,  0.7347]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check batches 0\n",
    "# This transformation allows us to multiply the q and k tensors.\n",
    "print(f\"{k[0]}, \\n\\n {k.transpose(-2, -1)[0]}\")\n",
    "\n",
    "# The @ operator is used for matrix multiplication.\n",
    "# k.size(-1) is the length of the embeddings.\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E5 - Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.1619,    -inf,    -inf],\n",
      "          [ 0.2779, -0.2243,    -inf],\n",
      "          [ 0.0772, -0.3985, -0.0152]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2177,    -inf,    -inf],\n",
      "          [ 0.3310, -0.3306,    -inf],\n",
      "          [ 0.5006, -0.2364, -0.0714]]],\n",
      "\n",
      "\n",
      "        [[[-0.2720,    -inf,    -inf],\n",
      "          [-0.4136, -0.3042,    -inf],\n",
      "          [-0.3444, -0.3535, -0.1887]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3181,    -inf,    -inf],\n",
      "          [-0.0352,  0.3169,    -inf],\n",
      "          [-0.4137, -0.0638,  0.1782]]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We now apply the mask and set.\n",
    "# We replace values with '-inf' where the mask is zero.\n",
    "# While mask has the dimensions of the the sequence length we define in the overall setting (e.g. 1024)\n",
    "# It automatically adapts to the sequence length of the current batch (e.g. attn)\n",
    "att = att.masked_fill(mask=mask[:, :, :T, :T] == 0, value=float('-inf'))\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E6 - Merging\n",
    "\n",
    "### Step 1 - Check for contigiousy\n",
    "Contiguous Memory:  `[   Book 1   ] [   Book 2   ] [   Book 3   ] [   Book 4   ] [   Book 5   ]`\n",
    "\n",
    "Non-Contiguous Memory:  `[         ] [   Book 3   ] [         ] [   Book 1   ] [   Book 2   ]`\n",
    "\n",
    "Think of `.contiguous()` as a librarian who reorganizes the books on the shelf:\n",
    "`[   Book 1   ] [   Book 2   ] [   Book 3   ] [   Book 4   ] [   Book 5   ]`\n",
    "\n",
    "### Step 2 - Merging the `heads`\n",
    "We started out with `n_head`. In reality we would use several heads to capture different different \"perspectives\".\n",
    "In order to merge the 'heads' we apply .view(B, T, C) to our originally four-dimensional tensor (B, T, n_heads, C).\n",
    "We are essentially flattening (combining) the elements in the fourth dimension into the existing dimensions (B, T, C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0430,  0.4818, -0.7781, -0.1399,  0.3219],\n",
      "         [-0.2011,  0.5236, -0.6125, -0.3346,  0.3160],\n",
      "         [-0.0851,  0.4677, -0.7423, -0.3454,  0.1781]],\n",
      "\n",
      "        [[ 0.2435,  0.8231, -1.1731, -0.8928,  0.5210],\n",
      "         [-0.0878,  0.6865, -0.6798, -0.5871,  0.4745],\n",
      "         [-0.2452,  0.6413, -0.3379, -0.4157,  0.5949]],\n",
      "\n",
      "        [[-0.8175,  0.1464,  0.6494,  0.6556,  0.6400],\n",
      "         [-0.2834,  0.4094,  0.2859,  0.3151,  0.5472],\n",
      "         [-0.2951,  0.3427,  0.2294,  0.3045,  0.3887]],\n",
      "\n",
      "        [[-1.6209,  0.4503,  0.0465, -0.6641,  1.2012],\n",
      "         [-0.8728,  0.2269,  0.2086, -0.0842,  0.7600],\n",
      "         [-0.2434,  0.1376, -0.1420,  0.3246,  0.1729]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.0402,  0.5025,  0.7070, -0.5162,  0.6892],\n",
      "         [-0.1152,  0.4472,  0.7358, -0.3647,  0.6219],\n",
      "         [-0.0133,  0.4823,  0.7441, -0.4807,  0.5555]],\n",
      "\n",
      "        [[ 0.1551,  1.1552,  0.9167, -0.3060,  0.7705],\n",
      "         [-0.0968,  0.6976,  0.7757, -0.2331,  0.7158],\n",
      "         [-0.2705,  0.4810,  0.6702, -0.1059,  0.7661]],\n",
      "\n",
      "        [[-0.7204, -0.5923,  0.2955, -0.0808,  0.7469],\n",
      "         [-0.4351, -0.1187,  0.3447, -0.1709,  0.8159],\n",
      "         [-0.3989, -0.1624,  0.3594, -0.2582,  0.7164]],\n",
      "\n",
      "        [[-0.9550,  0.1387,  0.9260,  0.5125,  0.6835],\n",
      "         [-0.6582, -0.0911,  0.5612,  0.0895,  0.6544],\n",
      "         [-0.2177, -0.1294,  0.4563, -0.5051,  0.5528]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "att = F.softmax(att, dim=-1)\n",
    "y = att @ v # matrix multiplication attention * values\n",
    "y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "print(y)\n",
    "c_proj = nn.Linear(embedding_dimension, embedding_dimension)\n",
    "y = c_proj(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5]) torch.Size([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# When we compare the shape of the output tensor with the input tensor, we see that the dimensions are the same.\n",
    "# batch_size=4, sequence_length=3, embedding_dimension=5\n",
    "print(input_tensor.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E7 - Stacking blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of 'test' the origial code uses the class Block(config)\n",
    "# This leads to a list with n.layers of blocks. E.g. n.layer=3 --> 3 blocks stacked on each other\n",
    "blocks = ['test' for _ in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E8 - ModuleDict\n",
    "\n",
    "`ModuleDict` allows to create a dict with layers as items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'lin1': nn.Linear(5, 20),  # Input size is 5, output size is 20\n",
    "            'lin2': nn.Linear(20, 100),  # Input size is 20, output size is 3\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['lin1'](x)\n",
    "        x = self.layers['lin2'](x)\n",
    "        return x\n",
    "\n",
    "# Dummy tensor\n",
    "dummy_input = torch.randn(3, 5)  # Batch size of 3, input size of 5\n",
    "\n",
    "model = MyModel()\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E9 - Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) [-0.28834486 -0.42112446 -0.90688705 -1.8311492  -1.5700469 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "block_size = 200 # The maximum length of the input sequence\n",
    "n_embd = 5\n",
    "T = 100 # Sequence length\n",
    "\n",
    "pos = torch.arange(start=0, end=T, step=1, dtype=torch.long, device='cpu') # Shape (T)\n",
    "wpe = nn.Embedding(block_size, n_embd)\n",
    "pos_emd = wpe(pos)\n",
    "\n",
    "numpy_array = pos_emd.detach().numpy()\n",
    "print(pos[1], numpy_array[1]) # The the number '1' is embedded into a 5-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E10 - Regular Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 588, 6844, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.9184e-01, -1.8015e+00, -1.3186e+00,  3.9006e-01, -3.5522e-01],\n",
       "        [-5.0073e-01,  1.9695e-03,  2.5221e+00,  1.9489e-02, -1.6430e+00],\n",
       "        [-1.3805e+00, -3.6425e-01,  1.0110e-01, -7.2621e-01, -9.4091e-01],\n",
       "        [-1.0360e+00, -8.7827e-02, -4.5891e-01,  4.8211e-02, -9.3275e-01]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"I like dogs!\")\n",
    "print(tokens) # Check tokens created here: https://tiktokenizer.vercel.app/?model=gpt2\n",
    "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # Shape: (1, 8)\n",
    "n_embd = 64  # Adjust embedding dimension as needed\n",
    "wte = nn.Embedding(50257, 5) # 50257 is the number of tokens in the GPT-2 vocabulary. This needs to match.\n",
    "embeddings = wte(tokens)  # Shape: (1, 8, n_embd)\n",
    "\n",
    "embeddings[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2879, -2.4126,  0.3785,  0.4026,  1.1703],\n",
      "        [ 0.1173,  1.7102,  0.6545,  2.2376,  1.0987],\n",
      "        [ 1.6030, -1.5305,  2.1726, -0.8943,  0.1090],\n",
      "        [ 1.1404, -0.8628, -1.5088,  3.5785, -0.2907]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "result = torch.add(pos_emd[0], embeddings[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D1\n",
    "\n",
    "Every head captures a different dimension. We split the embedding by the number of heads:\n",
    "\n",
    "1) `k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)`\n",
    "\n",
    "We need to do so as we concatonate the final attention matrices afterwards:\n",
    "\n",
    "2) `y = y.transpose(1, 2).contiguous().view(B, T, C)`\n",
    "\n",
    "We see in #2 that we transpose the tensor `y` from four to three dimensions. The transformation to three dimensions merges the `h` Attention matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/trans_head.jpg\" alt=\"Alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'test', 'test']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
