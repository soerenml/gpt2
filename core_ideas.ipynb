{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E0 - Fetching the Original Model\n",
    "\n",
    "Let's examine the weights of the original model.\n",
    "\n",
    "#### Model Components\n",
    "\n",
    "* **Token Embeddings:** `transformer.wte.weight` is a lookup table for the model's vocabulary. Its size is `torch.Size([50257, 768])`, meaning there are **50,257** unique tokens, each with a vector representation (embedding) of **768** dimensions.\n",
    "\n",
    "* **Positional Embeddings:** `transformer.wpe.weight` is a lookup table that provides an embedding for each position in the sequence. Its size is `torch.Size([1024, 768])`, which corresponds to **1,024** possible positions, each with a **768-dimensional** embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soeren/code/gpt2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/soeren/code/gpt2/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2') #124M\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "# Print key and values of the tensor\n",
    "for k in sd_hf:\n",
    "    print(k, sd_hf[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Blocks (Layers)\n",
    "\n",
    "The `transformer.h` part of the names refers to the \"hidden\" or **decoder blocks**, which are the core computational units of the Transformer model. The number `0` (e.g., `transformer.h.0`) indicates the first block. A full model would have multiple blocks (e.g., `transformer.h.1`, `transformer.h.2`, etc.).\n",
    "\n",
    "Each decoder block contains two main components: a **Multi-Head Self-Attention** mechanism and a **Multi-Layer Perceptron (MLP)**.\n",
    "\n",
    "**1. Layer Normalization**\n",
    "\n",
    "* **`transformer.h.0.ln_1.weight`** and **`transformer.h.0.ln_1.bias`**: These are the parameters for the first **layer normalization** layer. This layer normalizes the data to have a mean of 0 and a standard deviation of 1 before it enters the attention mechanism. This helps stabilize training.\n",
    "\n",
    "**2. Attention Mechanism**\n",
    "\n",
    "* **`transformer.h.0.attn.c_attn.weight`**: This weight matrix is used to compute the **query, key, and value** vectors for the attention mechanism.\n",
    "    * `torch.Size([768, 2304])`: The input dimension is **768** (the embedding size). The output dimension is **2304**, which is three times the input dimension (`3 * 768`), because it's used to generate the three vectors (query, key, and value) at once.\n",
    "* **`transformer.h.0.attn.c_attn.bias`**: This is the corresponding bias vector for `c_attn`.\n",
    "* **`transformer.h.0.attn.c_proj.weight`** and **`transformer.h.0.attn.c_proj.bias`**: These parameters project the concatenated output of the attention heads back to the original `768-dimensional` space.\n",
    "\n",
    "**3. Second Layer Normalization**\n",
    "\n",
    "* **`transformer.h.0.ln_2.weight`** and **`transformer.h.0.ln_2.bias`**: These are the parameters for the second layer normalization, which is applied before the data enters the MLP.\n",
    "\n",
    "**4. Multi-Layer Perceptron (MLP)**\n",
    "\n",
    "* **`transformer.h.0.mlp.c_fc.weight`** and **`transformer.h.0.mlp.c_fc.bias`**: These are the parameters for the first layer of the MLP, a fully connected feed-forward network.\n",
    "    * `torch.Size([768, 3072])`: It expands the `768-dimensional` data to a larger, `3072-dimensional` representation (`4 * 768`).\n",
    "* **`transformer.h.0.mlp.c_proj.weight`** and **`transformer.h.0.mlp.c_proj.bias`**: These are the parameters for the second layer of the MLP, which projects the data back down to the original `768-dimensional` size.\n",
    "\n",
    "---\n",
    "\n",
    "### Follow up questions:\n",
    "\n",
    "#### Q1 - Why do we need a bias when running layer normalization (`transformer.h.0.ln_1.bias`)\n",
    "\n",
    "We need a bias for layer normalization to allow the normalized outputs to be shifted to any range. Without a bias, the layer normalization output would always have a mean of zero, which might not be the optimal mean for the subsequent layer. The bias, along with the scale parameter, provides the layer with the flexibility to learn the best possible mean and standard deviation for its output, improving the model's expressive power and helping it learn more complex patterns.\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "Layer normalization standardizes the inputs to a layer to have a mean of 0 and a variance of 1. It then applies two learnable parameters: a **scale parameter ($\\gamma$)** and a **bias parameter ($\\beta$)**. The final output of the layer normalization is calculated using the following formula:\n",
    "\n",
    "$$\\text{Output} = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "* **$x$**: The input to the layer.\n",
    "* **$\\mu$**: The mean of the input.\n",
    "* **$\\sigma$**: The standard deviation of the input.\n",
    "* **$\\frac{x - \\mu}{\\sigma}$**: This is the core normalization step, ensuring the output has a mean of 0 and a variance of 1.\n",
    "* **$\\gamma$**: The learnable scale parameter.\n",
    "* **$\\beta$**: The learnable bias parameter.\n",
    "\n",
    "The bias parameter ($\\beta$) allows the model to shift the mean of the normalized output. This is crucial because while normalizing the data to a mean of zero is a good starting point for training, the ideal mean for a specific layer's input might not be zero. The bias allows the network to find the optimal data distribution for each layer, which helps in faster convergence and better performance. In essence, the scale and bias parameters allow the model to undo the normalization if needed, giving it the flexibility to learn the most effective data representation for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/trans_arch.jpg\" alt=\"Alt text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1 - Linear layers\n",
    "\n",
    "A **linear layer** (or **fully connected layer**) in a neural network is a layer where each input neuron is connected to each output neuron with a certain weight. This layer performs a linear transformation on the input vector.\n",
    "\n",
    "Given an input vector $\\mathbf{x} \\in \\mathbb{R}^n$ and a weight matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, and a bias vector $\\mathbf{b} \\in \\mathbb{R}^m$, the output $\\mathbf{y}$ of a linear layer can be computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W}$: $\\begin{bmatrix} w_{11} & w_{21} & \\cdots & w_{n1} \\\\ w_{12} & w_{22} & \\cdots & w_{n2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{1m} & w_{2m} & \\cdots & w_{nm} \\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/q_k_v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create the following:\n",
    "\n",
    "`nn.Linear(3, 4)`\n",
    "\n",
    "We get a weight matrix according to the dimensionality of of the output neurons $y$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W} = \\begin{bmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32} \\\\\n",
    "w_{13} & w_{23} & w_{33} \\\\\n",
    "w_{14} & w_{24} & w_{34}\n",
    "\\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['linear.weight', 'linear.bias'])\n",
      "tensor([[-0.4671,  0.2912, -0.4283],\n",
      "        [-0.0576, -0.1183, -0.1192],\n",
      "        [ 0.5676,  0.1285,  0.5333],\n",
      "        [ 0.4620, -0.0255, -0.4831],\n",
      "        [ 0.0646, -0.3111,  0.1845],\n",
      "        [-0.1656,  0.2905, -0.3450],\n",
      "        [ 0.2120, -0.0692,  0.4314],\n",
      "        [ 0.1323, -0.3696, -0.4060],\n",
      "        [-0.1731,  0.1358, -0.2280]])\n"
     ]
    }
   ],
   "source": [
    "class MyLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    # Define flow\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input_size = 3\n",
    "output_size = 9\n",
    "\n",
    "x = torch.randn(1, input_size)  # Random input tensor\n",
    "\n",
    "model = MyLinearLayer(input_size, output_size)\n",
    "output = model(x)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Print of the state_dict\n",
    "print(state_dict.keys())\n",
    "\n",
    "# Access weights of a specific layer\n",
    "print(state_dict['linear.weight'])\n",
    "\n",
    "# When we look at the weight matrix, it's had exactly the dimension we expectec based on the math above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2 - Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), \n",
      "\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      " tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 10  # The maximum length of the input sequence\n",
    "\n",
    "ones_matrix = torch.ones(block_size, block_size)\n",
    "lower_triangular_matrix = torch.tril(ones_matrix)\n",
    "mask = lower_triangular_matrix.view(1, 1, block_size, block_size)\n",
    "\n",
    "print(f\"\"\"{ones_matrix}, \\n\\n {lower_triangular_matrix} \\n\\n {mask}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E3 - Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input tensor \n",
      "\n",
      " tensor([[[-2.1698,  0.6826, -1.1638,  1.5941,  1.8476],\n",
      "         [ 1.1437, -0.9423,  1.0330, -1.4596, -0.3861],\n",
      "         [ 0.3245,  1.5895, -1.0048, -0.8807,  1.3424]],\n",
      "\n",
      "        [[-0.8365, -2.1496,  0.5740, -0.5460, -0.9132],\n",
      "         [-0.2894,  0.0986, -0.9958, -0.8240, -1.0636],\n",
      "         [ 0.3557, -0.6433,  1.0517,  1.5815,  0.6039]],\n",
      "\n",
      "        [[ 0.8503, -0.3699,  0.5846, -0.8663,  1.4372],\n",
      "         [ 0.1906, -0.6420,  0.3558,  0.1981, -1.4585],\n",
      "         [ 0.4030,  0.3126, -0.1588,  0.8097,  0.3528]],\n",
      "\n",
      "        [[ 1.0425, -0.0618, -0.1667, -1.1555, -0.0031],\n",
      "         [-0.7524,  0.7781, -1.0059, -0.0085,  0.8720],\n",
      "         [-0.8803, -0.6547, -1.0416,  0.7096,  1.1041]]])\n",
      "\n",
      "\n",
      "Weights linear layer \n",
      "\n",
      " Parameter containing:\n",
      "tensor([[-0.1216,  0.2178,  0.1190, -0.0951,  0.3432],\n",
      "        [-0.3136,  0.1212,  0.3269, -0.0258,  0.1089],\n",
      "        [ 0.1307,  0.0847,  0.3193,  0.1209, -0.0460],\n",
      "        [ 0.4076, -0.2450, -0.0261, -0.3485,  0.1512],\n",
      "        [ 0.0374, -0.1124,  0.0687, -0.3511,  0.3407],\n",
      "        [-0.0027, -0.1978,  0.2237, -0.0768, -0.1568],\n",
      "        [ 0.1448, -0.0454, -0.3983,  0.3846,  0.0429],\n",
      "        [ 0.2207, -0.3794, -0.1714,  0.0094, -0.1393],\n",
      "        [-0.1519,  0.1356,  0.2234, -0.3640, -0.2257],\n",
      "        [ 0.3510, -0.0077, -0.3053,  0.0115,  0.3689],\n",
      "        [ 0.0233,  0.1269, -0.2634,  0.1615,  0.3596],\n",
      "        [-0.0372,  0.0722, -0.1320,  0.1824,  0.2816],\n",
      "        [ 0.3805, -0.0161, -0.1861, -0.2233,  0.1087],\n",
      "        [ 0.3832, -0.4366,  0.4330,  0.0614, -0.3230],\n",
      "        [ 0.0293, -0.0692,  0.1355,  0.0539,  0.4217]], requires_grad=True)\n",
      "\n",
      "\n",
      "qkv tensor \n",
      "\n",
      " tensor([[[ 7.3858e-01,  3.1958e-01, -4.8283e-01, -1.6039e+00, -4.7740e-01,\n",
      "          -9.9109e-01,  9.7530e-01, -7.6921e-01, -1.1579e+00,  2.4019e-01,\n",
      "           1.2305e+00,  8.8138e-01, -9.1518e-01, -2.4855e+00,  9.5353e-01],\n",
      "         [-2.3262e-01, -3.6277e-01,  2.4753e-01,  8.1398e-01,  2.9115e-01,\n",
      "           3.9758e-01, -6.1630e-01,  4.8466e-01,  2.2516e-01, -1.1378e-01,\n",
      "          -7.7380e-01, -8.3498e-01,  4.0190e-01,  9.7890e-01,  3.5442e-01],\n",
      "         [ 7.1394e-01, -2.9192e-01, -3.0519e-01, -2.7555e-02,  2.2168e-01,\n",
      "          -8.7235e-01,  2.5858e-01, -5.4285e-01, -3.6326e-01,  8.4542e-01,\n",
      "           7.8030e-01,  2.3947e-01,  4.8732e-01, -1.8456e+00,  6.3915e-01]],\n",
      "\n",
      "        [[-5.7729e-01, -1.1906e-01, -1.2540e-01, -8.3454e-02, -1.7910e-01,\n",
      "           5.5148e-01, -3.3676e-01,  6.6624e-01,  4.6025e-02, -8.4337e-01,\n",
      "          -8.9415e-01, -7.6967e-01, -5.0795e-01,  7.7474e-01,  1.4453e-01],\n",
      "         [-3.6614e-01, -5.4057e-01, -3.9129e-01, -2.9626e-01, -4.7271e-01,\n",
      "          -2.0086e-01,  1.5224e-01,  2.2144e-01,  5.2262e-02, -2.4816e-01,\n",
      "          -2.8168e-01, -5.1365e-01,  1.7996e-03, -6.4537e-01, -2.8614e-01],\n",
      "         [-1.9106e-02, -4.4003e-02,  4.9796e-01, -4.9106e-01, -5.0109e-01,\n",
      "          -4.3997e-02,  4.6068e-01,  8.4787e-02, -9.4090e-01,  1.7885e-03,\n",
      "           8.8174e-02,  4.6833e-02, -4.7765e-01,  4.2149e-01,  8.9452e-01]],\n",
      "\n",
      "        [[ 4.4358e-01, -1.6475e-01,  1.0236e-01,  6.3480e-01,  5.9795e-01,\n",
      "          -1.4664e-01, -1.9973e-01,  3.1162e-02, -3.8037e-01,  5.9509e-01,\n",
      "           1.6170e-01, -1.0195e-01,  4.3030e-01, -1.3006e-01,  1.0464e+00],\n",
      "         [-6.5768e-01, -4.0840e-01,  1.8195e-01, -3.7031e-01, -7.7213e-01,\n",
      "           2.3014e-01,  9.3250e-02,  4.4137e-01, -1.0202e-01, -6.2042e-01,\n",
      "          -6.9740e-01, -6.8812e-01, -3.2636e-01,  6.3746e-01, -1.4907e-01],\n",
      "         [ 2.6549e-02, -3.4613e-01,  1.1688e-01, -4.4346e-01, -5.0444e-01,\n",
      "          -4.0538e-01,  5.9864e-01, -3.2285e-02, -7.5128e-01,  2.7905e-01,\n",
      "           3.1446e-01,  6.2383e-02, -1.0475e-01, -4.6824e-01,  5.1821e-01]],\n",
      "\n",
      "        [[-6.8835e-02, -5.8265e-01, -5.4994e-02,  5.4021e-01,  1.2975e-01,\n",
      "          -1.2811e-01, -5.9727e-02,  2.8337e-01, -1.0522e-01,  3.5490e-01,\n",
      "          -1.6155e-01, -4.4602e-01,  5.4621e-01, -6.8841e-02,  3.0581e-01],\n",
      "         [ 4.2358e-01, -1.2662e-01, -3.8795e-01, -6.4273e-01, -1.9396e-01,\n",
      "          -7.0243e-01,  4.5513e-01, -3.9879e-01, -5.2132e-01,  3.1053e-01,\n",
      "           6.2430e-01,  2.4773e-01, -1.5503e-01, -1.6990e+00,  5.1215e-01],\n",
      "         [ 1.3413e-01, -2.6505e-01, -4.6126e-01, -5.5805e-01, -2.1330e-01,\n",
      "          -5.1828e-01,  8.0204e-01,  9.7081e-02, -1.0180e+00,  3.8146e-01,\n",
      "           6.4845e-01,  3.5014e-01, -3.0909e-01, -1.1687e+00,  7.3926e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # Number of examples in the batch\n",
    "sequence_length = 3  # Length of the sequence for each example (e.g. \"I love dogs\"), assuming \"I love dogs\" would result in three tokens: \"I\", \"love\", \"dogs\"\n",
    "embedding_dimension = 5  # Dimension of the embedding space\n",
    "\n",
    "# Create a sample tensor with random values\n",
    "input_tensor = torch.randn(batch_size, sequence_length, embedding_dimension)\n",
    "print(\"\\n\\nInput tensor \\n\\n\", input_tensor)\n",
    "\n",
    "B, T, C = input_tensor.size()\n",
    "\n",
    "# In a second step we are going to use a linear layer with three times the embedding dimension.\n",
    "lin_layer = nn.Linear(embedding_dimension, 3 * embedding_dimension)\n",
    "qkv = lin_layer(input_tensor)\n",
    "print(\"\\n\\nWeights linear layer \\n\\n\", lin_layer.weight)\n",
    "\n",
    "print(\"\\n\\nqkv tensor \\n\\n\", qkv)\n",
    "q, k, v = qkv.split(embedding_dimension, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9911,  0.9753, -0.7692, -1.1579,  0.2402],\n",
       "          [ 0.3976, -0.6163,  0.4847,  0.2252, -0.1138],\n",
       "          [-0.8723,  0.2586, -0.5428, -0.3633,  0.8454]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5515, -0.3368,  0.6662,  0.0460, -0.8434],\n",
       "          [-0.2009,  0.1522,  0.2214,  0.0523, -0.2482],\n",
       "          [-0.0440,  0.4607,  0.0848, -0.9409,  0.0018]]],\n",
       "\n",
       "\n",
       "        [[[-0.1466, -0.1997,  0.0312, -0.3804,  0.5951],\n",
       "          [ 0.2301,  0.0933,  0.4414, -0.1020, -0.6204],\n",
       "          [-0.4054,  0.5986, -0.0323, -0.7513,  0.2790]]],\n",
       "\n",
       "\n",
       "        [[[-0.1281, -0.0597,  0.2834, -0.1052,  0.3549],\n",
       "          [-0.7024,  0.4551, -0.3988, -0.5213,  0.3105],\n",
       "          [-0.5183,  0.8020,  0.0971, -1.0180,  0.3815]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_head = 1\n",
    "# B = batch\n",
    "# T = sequence length\n",
    "# head = number of heads\n",
    "# C = embedding dimension --> The tensor k is being split into head smaller chunks. This is a common step in multi-head attention, where the input is projected into multiple subspaces corresponding to different heads.\n",
    "k = k.view(B, T, n_head, C // n_head).transpose(1, 2) # transpose starts from zero. Hence, we are transposing T and head.\n",
    "q = q.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#E4 - Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3, 5]) torch.Size([4, 1, 3, 5]) torch.Size([4, 1, 3, 5])\n",
      "tensor([[[[ 0.7386,  0.3196, -0.4828, -1.6039, -0.4774],\n",
      "          [-0.2326, -0.3628,  0.2475,  0.8140,  0.2911],\n",
      "          [ 0.7139, -0.2919, -0.3052, -0.0276,  0.2217]]],\n",
      "\n",
      "\n",
      "        [[[-0.5773, -0.1191, -0.1254, -0.0835, -0.1791],\n",
      "          [-0.3661, -0.5406, -0.3913, -0.2963, -0.4727],\n",
      "          [-0.0191, -0.0440,  0.4980, -0.4911, -0.5011]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4436, -0.1647,  0.1024,  0.6348,  0.5979],\n",
      "          [-0.6577, -0.4084,  0.1820, -0.3703, -0.7721],\n",
      "          [ 0.0265, -0.3461,  0.1169, -0.4435, -0.5044]]],\n",
      "\n",
      "\n",
      "        [[[-0.0688, -0.5827, -0.0550,  0.5402,  0.1298],\n",
      "          [ 0.4236, -0.1266, -0.3879, -0.6427, -0.1940],\n",
      "          [ 0.1341, -0.2651, -0.4613, -0.5580, -0.2133]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "tensor([[[[-0.9911,  0.3976, -0.8723],\n",
      "          [ 0.9753, -0.6163,  0.2586],\n",
      "          [-0.7692,  0.4847, -0.5428],\n",
      "          [-1.1579,  0.2252, -0.3633],\n",
      "          [ 0.2402, -0.1138,  0.8454]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5515, -0.2009, -0.0440],\n",
      "          [-0.3368,  0.1522,  0.4607],\n",
      "          [ 0.6662,  0.2214,  0.0848],\n",
      "          [ 0.0460,  0.0523, -0.9409],\n",
      "          [-0.8434, -0.2482,  0.0018]]],\n",
      "\n",
      "\n",
      "        [[[-0.1466,  0.2301, -0.4054],\n",
      "          [-0.1997,  0.0933,  0.5986],\n",
      "          [ 0.0312,  0.4414, -0.0323],\n",
      "          [-0.3804, -0.1020, -0.7513],\n",
      "          [ 0.5951, -0.6204,  0.2790]]],\n",
      "\n",
      "\n",
      "        [[[-0.1281, -0.7024, -0.5183],\n",
      "          [-0.0597,  0.4551,  0.8020],\n",
      "          [ 0.2834, -0.3988,  0.0971],\n",
      "          [-0.1052, -0.5213, -1.0180],\n",
      "          [ 0.3549,  0.3105,  0.3815]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the tensors: batch, sequence length, number of heads, embedding dimension (i.e. the embedding)\n",
    "print(q.shape, k.shape, v.shape)\n",
    "\n",
    "\n",
    "print(q)\n",
    "print(\"\\n\\n\")\n",
    "print(k.transpose(-2, -1))\n",
    "\n",
    "# What happens is that the transformation allows us to multiply the q and k tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9911,  0.9753, -0.7692, -1.1579,  0.2402],\n",
      "         [ 0.3976, -0.6163,  0.4847,  0.2252, -0.1138],\n",
      "         [-0.8723,  0.2586, -0.5428, -0.3633,  0.8454]]],\n",
      "       grad_fn=<SelectBackward0>), \n",
      "\n",
      " tensor([[[-0.9911,  0.3976, -0.8723],\n",
      "         [ 0.9753, -0.6163,  0.2586],\n",
      "         [-0.7692,  0.4847, -0.5428],\n",
      "         [-1.1579,  0.2252, -0.3633],\n",
      "         [ 0.2402, -0.1138,  0.8454]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check batches 0\n",
    "# This transformation allows us to multiply the q and k tensors.\n",
    "print(f\"{k[0]}, \\n\\n {k.transpose(-2, -1)[0]}\")\n",
    "\n",
    "# The @ operator is used for matrix multiplication.\n",
    "# k.size(-1) is the length of the embeddings.\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E5 - Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.7574,    -inf,    -inf],\n",
      "          [-0.5305,  0.1794,    -inf],\n",
      "          [-0.3007,  0.1272, -0.1499]]],\n",
      "\n",
      "\n",
      "        [[[-0.0960,    -inf,    -inf],\n",
      "          [ 0.0467,  0.0029,    -inf],\n",
      "          [ 0.3292,  0.0922,  0.2164]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0382,    -inf,    -inf],\n",
      "          [-0.0604,  0.1823,    -inf],\n",
      "          [-0.0280,  0.1716, -0.0131]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0077,    -inf,    -inf],\n",
      "          [-0.0706,  0.0333,    -inf],\n",
      "          [-0.0667,  0.0867,  0.0715]]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We now apply the mask and set.\n",
    "# We replace values with '-inf' where the mask is zero.\n",
    "# While mask has the dimensions of the the sequence length we define in the overall setting (e.g. 1024)\n",
    "# It automatically adapts to the sequence length of the current batch (e.g. attn)\n",
    "att = att.masked_fill(mask=mask[:, :, :T, :T] == 0, value=float('-inf'))\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E6 - Merging\n",
    "\n",
    "### Step 1 - Check for contigiousy\n",
    "Contiguous Memory:  `[   Book 1   ] [   Book 2   ] [   Book 3   ] [   Book 4   ] [   Book 5   ]`\n",
    "\n",
    "Non-Contiguous Memory:  `[         ] [   Book 3   ] [         ] [   Book 1   ] [   Book 2   ]`\n",
    "\n",
    "Think of `.contiguous()` as a librarian who reorganizes the books on the shelf:\n",
    "`[   Book 1   ] [   Book 2   ] [   Book 3   ] [   Book 4   ] [   Book 5   ]`\n",
    "\n",
    "### Step 2 - Merging the `heads`\n",
    "We started out with `n_head`. In reality we would use several heads to capture different different \"perspectives\".\n",
    "In order to merge the 'heads' we apply .view(B, T, C) to our originally four-dimensional tensor (B, T, n_heads, C).\n",
    "We are essentially flattening (combining) the elements in the fourth dimension into the existing dimensions (B, T, C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2305,  0.8814, -0.9152, -2.4855,  0.9535],\n",
      "         [-0.1131, -0.2692, -0.0322, -0.1630,  0.5519],\n",
      "         [ 0.2572, -0.0327,  0.0725, -0.8466,  0.6060]],\n",
      "\n",
      "        [[-0.8941, -0.7697, -0.5080,  0.7747,  0.1445],\n",
      "         [-0.5946, -0.6445, -0.2587,  0.0802, -0.0661],\n",
      "         [-0.3868, -0.4224, -0.3479,  0.2394,  0.2676]],\n",
      "\n",
      "        [[ 0.1617, -0.1020,  0.4303, -0.1301,  1.0464],\n",
      "         [-0.3197, -0.4304,  0.0063,  0.3000,  0.3765],\n",
      "         [-0.1145, -0.2716, -0.0230,  0.0534,  0.4297]],\n",
      "\n",
      "        [[-0.1615, -0.4460,  0.5462, -0.0688,  0.3058],\n",
      "         [ 0.2518, -0.0812,  0.1774, -0.9262,  0.4143],\n",
      "         [ 0.3955,  0.0739,  0.0032, -1.0233,  0.5286]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0579, -0.1446, -0.1626, -0.6017,  2.0666],\n",
      "         [ 0.5001, -0.1250, -0.2764,  0.1458,  0.4454],\n",
      "         [ 0.4394, -0.2966, -0.2468,  0.0471,  0.7320]],\n",
      "\n",
      "        [[ 0.4326,  0.2188, -0.2668,  0.1565,  0.0176],\n",
      "         [ 0.3576, -0.0469, -0.2393,  0.1365,  0.1449],\n",
      "         [ 0.3954,  0.0587, -0.2948,  0.1632,  0.3028]],\n",
      "\n",
      "        [[ 0.7383, -0.2661, -0.3227,  0.2865,  0.4574],\n",
      "         [ 0.5311, -0.0721, -0.3201,  0.2960,  0.1787],\n",
      "         [ 0.4848, -0.1012, -0.3244,  0.2571,  0.3459]],\n",
      "\n",
      "        [[ 0.6331, -0.3860, -0.3084,  0.4041,  0.0736],\n",
      "         [ 0.4165, -0.3787, -0.2473,  0.0942,  0.6524],\n",
      "         [ 0.3609, -0.3092, -0.2557,  0.0281,  0.8388]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "att = F.softmax(att, dim=-1)\n",
    "y = att @ v # matrix multiplication attention * values\n",
    "y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "print(y)\n",
    "c_proj = nn.Linear(embedding_dimension, embedding_dimension)\n",
    "y = c_proj(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5]) torch.Size([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# When we compare the shape of the output tensor with the input tensor, we see that the dimensions are the same.\n",
    "# batch_size=4, sequence_length=3, embedding_dimension=5\n",
    "print(input_tensor.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E7 - Stacking blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'test', 'test']\n"
     ]
    }
   ],
   "source": [
    "# Instead of 'test' the origial code uses the class Block(config)\n",
    "# This leads to a list with n.layers of blocks. E.g. n.layer = 3 --> 3 blocks stacked on each other\n",
    "# In the original code we inititaite the blocks with the config n times (range(n))\n",
    "blocks = ['test' for _ in range(3)]\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E8 - ModuleDict\n",
    "\n",
    "`ModuleDict` allows to create a dict with layers as items.\n",
    "It therefore allows us to store a collection of modules in a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'lin1': nn.Linear(5, 20),  # Input size is 5, output size is 20\n",
    "            'lin2': nn.Linear(20, 100),  # Input size is 20, output size is 3\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['lin1'](x)\n",
    "        x = self.layers['lin2'](x)\n",
    "        return x\n",
    "\n",
    "# Dummy tensor\n",
    "dummy_input = torch.randn(3, 5)  # Batch size of 3, input size of 5\n",
    "\n",
    "model = MyModel()\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E9 - Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) [ 0.6777694  -1.5703939   1.3001819  -0.8920295  -0.18858036]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "block_size = 200 # The maximum length of the input sequence\n",
    "n_embd = 5\n",
    "T = 100 # Sequence length\n",
    "\n",
    "# Arrange returns a 1D tensor with values from the start (0 in this case) to the end (T), excluding T.\n",
    "# it's similar to Python’s built-in range function but returns a tensor instead of a list.\n",
    "pos = torch.arange(start=0, end=T, step=1, dtype=torch.long, device='cpu') # Shape (T)\n",
    "wpe = nn.Embedding(block_size, n_embd)\n",
    "pos_emd = wpe(pos)\n",
    "\n",
    "numpy_array = pos_emd.detach().numpy()\n",
    "\n",
    "# The the number '1' is embedded into a 5-dimensional vector.\n",
    "print(pos[1], numpy_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E10 - Regular Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 588, 6844, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3620, -1.2837, -0.3968,  0.6417,  0.1096],\n",
       "        [-0.4312,  1.9651,  0.2502,  2.1025, -0.0606],\n",
       "        [ 1.6636,  0.5339,  2.7540, -0.9897, -0.3627],\n",
       "        [ 0.0473,  0.4471,  0.3962, -0.1315,  0.4331]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"I like dogs!\")\n",
    "print(tokens) # Check tokens created here: https://tiktokenizer.vercel.app/?model=gpt2\n",
    "\n",
    "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # Shape: (1, 8)\n",
    "\n",
    "n_embd = 64  # Adjust embedding dimension as needed\n",
    "wte = nn.Embedding(50257, 5) # 50257 is the number of tokens in the GPT-2 vocabulary. This needs to match.\n",
    "embeddings = wte(tokens)  # Shape: (1, 8, n_embd)\n",
    "\n",
    "embeddings[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11 - Multinomal distribution\n",
    "\n",
    "I case we have a set of probabilities a multinomal distribution can help to introduce some randomness.\n",
    "We see, for example, that p=0.8 is not picked 80% of the time. In context of a LLM we want to introduce some randomness:\n",
    "If we only choose the most probable token at each step this would leading to repetitive and predictable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 11 75]\n",
      " [11 11 78]\n",
      " [ 9  8 83]\n",
      " [13 12 75]\n",
      " [13 14 73]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "# Parameters:\n",
    "n = 100  # number of trials\n",
    "p = [0.1, 0.1, 0.8]  # probabilities of each outcome\n",
    "\n",
    "# Generate random samples\n",
    "samples = multinomial.rvs(n, p, size=5)  # Generate 10 samples\n",
    "\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E12 - Weight tying\n",
    "\n",
    "Weight tying works by reducing the redundancy between the embedding and output layers in a neural network. This reduction leads to fewer parameters, faster training, and better generalization, making the model more efficient and effective, especially for language-related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'lin1': nn.Linear(20, 20),  # Input size is 5, output size is 20\n",
    "            'lin2': nn.Linear(20, 20),  # Input size is 20, output size is 3\n",
    "        })\n",
    "\n",
    "        # Tye the weights of the second linear layer to the weights of the first linear layer.\n",
    "        self.layers.lin2.weight = self.layers.lin1.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['lin1'](x)\n",
    "        x = self.layers['lin2'](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Dummy tensor\n",
    "dummy_input = torch.randn(3, 20)  # Batch size of 3, input size of 5\n",
    "\n",
    "model = MyModel()\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(model.layers['lin1'].weight - model.layers['lin2'].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E12 - weight sharing\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E13 - Residual stream\n",
    "\n",
    "The residual stream adds the input of an layer to its output.<br>\n",
    "We are not talking about weights here. We are talking about the acutal- in and outputs.\n",
    "<br>\n",
    "\n",
    "Example:<br>\n",
    "`attn_output, _ = self.attention(x, x, x)`<br>\n",
    "`x = x + attn_output # Residual connection: add the input 'x' back to the output`<br>\n",
    "`x = self.norm1(x)`<br>\n",
    "\n",
    "<br>\n",
    "The problem with this approch is an explosion of the deviation of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/res.png\" alt=\"Alt text\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.2054)\n",
      "tensor(0.9007)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(30)\n",
    "n = 100\n",
    "for i in range(n):\n",
    "    # We create random values with mean 0 and standard deviation 1.\n",
    "    x += torch.randn(30)\n",
    "# Just after 100 interations the std moved to >10!\n",
    "print(x.std())\n",
    "\n",
    "# We can fix this by dividing the sum by the square root of the number of iterations.\n",
    "x = torch.zeros(30)\n",
    "for i in range(n):\n",
    "    x += n**-0.5 * torch.randn(30)\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E14 - Quantization\n",
    "\n",
    "Quantization is used to speed up training time and work more memory efficient.\n",
    "It works as follows:\n",
    "\n",
    "<img src=\"images/prec2.png\" alt=\"Alt text\" width=\"400\"/>\n",
    "\n",
    "Different procedures are available with `TF32` being the gold standard for training.\n",
    "\n",
    "<img src=\"images/prec1.png\" alt=\"Alt text\" width=\"400\"/>\n",
    "\n",
    "What is missing here is sign (0 for positive and 1 for negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.unsqueeze()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6])\n",
      "torch.Size([1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2009, 2003, 1037, 3722,  102],\n",
       "        [ 101, 2009, 2003, 1037, 3722,  102],\n",
       "        [ 101, 2009, 2003, 1037, 3722,  102],\n",
       "        [ 101, 2009, 2003, 1037, 3722,  102],\n",
       "        [ 101, 2009, 2003, 1037, 3722,  102]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adds a new dimension to the tensor at the specified position. In this case, 0 indicates that the new dimension will be added at the beginning.\n",
    "Adding this extra dimension is often necessary for batching in PyTorch. Transformer models expect input tensors to have a specific shape, typically [batch_size, sequence_length].\n",
    "By unsqueezing the tensor, we create a batch dimension, even if there's only one sequence in the batch.\n",
    "\"\"\"\n",
    "\n",
    "tokens = [101, 2009, 2003, 1037, 3722, 102]\n",
    "tens = torch.tensor(tokens)\n",
    "\n",
    "# The shape of the tensor is (6,) because it's a 1D tensor with 6 elements.\n",
    "# For our model we need a batch dimension, so we add a dimension at the beginning.\n",
    "print(tens.shape)\n",
    "\n",
    "tens2 = tens.unsqueeze(0)\n",
    "print(tens2.shape)\n",
    "\n",
    "# Repeat the tensor 5 times along the first dimension\n",
    "tens.unsqueeze(0).repeat(5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `@classmethod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof!\n",
      "Miau!\n",
      "Can't call method without instance\n"
     ]
    }
   ],
   "source": [
    "class Dog():\n",
    "    def bark(self):\n",
    "        print(\"Woof!\")\n",
    "\n",
    "\n",
    "class Cat():\n",
    "    @classmethod\n",
    "    def miau(self):\n",
    "        print(\"Miau!\")\n",
    "\n",
    "# Using the Dog class to create an instance of a dog and call the bark method\n",
    "doggi = Dog()\n",
    "doggi.bark()\n",
    "\n",
    "# While the Dog class has an instance method, the Cat class has a class method.\n",
    "# This means that the Cat class method can be called without creating an instance of the class.\n",
    "Cat.miau()\n",
    "try:\n",
    "    Dog.bark()\n",
    "except:\n",
    "    print(\"Can't call method without instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `state_dict()`\n",
    "\n",
    "`state_dict()` returns the different model layers.\n",
    "\n",
    "h.# stand for the individual heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model information: GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Number of heads: 12\n",
      "Number of layers: 12\n",
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # Load the GPT-2 model\n",
    "sd_hf = model_hf.state_dict() # State dict are the raw tensors\n",
    "\n",
    "print(\"Model information:\", model_hf.config)\n",
    "print(\"Number of heads:\", model_hf.config.num_attention_heads)\n",
    "print(\"Number of layers:\", model_hf.config.num_hidden_layers)\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.8821e-02, -1.9742e-01,  4.0267e-03,  1.1347e-02,  6.3824e-02,\n",
       "        -1.0501e-01,  3.6937e-02, -1.6803e-01, -4.9111e-02, -5.6461e-02,\n",
       "        -2.4560e-03,  1.3503e-02, -4.1711e-03,  1.5115e-02,  1.6595e-02,\n",
       "        -1.3808e-01, -6.3314e-03, -4.6150e-02,  2.6675e-02, -2.0417e-01,\n",
       "         1.3454e-02, -3.6267e-02,  1.9301e-02, -2.5931e-02,  8.0243e-03,\n",
       "         8.4712e-03, -1.9906e-02,  6.6802e-02,  7.1151e-03, -2.6618e-02,\n",
       "         2.0829e-02, -3.3732e-02, -8.2898e-03,  9.8622e-03, -2.7369e-02,\n",
       "        -9.9118e-02, -7.5254e-01,  2.3550e-02, -3.0513e-02,  7.7456e-02,\n",
       "         3.4301e-03,  7.1132e-03,  2.6479e-02, -1.2113e-03,  1.1219e-01,\n",
       "        -2.0606e-03, -2.2458e-02, -2.2287e-02,  2.3570e-02,  3.9777e-01,\n",
       "         1.8856e-02,  2.0280e-02,  6.3043e-01,  2.3146e-02, -4.6894e-02,\n",
       "         4.0653e+00, -1.7403e-02, -5.1683e-02,  7.2271e-02, -7.9312e-02,\n",
       "         4.0248e-02,  1.9908e-02, -4.6380e-02, -2.8380e-02,  7.2535e-03,\n",
       "         2.6772e-02,  1.4972e-03, -2.9892e-01, -1.1627e-01,  9.1860e-03,\n",
       "        -2.3379e-02,  7.8674e-02,  3.6326e-02, -4.1279e-02,  3.5415e-02,\n",
       "         2.6852e-02,  3.7368e-04,  4.5535e-03,  2.5672e-02, -3.8062e-02,\n",
       "         1.5198e-02, -1.1324e-03, -3.6556e-02, -9.6938e-03, -7.7945e-02,\n",
       "        -5.0139e-02, -1.0231e+00, -1.2559e-02, -1.8484e-03,  6.6347e-02,\n",
       "         1.1819e-03, -3.3319e-02, -1.1513e-01,  2.1627e-02, -4.3646e-02,\n",
       "        -1.1303e-02, -7.7890e-03,  1.6647e-02, -7.2352e-03,  1.0707e-02,\n",
       "         1.3787e-02, -3.7777e-02, -1.2361e-01, -1.7224e-01, -1.8045e-02,\n",
       "         6.3456e-03, -4.8349e-02,  4.1328e-01, -1.0673e-03, -1.1704e-02,\n",
       "         1.6934e-02,  3.2545e-03, -2.3711e-02,  5.9829e-02, -5.9910e-02,\n",
       "        -1.4971e-02, -8.3736e-01, -3.8427e-02, -7.6987e-03,  4.9164e-02,\n",
       "         2.3555e-02, -5.3212e-02,  1.8699e-03, -1.5544e-01,  6.5622e-02,\n",
       "         5.2014e-03, -1.3533e-02,  2.8135e-02, -1.0581e-01,  5.5449e-02,\n",
       "         3.4420e-02,  1.4542e-02,  1.7542e-02, -2.5723e-03, -1.6590e-03,\n",
       "         1.6827e-02, -3.0037e-02, -1.1984e-01,  8.1251e-02, -4.5297e-01,\n",
       "         1.6045e-02,  4.2054e-03,  3.4240e-02, -2.0139e-02, -3.2975e-02,\n",
       "        -5.1996e-02, -3.6392e-02, -1.1540e-02,  2.2939e-02,  7.3400e-02,\n",
       "        -4.6437e-02, -8.2206e-02, -1.3095e-02,  1.5196e-02,  1.6183e-01,\n",
       "        -5.1890e-02,  4.9133e-02,  5.4630e-03, -3.1565e-02, -2.2381e-02,\n",
       "        -1.6066e-01,  2.7186e-02,  3.3445e-02, -2.9198e-02,  3.6843e-02,\n",
       "         1.0586e-03,  2.1095e-02,  6.4639e-02, -2.7470e-02,  8.8352e-02,\n",
       "         5.5776e-02, -2.3872e-02, -1.2993e-02, -2.6942e-04,  1.0038e-01,\n",
       "        -9.2795e-02, -7.9185e-01, -4.2195e-02,  2.4330e-02, -1.9673e-02,\n",
       "        -2.2679e-02,  1.3850e-02, -2.0604e-02,  9.8258e-02,  3.8011e-02,\n",
       "         2.3985e-02,  2.6789e-02,  3.1317e-02, -1.0934e-02, -7.4405e-02,\n",
       "        -4.1879e-02, -7.8305e-03, -9.1414e-02,  1.4571e-01, -2.2372e-02,\n",
       "        -7.4952e-02,  3.6658e-02,  4.8322e-03,  1.1366e-02,  7.7893e-02,\n",
       "        -5.3751e-02, -1.1380e-02, -1.0407e-02, -1.5257e-02,  3.0442e-02,\n",
       "        -5.0440e-03,  2.6727e-02,  5.8141e-03, -3.8137e-02,  1.3994e-02,\n",
       "        -2.9547e-03,  4.7238e-02,  5.7311e-02,  6.8433e-02, -4.4144e-02,\n",
       "         4.0254e-03,  3.9776e-02, -3.2052e-02,  2.8832e-02, -2.5070e-02,\n",
       "        -3.2514e-03, -7.3319e-02,  5.0036e-02, -7.9418e-02, -1.7448e-02,\n",
       "        -6.5178e-03,  8.9161e-03,  1.6941e-02,  2.8391e-02,  1.0850e-02,\n",
       "         1.5500e-02, -1.5806e-02, -1.5287e-02, -2.9555e-02, -4.0786e-02,\n",
       "        -5.6020e-02, -3.3106e-02,  3.6305e-02, -5.9147e-03, -5.6259e-03,\n",
       "        -3.0364e-03,  1.1340e-01,  2.6560e-02,  1.6906e-03,  1.5023e-02,\n",
       "        -3.3559e-02,  1.6866e-02, -3.8061e-02,  7.5497e-03,  1.0009e-02,\n",
       "         2.3764e-02,  3.2676e-02, -8.4955e-03,  8.1304e-03, -6.6838e-02,\n",
       "         6.7926e-02, -3.7586e-02,  4.8547e-02,  8.3520e-02,  2.8259e-02,\n",
       "         6.7701e-03, -9.8969e-02,  1.6984e-02,  1.0438e-02, -4.1560e-02,\n",
       "         9.8130e-03, -3.7107e-03,  1.7649e-02, -2.9944e-02,  1.1517e-02,\n",
       "         2.1170e+00,  8.0320e-01, -2.7883e-02, -2.5582e-02, -2.4898e-02,\n",
       "         2.4827e-02, -3.8706e-01,  3.0922e-02,  5.3630e-02, -2.1325e-02,\n",
       "        -1.7633e-02,  1.4735e-02, -1.3573e-02,  9.3698e-02,  6.3691e-02,\n",
       "        -8.5184e-03, -3.6612e-02,  1.6177e-02, -4.6694e-01,  6.3364e-01,\n",
       "        -6.5773e-02,  5.9070e-02,  4.0830e-02,  3.7846e-02,  2.1491e-02,\n",
       "        -1.2280e-02,  2.3577e-02,  6.6729e-03,  2.7811e-02, -6.1744e-02,\n",
       "         3.0685e+00, -1.8870e-02,  1.8418e-02, -5.8950e-04, -3.9127e-02,\n",
       "        -1.2742e-02, -3.3234e-02,  4.1388e-03, -2.1280e-01,  3.9684e-02,\n",
       "         2.4262e-02,  1.7842e-01, -4.1642e-04, -5.5655e-02, -3.9652e-02,\n",
       "         3.6324e-02, -3.6439e-04,  3.3898e-02,  3.8444e-02,  3.1045e-02,\n",
       "         1.6215e-01,  1.9416e-02,  3.9075e-02, -7.9726e-02, -3.5166e-02,\n",
       "        -6.1327e-02, -9.5163e-02, -1.3732e-02, -2.8533e-02, -3.2208e-02,\n",
       "         5.1764e-02,  5.1271e-02,  1.3170e-02, -2.8410e-02,  1.9225e-02,\n",
       "        -6.1447e-02,  9.4512e-02,  1.7898e-02,  3.3249e-02, -8.4298e-02,\n",
       "        -1.6876e-03,  2.2678e-02,  3.8959e-02, -2.6315e-02,  2.6722e-02,\n",
       "        -4.1036e-02, -4.3836e-02,  7.2811e-03,  2.3958e-02,  2.5368e-02,\n",
       "         7.1948e-02, -2.8513e-01,  6.8697e-03, -3.6890e-02, -2.6570e-03,\n",
       "        -2.4861e-02, -2.1709e+00, -7.5174e-02, -3.7429e-02,  4.7887e-02,\n",
       "         2.3458e-02, -9.0910e-01,  2.8369e-01, -7.7521e-03,  2.6658e-02,\n",
       "        -5.5254e-01, -2.5945e-02, -2.3110e-02,  3.5670e-02, -3.8938e-02,\n",
       "         6.3535e-02,  2.6753e-02,  3.8857e-02, -7.1311e-03, -5.0512e-01,\n",
       "        -8.4332e-02, -8.9942e-03, -9.6385e-01, -1.9003e-01, -1.6461e-02,\n",
       "         1.5758e-02, -2.9520e-02, -3.7219e-02,  2.6768e-02,  7.8288e-02,\n",
       "         3.6225e-02, -2.4597e-02, -9.8554e-03,  1.7192e-02,  2.0096e-01,\n",
       "         4.9101e-02,  1.9576e-02, -4.3461e-03,  9.6674e-02, -2.8922e-02,\n",
       "         1.3446e-02, -1.8345e-02,  5.6624e-02,  1.7636e-02,  8.8476e-03,\n",
       "        -2.3983e-02, -4.1040e-02, -3.5934e-01,  1.3422e-02, -1.9759e-02,\n",
       "         5.2325e-02, -2.6986e-02,  5.4985e-03, -1.2450e-01, -1.7049e-03,\n",
       "         3.3860e-04, -4.5541e-02, -2.4598e-02, -1.8265e-02, -7.3527e-03,\n",
       "         5.3070e-02,  2.5912e-02, -4.8607e-02,  3.5768e-02, -2.2845e-02,\n",
       "        -3.4739e-01, -3.7481e-02, -8.0708e-03,  7.6596e-02, -4.2530e-02,\n",
       "         4.2420e-02,  4.8602e-02,  2.1992e-02,  1.1385e-01, -2.4760e-02,\n",
       "        -9.0741e-02,  1.1689e-02,  9.7670e-02,  6.2420e-02,  8.3420e-04,\n",
       "         5.7120e-02, -3.3638e-03,  4.3362e-02,  6.4554e-03,  4.3409e-01,\n",
       "        -2.8289e-02,  8.4534e-03,  1.6225e-01,  2.1655e-02,  5.4778e-02,\n",
       "         3.3466e-02,  1.0512e-02,  6.3881e-03, -1.7619e-02, -3.7032e-02,\n",
       "        -5.3927e-02, -2.1898e-02,  2.0860e-02,  4.2955e-01,  2.8156e-02,\n",
       "        -3.0496e-01, -5.0267e-02,  3.3228e-02, -1.9458e-03,  4.3604e-01,\n",
       "         1.2594e-02,  1.6785e-03,  3.2018e-02, -1.1435e-02,  3.0255e-02,\n",
       "        -2.8112e-02,  4.1541e-03, -2.9137e-02, -5.2214e-02, -7.5817e-02,\n",
       "        -5.0438e-03,  3.8585e-03,  9.9854e-02, -2.6922e-02,  9.8854e-03,\n",
       "        -7.0485e-03, -5.8754e-03,  2.7603e-02,  9.0747e-02, -8.6773e-01,\n",
       "        -2.5373e-02,  9.3951e-03,  4.5241e-03,  2.1289e-02, -1.7936e-02,\n",
       "        -3.6137e-02, -2.0088e-02, -1.9245e-02,  1.3789e-01,  3.2350e-02,\n",
       "         4.9784e-02,  7.1033e-03,  2.4016e-02, -4.6873e-04,  1.0043e-02,\n",
       "        -1.8570e-02, -1.9425e-01,  2.0531e-01,  4.4722e-02, -4.8765e-02,\n",
       "         3.0511e-02,  1.5151e-04,  1.1109e-02, -1.5149e+00,  2.5688e-02,\n",
       "         7.3238e-03,  4.4182e-02, -1.5863e-02, -6.9036e-02,  9.6258e-03,\n",
       "         2.1722e-02,  3.8190e-01,  1.3655e-02, -3.3429e-02,  3.3138e-02,\n",
       "        -1.7596e-02,  1.2354e-02, -9.8003e-02,  1.7169e-02,  3.9591e-02,\n",
       "         4.0094e-02, -3.8441e-02,  7.1837e-02, -3.8313e-03, -3.9395e-02,\n",
       "        -2.5580e-01, -4.1298e-02, -2.0359e-01, -9.5320e-01,  2.5642e-02,\n",
       "         9.2723e-03,  3.7339e-02,  3.6342e-02, -7.0528e-03, -6.5394e-02,\n",
       "         1.2265e-02,  1.0122e-02,  1.1549e-02, -1.8140e-02,  6.1482e-03,\n",
       "         1.7135e+00, -3.5519e-02, -5.0767e-02,  7.6745e-03,  2.9331e-02,\n",
       "         3.0430e-03, -3.1479e-02, -1.2568e-02, -2.1320e-02,  2.1707e-02,\n",
       "         1.8987e-02, -8.8883e-01,  1.2125e-02,  2.6438e-02,  2.0943e-01,\n",
       "         3.2970e-02,  6.6763e-02,  3.3256e-02, -3.2825e-02, -1.2005e-01,\n",
       "        -4.9361e-02,  5.1311e-02, -3.3480e-02,  9.5489e-03,  6.4813e-02,\n",
       "        -2.4486e-02, -2.7666e-01,  4.9428e-04,  4.4311e-02, -1.4237e-02,\n",
       "        -1.6418e-02,  1.8140e-02,  8.0819e-02,  1.3806e-02,  2.2750e-03,\n",
       "        -2.3704e-02,  3.5261e-02,  7.6599e-03,  3.8602e-02,  6.6556e-03,\n",
       "        -1.7804e-02,  1.3549e+00,  1.9752e-02, -3.4907e-02,  6.4030e-03,\n",
       "        -6.0518e-02,  6.0842e-02, -1.7444e-02,  1.0880e-01, -1.8750e-02,\n",
       "        -1.0402e-02, -6.6880e-03, -7.7606e-03, -2.2134e-02,  1.2712e-02,\n",
       "        -1.2865e-02, -3.5302e-02, -1.6899e-03,  6.8769e-03,  2.6735e-02,\n",
       "         3.6005e-02,  2.0541e-03, -4.8858e-02, -5.8962e-02, -4.0852e-02,\n",
       "        -1.1628e-02,  1.2221e-02, -1.7475e-02,  2.4019e-01, -2.5707e-02,\n",
       "         1.1639e-02, -7.5646e-03, -2.8997e-03, -1.0732e-02, -1.1266e-01,\n",
       "         1.8306e-01, -9.5816e-03,  9.0851e-02, -4.1114e-02, -4.6769e-02,\n",
       "        -1.4158e-02,  2.6344e-02,  6.5998e-01, -5.0159e-01,  6.4231e-02,\n",
       "         6.2835e-03, -3.3330e-02, -1.6098e+00,  3.8070e-02,  1.1026e-02,\n",
       "        -2.5894e-02,  6.1403e-03,  4.6618e-02,  2.7380e-02, -3.2654e-02,\n",
       "         2.3865e-02, -2.8636e-02,  2.9206e-03, -3.3496e-03,  2.7162e-02,\n",
       "         1.9196e-01, -6.2142e-02,  2.1860e-02,  2.7283e-01,  3.2564e-02,\n",
       "         9.9084e-03, -3.1970e-02,  3.6526e-02,  2.2737e-02, -1.8527e-01,\n",
       "        -8.3985e-03, -3.7638e-02,  2.2447e-02,  2.3149e-02,  4.9008e-03,\n",
       "        -4.9940e-01, -5.8577e-02,  1.3585e-02, -5.3179e-02,  8.9382e-03,\n",
       "        -2.9503e-02,  1.1848e-02, -6.9815e-03,  1.5438e-02, -6.4257e-02,\n",
       "        -8.2343e-02, -8.6457e-02, -7.3714e-02,  1.6764e-02,  6.2733e-02,\n",
       "         3.0075e+00,  2.3481e-03, -8.1105e-03,  3.6314e-02, -4.5381e+00,\n",
       "         3.0046e-02, -4.3590e-03,  1.9305e-03, -1.9914e-02, -1.3940e+00,\n",
       "         8.5825e-02,  1.5078e-03, -3.5813e-02, -2.5510e-02,  8.1946e-02,\n",
       "        -3.9880e-02, -4.3724e-03,  4.6706e-02, -7.5673e-02, -5.7956e-02,\n",
       "        -4.0163e-02,  8.6380e-03, -1.8892e-02,  1.2066e-02,  7.1348e-02,\n",
       "         3.0804e-02,  3.5332e-02, -3.0011e-02, -4.6266e-03, -3.0416e-01,\n",
       "         7.3280e-03, -3.6848e-01,  1.8192e-02,  2.1566e-02,  2.4931e-02,\n",
       "        -3.1073e-02,  9.5428e-05, -1.1928e-01, -1.5507e-03, -2.4268e-01,\n",
       "        -8.4555e-03, -1.3348e-02,  1.5365e-03,  6.2889e-03,  1.3201e-02,\n",
       "         1.5080e-02, -3.1683e-03,  6.8163e-02, -7.9179e-01, -4.6202e-03,\n",
       "         9.1074e-01, -1.6140e-01,  1.3297e-02,  6.4707e-03,  1.4513e+00,\n",
       "         5.0443e-02,  1.4084e-01,  1.5878e-02,  3.4399e-01, -8.5022e-03,\n",
       "        -5.8426e-03, -1.0584e-02,  5.1128e-02,  3.6588e-02,  3.8108e-02,\n",
       "        -4.0882e-02, -3.5466e-02, -1.0839e-02,  2.4481e-03, -2.0107e-01,\n",
       "         6.0284e-03,  4.9156e-02, -2.1477e-02, -2.4268e-03, -9.1521e-03,\n",
       "         3.4912e-01, -1.5139e-01, -7.5483e-02,  2.2206e-02, -6.1442e-03,\n",
       "        -6.3152e-03,  9.7740e-02,  1.3022e-02,  1.3445e-02,  2.6431e-02,\n",
       "        -3.6497e-02, -1.5883e+00, -6.3216e-02, -4.6221e-02, -2.8667e-02,\n",
       "         5.4453e-02, -5.2860e-02,  2.5602e-03, -8.4281e-03,  3.3671e-03,\n",
       "        -4.3044e-02,  2.8267e-02,  5.4490e-02])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a singular layer\n",
    "\n",
    "# We get a context length `n_ctx` of 1024 tokens with am embedding dimension of 768 per token.\n",
    "print(sd_hf[\"transformer.wpe.weight\"].shape)\n",
    "\n",
    "# The first element of the tensor is the embedding for the first token.\n",
    "sd_hf[\"transformer.wpe.weight\"][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash attention (Fa) [E14]\n",
    "\n",
    "Fa is a kernel fusion algorithm. Fa needs more flops than the regular procedure but it's way more mindful of the memory (reads and writes).<br>\n",
    "\n",
    "<img src=\"images/flash.png\" alt=\"Alt text\" width=\"800\"/><br><br>\n",
    "\n",
    "\n",
    "Our original code looks like this:\n",
    "\n",
    "<code>\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))<br>\n",
    "att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf')<br>\n",
    "att = F.softmax(att, dim=-1)<br>\n",
    "y = att @ v\n",
    "</code>\n",
    "\n",
    "<br>\n",
    "\n",
    "Instead we are using:\n",
    "\n",
    "`F.scaled_dot_product_attention(q, k, v, is_causal=True)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E14 - Gradient clipping\n",
    "\n",
    "## Intuition\n",
    "GC is used to limit the size of a gradient. If gradient become to large:\n",
    "\n",
    "- Exploding Gradients: When the gradients are too large, they can cause the model's weights to update excessively, leading to unstable training. The loss might start to fluctuate wildly or even become NaN (not a number).\n",
    "\n",
    "- Divergence: If the updates to the weights are too large, the model may never converge, meaning it won’t learn properly and might get stuck in a bad state.\n",
    "\n",
    "## Computational approach\n",
    "\n",
    "**Calculates the L2 Norm for Each Vector**: For each vector it computes:\n",
    "\n",
    "$||\\mathbf{v}||_2 = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_n^2}$\n",
    "\n",
    "Imagine plotting a vector: \n",
    "\n",
    "$\\mathbf{v} = (3, 4)$<br><br>\n",
    "$||\\mathbf{v}||_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5$<br><br>\n",
    "$\\mathbf{v}_{\\text{normalized}} = \\left(\\frac{3}{5}, \\frac{4}{5}\\right) = (0.6, 0.8)$<br><br>\n",
    "\n",
    "If we calculate the L2 norm of the normalized vector. We get 1.\n",
    "<br>\n",
    "\n",
    "$\\sqrt{0.6^2 + 0.8^2} = \\sqrt{0.36 + 0.64} = 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49671415 -0.1382643 ] [0.51559865]\n",
      "0.9633736512703545 -0.2681626522057565\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAEiCAYAAADOABflAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAACJ10lEQVR4nO2deVxUdffHPzPADJtsAiquICruIgpp5pIi7VlmZWpqpdWTlj/LUvPRsIVMS5/MXOpxadG2x2yxTFKzLHNBCFcMd1GUfdiEYeb+/jjd2ZiBmWF2zvv1mtcwd+5y7nDv936+55zv+UoEQRDAMAzDMAzD2AWpsw1gGIZhGIbxZFhsMQzDMAzD2BEWWwzDMAzDMHaExRbDMAzDMIwdYbHFMAzDMAxjR1hsMQzDMAzD2BEWWwzDMAzDMHaExRbDMAzDMIwdYbHFMAzDMAxjR1hsNUNeeeUVSCQSq7bduHEjJBIJzp8/b1ujdDh//jwkEgk2btxot2MwDNN0hg8fjuHDh2s+O+venTJlCjp16uTQY3oCarUavXr1wuuvv+5sU9yCm266CS+++KJV27LYciOOHz+OiRMnom3btpDL5YiKisKECRNw/PhxZ5vmcJ599llIJBLk5uaaXOfll1+GRCJBdna2TY/9/vvvsxBkzELsnPj6+iIvL6/e98OHD0evXr2cYFnz4vr16/D29sbEiRNNrlNeXg4/Pz/cf//9Nj32iRMn8Morr9i1g2otW7ZswaVLlzBjxgzNMvGaPXz4sMntLl26hNTUVCQmJiI0NBTh4eEYPnw4fv75Z7OO+8svv0AikUAikSAjI6Pe91OmTEFgYKDlJ2RnXnrpJaxatQr5+fkWb8tiy03YunUr+vfvj127dmHq1Kl4//338fjjj2PPnj3o378/vv76a7P3tWDBAlRXV1tlx6RJk1BdXY2OHTtatb2tmDBhAgBg8+bNJtfZsmULevfujT59+tj02Cy2GEupqanBm2++6Wwz7E7Hjh1RXV2NSZMmOdsUPSIjI5GcnIxvvvkGVVVVRtfZunUrbty40aAgs4YTJ04gNTXVJcXW0qVL8fDDDyM4ONii7b755hssWbIEsbGxeO211/Dvf/8b5eXlSE5OxoYNGyza1yuvvGLR+s7k3nvvRVBQEN5//32Lt2Wx5QacOXMGkyZNQkxMDLKzs/Haa6/h8ccfx6uvvors7GzExMRg0qRJOHv2bIP7qaysBAB4e3vD19fXKlu8vLzg6+trdRjSViQlJSE2NhZbtmwx+v3+/ftx7tw5jShzdW7cuAG1Wu1sMxg70a9fP3zwwQe4cuWK3Y4hCILVnShbIXrxvLy8nGqHMSZMmICKigp8++23Rr/fvHkzgoODceeddzrYMuswJRrNJTMzE3/99RcefPBBi7cdMWIELl68iM2bN+OZZ57Bc889hz/++ANxcXFYuHCh2fvp168fvv/+exw5csRiGyyhqb+ViFQqxQMPPICPPvoIgiBYtq1NLGDsytKlS1FVVYV169YhIiJC77vw8HCsXbsWlZWVeOuttzTLxbysEydO4JFHHkFoaCiGDBmi950u1dXVePbZZxEeHo4WLVrgnnvuQV5eHiQSiV7Pw1jOVqdOnXDXXXdh3759SExMhK+vL2JiYvDRRx/pHaO4uBgvvPACevfujcDAQAQFBeH222/HX3/9ZdXvMmHCBJw6dcrojbp582ZIJBKMHz8eAHkWFi1ahNjYWMjlcrRv3x4vvvgiampq6m37ySefIDExEf7+/ggNDcXQoUOxc+dOzbkeP34ce/fu1bjBdXNWzp49i3HjxiEsLAz+/v646aabsH37dr39iy70zz77DAsWLEDbtm3h7+8PhUIBpVKJ1NRUdOnSBb6+vmjZsiWGDBmC9PR0q34jxjWYP38+VCqVWd6turo6vPrqq+jcuTPkcjk6deqE+fPn17tWxfvup59+woABA+Dn54e1a9dqrq8vvvgCqampaNu2LVq0aIEHHngAZWVlqKmpwaxZsxAZGYnAwEBMnTq13r43bNiAW2+9FZGRkZDL5ejRowdWr17dqO2GOVu64SLDl2GO1Y8//ohbbrkFAQEBaNGiBe68806jKRLbtm1Dr1694Ovri169epnt1b/vvvsQEBBg1Bt+/fp17Nq1Cw888ADkcjkA4MCBA7jtttsQHBwMf39/DBs2DL///nu9bfPy8vD4448jKioKcrkc0dHRePrpp1FbW4uNGzdi3LhxAEigiOf+yy+/aLZ///330bNnT01qyDPPPIPS0lK9Y4jh5oyMDAwdOhT+/v6YP38+AODw4cNISUlBeHg4/Pz8EB0djccee6zR32Pbtm2QyWQYOnSoWb+fLj179kR4eLjeMrlcjjvuuAOXL19GeXm5WfuZOXMmQkNDzfZuNeW3Eq/NZcuWYdWqVYiJiYG/vz9Gjx6NS5cuQRAEvPrqq2jXrh38/Pxw7733ori4uJ4NycnJuHDhArKyssyyWcTborUZp/Ddd9+hU6dOuOWWW4x+P3ToUHTq1KneQx0Axo0bhy5duuCNN95oUIlPmTIFX3zxBSZNmoSbbroJe/futaiHl5ubiwceeACPP/44Jk+ejPXr12PKlClISEhAz549AZAQ2bZtG8aNG4fo6Ghcu3YNa9euxbBhw3DixAlERUWZfTyAxFZqaio2b96M/v37a5arVCp88cUXuOWWW9ChQweo1Wrcc8892LdvH6ZPn47u3bvj6NGjWL58OU6fPo1t27Zptk1NTcUrr7yCwYMHY/HixZDJZDhw4AB2796N0aNHY8WKFZg5cyYCAwPx8ssvAwBatWoFALh27RoGDx6MqqoqPPvss2jZsiU2bdqEe+65B1999RXuu+8+PftfffVVyGQyvPDCC6ipqYFMJsMrr7yCtLQ0PPHEE0hMTIRCocDhw4dx5MgRJCcnW/T7MK5DdHQ0Hn30UXzwwQeYO3dug9f6E088gU2bNuGBBx7A888/jwMHDiAtLQ0nT56sJyxycnIwfvx4PPnkk5g2bRq6deum+S4tLQ1+fn6YO3cucnNzsXLlSvj4+EAqlaKkpASvvPIK/vzzT2zcuBHR0dF6HonVq1ejZ8+euOeee+Dt7Y3vvvsO//rXv6BWq/HMM8+Yfd7du3fHxx9/rLestLQUs2fPRmRkpGbZxx9/jMmTJyMlJQVLlixBVVUVVq9ejSFDhiAzM1MjzHbu3ImxY8eiR48eSEtLQ1FREaZOnYp27do1aktAQADuvfdefPXVVyguLkZYWJjmu88//xwqlUrjCd+9ezduv/12JCQkYNGiRZBKpRoB+ttvvyExMREAcOXKFSQmJqK0tBTTp09HXFwc8vLy8NVXX6GqqgpDhw7Fs88+i3fffRfz589H9+7dNb8LQB3f1NRUjBo1Ck8//TRycnKwevVqHDp0CL///jt8fHw0NhYVFeH222/Hww8/jIkTJ6JVq1a4fv06Ro8ejYiICMydOxchISE4f/48tm7d2ujv8ccff6BXr156x2gq+fn58Pf3h7+/v1nrBwUF4f/+7/+wcOFCHDlyRK8dN6Spv5XIp59+itraWsycORPFxcV466238OCDD+LWW2/FL7/8gpdeeklzv7zwwgtYv369nh0JCQkAgN9//x3x8fHm/zgC49KUlpYKAIR77723wfXuueceAYCgUCgEQRCERYsWCQCE8ePH11tX/E4kIyNDACDMmjVLb70pU6YIAIRFixZplm3YsEEAIJw7d06zrGPHjgIA4ddff9Usu379uiCXy4Xnn39es+zGjRuCSqXSO8a5c+cEuVwuLF68WG8ZAGHDhg0NnrMgCMLAgQOFdu3a6e13x44dAgBh7dq1giAIwscffyxIpVLht99+09t2zZo1AgDh999/FwRBEP7++29BKpUK9913Xz071Wq15u+ePXsKw4YNq2fLrFmzBAB6xykvLxeio6OFTp06afa5Z88eAYAQExMjVFVV6e2jb9++wp133tnoeTPugXi/HDp0SDhz5ozg7e0tPPvss5rvhw0bJvTs2VPzOSsrSwAgPPHEE3r7eeGFFwQAwu7duzXLxPtux44deuuK11evXr2E2tpazfLx48cLEolEuP322/XWHzRokNCxY0e9ZYbXpSAIQkpKihATE6O3bNiwYXr3QmP3rlqtFu666y4hMDBQOH78uCAIdI+EhIQI06ZN01s3Pz9fCA4O1lver18/oU2bNkJpaalm2c6dOwUA9c7BGNu3b9drG0RuuukmoW3btoJKpRLUarXQpUsXISUlRe++r6qqEqKjo4Xk5GTNskcffVSQSqXCoUOHjJ6rIAjCl19+KQAQ9uzZo/f99evXBZlMJowePVqvvXnvvfcEAML69es1y4YNGyYAENasWaO3j6+//lpzfVlKu3bthLFjx9ZbrnvNWsLff/8t+Pr6CpMmTWp0XfEa/fLLL4XS0lIhNDRUuOeeezTfT548WQgICNB8tsVvJV6bERERetfPvHnzBABC3759BaVSqVk+fvx4QSaTCTdu3Khnv0wmE55++ulGz1MXDiO6OKI7tkWLFg2uJ36vUCj0lj/11FONHmPHjh0AgH/96196y2fOnGm2nT169NDzvEVERKBbt256eWRyuRxSKV1yKpUKRUVFCAwMRLdu3ayO2U+cOBGXL1/Gr7/+qlm2efNmyGQyjfv+yy+/RPfu3REXF4fCwkLN69ZbbwUA7NmzBwC51dVqNRYuXKixU8ScHLUffvgBiYmJmnAtAAQGBmL69Ok4f/48Tpw4obf+5MmT4efnp7csJCQEx48fx99//23Br8C4A2Ju5bp163D16lWj6/zwww8AgNmzZ+stf/755wGgnvc6OjoaKSkpRvf16KOP6vX2k5KSIAhCvRBTUlISLl26hLq6Os0y3euyrKwMhYWFGDZsGM6ePYuysrLGTtUkr776Kr7//nts3LgRPXr0AACkp6ejtLQU48eP17s/vby8kJSUpLk/r169iqysLEyePFkvoTs5OVmzr8YQvUC6ocRz587hzz//xPjx4yGVSpGVlYW///4bjzzyCIqKijT2VFZWYuTIkfj111+hVquhVquxbds23H333RgwYEC9YzXWZvz888+ora3FrFmz9NqbadOmISgoqN7/Wi6XY+rUqXrLQkJCAADff/89lEqlWb+BSFFREUJDQy3axhRVVVUYN24c/Pz8LB4IEhwcjFmzZuHbb79FZmam0XVs8VuJjBs3Tu/6SUpKAkDPEm9vb73ltbW1RkcRh4aGorCw0PyTBOdsuTyiiGosBm5KlEVHRzd6jAsXLkAqldZbNzY21mw7O3ToUG9ZaGgoSkpKNJ/VajWWL1+OLl26QC6XIzw8HBEREcjOzra6AX/44Yfh5eWlaTxv3LiBr7/+GrfffrumIfn7779x/PhxRERE6L26du0KgPI1ABqIIJVKzW64Dblw4YJeGEdEDBlcuHBBb7mx/83ixYtRWlqKrl27onfv3pgzZ47NS1cwzmPBggWoq6sz+UAS70XDe69169YICQkx6xoSMbwnxQdM+/bt6y1Xq9V69+Dvv/+OUaNGISAgACEhIYiIiNDkCFl7r+7YsQOpqamYN28exo4dq1kudixuvfXWevfozp07NfeneO5dunSpt29j950xvL298dBDD+G3337TPETFtkMMIYr2TJ48uZ49H374IWpqalBWVoaCggIoFAqrS3eI52Nou0wmQ0xMTL3/ddu2bSGTyfSWDRs2DGPHjkVqairCw8Nx7733YsOGDUZzUY0hWJjkbQyVSoWHH34YJ06cwFdffWVxOggAPPfccwgJCTGZu2WL30rEkvsCgN4zTEQQBIsHiXHOlosTHByMNm3aNPrAzc7ORtu2bREUFKS33NBzYi9MjT7SvZnfeOMN/Pvf/8Zjjz2GV199FWFhYZBKpZg1a5bVI/HEId3/+9//sGrVKnz33XcoLy/XG4WoVqvRu3dvvPPOO0b3YXiTOQpj/5uhQ4fizJkz+Oabb7Bz5058+OGHWL58OdasWYMnnnjCCVYytiQmJgYTJ07EunXrMHfuXJPrmduQN3R/m7onG7tXz5w5g5EjRyIuLg7vvPMO2rdvD5lMhh9++AHLly+36l4VRwYnJyfjtdde0/tO3N/HH3+M1q1b19tW19tgCyZOnIj33nsPW7ZswQsvvIAtW7agR48e6Nevn549S5cu1SwzJDAw0GjytD0x9r+WSCT46quv8Oeff+K7777DTz/9hMceewxvv/02/vzzzwZrVbVs2dKokLCUadOm4fvvv8enn36qiRZYiujdeuWVV0x6tyzBHveFLqWlpfUGCDQGiy034K677sIHH3yAffv26YWoRH777TecP38eTz75pFX779ixI9RqNc6dO6fXa2yoYKg1fPXVVxgxYgT++9//6i235sLVZcKECdixYwd+/PFHbN68GUFBQbj77rs133fu3Bl//fUXRo4c2eBDrHPnzlCr1Thx4oTJRhYw/SDs2LEjcnJy6i0/deqU5ntzCAsLw9SpUzF16lRUVFRg6NCheOWVV1hseQgLFizAJ598giVLltT7TrwX//77b41HFKDBF6WlpQ6pb/fdd9+hpqYG3377rZ4XQAznWUp1dTXuv/9+hISEYMuWLfVC9J07dwZAHadRo0aZ3I947sZC7MbuO1MkJSWhc+fO2Lx5M5KTk3H8+HG9CuqiPUFBQQ3aExERgaCgIBw7dqzB4zXUXoi2x8TEaJbX1tbi3LlzDR7bkJtuugk33XQTXn/9dWzevBkTJkzAZ5991mCbERcXh3Pnzpl9DGPMmTMHGzZswIoVKzQjv61l1qxZWLFiBVJTUzXhURFb/lZNJS8vD7W1tXr3pzlwGNENmDNnDvz8/PDkk0+iqKhI77vi4mI89dRT8Pf3x5w5c6zav5jzYViobeXKldYZbAIvL696vYQvv/zSaEzcEsaMGQN/f3+8//77+PHHH3H//ffr1RF78MEHkZeXhw8++KDettXV1Zr6Y2PGjIFUKsXixYvr9d517Q4ICKg33BgA7rjjDhw8eBD79+/XLKusrMS6devQqVMns8KThv/fwMBAxMbGmh0WYFyfzp07Y+LEiVi7dm29StR33HEHAGDFihV6y0WvrCNqQIk9fN1rvqyszOJilSJPPfUUTp8+ja+//tpojlBKSgqCgoLwxhtvGM07KigoAAC0adMG/fr1w6ZNm/RCmenp6fXyIRtjwoQJyMzMxKJFiyCRSPDII49ovktISEDnzp2xbNkyVFRUmLRHKpVizJgx+O6774xWWxd/v4CAAACo12aMGjUKMpkM7777rt5v/d///hdlZWVm/a9LSkrqtaliR7GxNmPQoEE4duyY1W3L0qVLsWzZMsyfPx/PPfecVfvQRfRuffPNN/XKKtjit7IVYsX7wYMHW7Qde7bcgC5dumDTpk2YMGECevfujccffxzR0dE4f/48/vvf/6KwsBBbtmzR9MgsJSEhAWPHjsWKFStQVFSkKf1w+vRpAOaHNBrjrrvuwuLFizF16lQMHjwYR48exaeffqrXU7GGwMBAjBkzpl7uhcikSZPwxRdf4KmnnsKePXtw8803Q6VS4dSpU/jiiy80dYpiY2Px8ssv49VXX8Utt9yC+++/H3K5HIcOHUJUVBTS0tIA0O+1evVqvPbaa4iNjUVkZCRuvfVWzJ07F1u2bMHtt9+OZ599FmFhYdi0aRPOnTuH//3vf/V69Mbo0aMHhg8fjoSEBISFheHw4cP46quv9KbTYNyfl19+GR9//DFycnI0pVEAoG/fvpg8eTLWrVuH0tJSDBs2DAcPHsSmTZswZswYjBgxwu62jR49GjKZDHfffTeefPJJVFRU4IMPPkBkZKTJxH5TbN++HR999BHGjh2L7OxsvXQI8b4NCgrC6tWrMWnSJPTv3x8PP/wwIiIicPHiRWzfvh0333wz3nvvPQBUzuLOO+/EkCFD8Nhjj6G4uBgrV65Ez549jQojU0ycOBGLFy/GN998g5tvvlmv5pdUKsWHH36I22+/HT179sTUqVPRtm1b5OXlYc+ePQgKCsJ3330HgFIjdu7ciWHDhmnKyly9ehVffvkl9u3bh5CQEPTr1w9eXl5YsmQJysrKIJfLNTXM5s2bh9TUVNx222245557kJOTg/fffx8DBw40q5L9pk2b8P777+O+++5D586dUV5ejg8++ABBQUEa4W6Ke++9F6+++ir27t2L0aNH1/t+/fr1msFTujz33HP4+eef8eKLL6JLly7o3r07PvnkE711kpOT9cotmMtzzz2H5cuX46+//tKIVIC8iE39rWxFeno6OnToYFnZB4BLP7gT2dnZwvjx44U2bdoIPj4+QuvWrYXx48cLR48erbeuWN6hoKDA5He6VFZWCs8884wQFhYmBAYGCmPGjBFycnIEAMKbb76pWc9U6Qdj5QoMh4XfuHFDeP7554U2bdoIfn5+ws033yzs37/f4uHjxhCHdLdp06Ze2QZBEITa2lphyZIlQs+ePQW5XC6EhoYKCQkJQmpqqlBWVqa37vr164X4+HjNesOGDRPS09M13+fn5wt33nmn0KJFCwGAnu1nzpwRHnjgASEkJETw9fUVEhMThe+//15v/7rDng157bXXhMTERCEkJETw8/MT4uLihNdff11vCD/jPjQ0jH7y5MkCAL3SD4IgCEqlUkhNTRWio6MFHx8foX379sK8efPqDUE3dd+Zur5M2WKsrfj222+FPn36CL6+vkKnTp2EJUuWCOvXr6937zd274rHNPYyLNWwZ88eISUlRQgODhZ8fX2Fzp07C1OmTBEOHz6st97//vc/oXv37oJcLhd69OghbN26VZg8ebJZpR90GThwoABAeP/9941+n5mZKdx///1Cy5YtBblcLnTs2FF48MEHhV27dumtd+HCBeHRRx8VIiIiBLlcLsTExAjPPPOMUFNTo1nngw8+EGJiYgQvL696ZSDee+89IS4uTvDx8RFatWolPP3000JJSYneMQxLhIgcOXJEGD9+vNChQwdBLpcLkZGRwl133VXvNzNFnz59hMcff1xvWUP/MwDCpUuXNNeMqZdhmQtDGmoDxX3rln4QacpvJV6bS5cuNcsWY/eLSqUS2rRpIyxYsKDB8zOGRBBsMByB8UiysrIQHx+PTz75xG2mvWEYhmHM4+OPP8YzzzyDixcv1suTYuqzbds2PPLIIzhz5gzatGlj0bacs8UAgNE51VasWAGpVGrVdA4MwzCMazNhwgR06NABq1atcrYpbsGSJUswY8YMi4UWALBniwFA09RkZGRgxIgR8Pb2xo8//ogff/wR06dPx9q1a51tHsMwDMO4LSy2GACU9JeamooTJ06goqICHTp0wKRJk/Dyyy/bvM4NwzAMwzQnWGwxDMMwDMPYEc7ZYhiGYRiGsSMsthiGYRiGYexIs0rGUavVuHLlClq0aGGzQp0MwxhHEASUl5cjKirKrIKuDMHtFMM4Dke1U81KbF25csVpkw4zTHPl0qVLaNeunbPNcBu4nWIYx2PvdqpZia0WLVoAoB81KCjIydYQSqUSO3fuxOjRo+Hj4+Nsc8zC3Wx2N3sB97PZmL0KhQLt27fX3HeMeZjbTrnbNWIKPg/XwlPOAzDvXBzVTjUrsSW65IOCglxKbPn7+yMoKMhtLmx3s9nd7AXcz+aG7OVQmGWY20652zViCj4P18JTzgOw7Fzs3U5xIgXDMAzDMIwdYbHFMAzDMAxjR1hsMW6FWg2cPg0cOkTvarWzLWI8mV9//RV33303oqKiIJFIsG3btka3+eWXX9C/f3/I5XLExsZi48aNdreTsSNio3PgAPDTT/QuNj6GDRIAHD5cfz3DfXED1uxoVjlbjHuTmQls2gScPAncuAH4+gLduwOTJwPx8c62jvFEKisr0bdvXzz22GO4//77G13/3LlzuPPOO/HUU0/h008/xa5du/DEE0+gTZs2SElJcYDFjE0RG50DB4CLF4GaGmp42rcHoqNpnaIioLAQqKgA3noLeOghoKpKu15SEjVSQMMNmFoN5OYCZWVAcDAQGwtwyRSPgcUW4xZkZgKLF1Ob1q4dEBAAVFYCGRnAhQvAwoUsuBjbc/vtt+P22283e/01a9YgOjoab7/9NgCge/fu2LdvH5YvX85iyxWwRNCIjc7580BBAaBSUcNTUwOcPQucOAF4ewPdulHDVFtL2ykUgJ8fUFcHXLpEwuqvv+i7ujrjDdiDD5KgMybE+vZlEeYBsNhiXB61mjqEhYXU/oiDRoKC6PPJk8BHH1GbxG0Q40z279+PUaNG6S1LSUnBrFmznGMQo8US17jY6BQUAEolCa3QUPrOz49EFADI5RQO9PbWNkxeXiTIIiOB0lISWDk59P0dd2gbKbEBO3QIWLAAaNOGPGG6Quyvv2h5URHZLJfT5+RkIDGRhZcbwWKLcXlyc6l9bNdO256JSCS0/MQJWq9rV+fYyDAAkJ+fj1atWukta9WqFRQKBaqrq+Hn51dvm5qaGtTU1Gg+KxQKADRsXalUmjyW+F1D67gDNjsP0Wt18iR97t5dK0ays4ElS0i0REVpBc3Ro0BaGvDSS0CfPtp95ebSKyICuHYNCAsDxNIBtbUk1ADaT2kpEBSkPY/AQAojSqW0XXk5CTBvb1oeEqI9jiCQkKuqAjp21H4nl9P57NtH4ctBg0jA5eSQzbt2AZ07AwMGAI88AvToAezdS7a2agUMG0bHswJPua4A887FUefJYotxecrKqFMXEGD8e39/4MoVWo9h3I20tDSkpqbWW75z5074+/s3un16ero9zHI4NjsPURSJgklkwgTT21y+TC9dpk2z6vDp775r2QYNhZcfeaTx7XVtDwkhUbZzp2U2GMFTriug4XOpqqpyiA0sthiXJziYOpKVleR5N0TMRQ0OdrxtDKNL69atce3aNb1l165dQ1BQkFGvFgDMmzcPs2fP1nwWK1qPHj260aKm6enpSE5Oduvikxafh1pNOVMKBTUIFRUUhjt2jLxJgYHk8i4vp3VjYmh5hw7GGxCFAigpAZYuJU8YQCJtzhwSbseOATKZvmdL/B8HBZFnKzSUzuO995D87LPwqaoirxhADZRKRZ6mwYP1PVsFBcCff9L+b7pJ24iVlVEOl7c39TT9/Gg/wcF0bkol2dGmDYUaBYEaQZVK+5LL6Rx0ri27/D9cGHPORfQk2xsWW4zLExtLEYGMDP2cLYDamMuXyZsutpMM4ywGDRqEH374QW9Zeno6Bg0aZHIbuVwOuVxeb7mPj49ZDztz13N1GjwPMUR48CCQng5cvUoeHLkcuH4dyMujUJ045YogkOgqKQFOnSIxExtL+VOGyGQkzCoqtIKqWzda//BhOkZRkVYkCQIJIICO6eVFQuifhsmnogI+gkA2l5aS6Coqos/+/vVtKC+n/C7d76qqyB4/P6C6ml4BAdokfEEgkVhQQL1QtZoEmEymbSBLSihEeuutlJO2axeQnw+0bg2MHNlomNFTriug4XNx1Dmy2GJcHqmUclgvXNDmbvn7U3t0+TIQHg48+ijniTK2p6KiArk6oahz584hKysLYWFh6NChA+bNm4e8vDx89NFHAICnnnoK7733Hl588UU89thj2L17N7744gts377dWafg/uiWX/j7b/LaREZSz6uujrxctbXUEBgmdQYGkuCpqiKhYWyiYWOucd1Gp7KSBFVJCQmvmhra740bZEu3biT2RCGkUpFIKi0lQSOOWARI+Ok2YFeuUF6XYbhYJqNjlpeTXRUV+uKoro6OU11N5yyR0PdiI+jtTTZUVgJPP03riqUrvL3JI7ZgQcOhVcam8OOJcQvi46m8Q0ICUFxMndziYvJocdkHxl4cPnwY8fHxiP/nAps9ezbi4+OxcOFCAMDVq1dx8eJFzfrR0dHYvn070tPT0bdvX7z99tv48MMPueyDtYjlFw4fphve25vEiUJB4bOqKvJGqdX0tyGi2PHxIUEkCPrfi67xHj3qu8bFRmfYMBol6OVF4sXbm0KTd91F+VYtWpDQE0OUQUHkZfL2pu2GDwfeeYdehg3YwIHAa68BnTpRT1Kh0Hq3RO9YdDTZr+sRq6zUJtGr1VrBpYuXF32XlUUir7ZW65X7+29g+nTyfDEOgT1bjNsQH9+0kjNi8WYuV8OYy/DhwyEYPqB1MFYdfvjw4cjMzLSjVR6Mbh2sFi2AjRup5ktUFHmZAgLI6yOTkecoL49EjZcXiYjaWvpOpK6OREjbtiSILHWN6zY6JSV0zJAQKgOhm99VVkb7zM0FPv+cRJPueuK+TTVgcXHashRXrpCnbfhw+ru6mvYtjgASPXFRURRGVKu1njBdVCpthXqZTJs3JorTykoaoTlqFIk+xq6w2GLcCqnU+vIO8+dTritXn2cYFyQ7G/j4Y20dLJWK6ll1706eoro6/XCbWL4hKIjCbWJoTZeKChIhw4YBkyZp9y8KmgEDSGg11Ag01uiI3ymVJKQGDNDmfpm7L1M9yb/+0oZQi4rIGyaGUOVy8vwplfWFllqtzSuTSumzr69+LTAxzJiWBnz1Ffc87QyLLcbjyc6m96wsaqe4+jzDuCBLllDiu1hh/dIl8iSdPEk5T97eJKhEz5W3N4mr9u1JoJSUkEdJFA3iaETdXlV8vOtWYzcmxHRFmO7ggJISEk9Dh1J9rZoa2t7Li36T2lr6rFLRu27ivIgo0E6f5iKFDoDFFuPRqNXA5s3UJnXrpu34cvV5hnEiYkz/2DH6Wy6vP0VEaCiFEquqSGAEB9M6Pj60Tl0dCa6ICAoThoZSyK2khLYPCgKGDAFeeEHbm2qKa9xZiDZ37Up1twzF4pIlwJtvanPWpFISqwEBNChAIjHeuKnVtFwiMV6kMDeXPIOuJkrdFBZbjEeTm0tFl4cO5erzDON01Grg22+B1aupEnp1NYmnDz6g/KvQUMqhAkgshYaSYCgpoST2ykrydonu6eBg2q5TJxpdFxBAAg4AevWim9qTRIIxsThvHuVdvfEGJb5LJFTeoUcP+q0vXaLfXTfUKAjkDQsIoJGJuiMxxVDAnDnkHeScC5vAYovxaMrKqE0xBVefZxgHkZlJhUN//JE8JhIJ5Q2JU99cu0YP90GDtGUcunal0GBpKa3Tty+5o69fJ/EQFkbJ3bp5V3FxTjk9pzJwIPC//9X3eiUm0qjDykr6rcURimIJiMhIoGdPbbJ/ZiZ5yiZMIKHbujXnXNgIFluMRxMcTBEKU3D1eYZxAJmZQGoq5R3duKEtyaBUasNfYjjr9GmgZUv6HB6ujfdXVZFI6NiRBNmoUTwZsy7GvF4TJlB9LWNhxshI8n6JIzHFybeLimi9oCAK1XLOhU1gscV4NLGx2nqCpkrscPV5hrEj4kP88mVKmhQLcIrV18XyBOK6BQXkzQoOppu0shK47z7gqae0RT5ZYJmPsTBjmzbk0dL1CIqTeEdF1d+Hbs7F6dP027viIAMXhsUW49FIpZRTevky5W6Js2Jw9XmGcRDiQ7xlS+D8eVom3nCi8AK0I+lqarSV0cWbdPLk5hketBWmwoy6DV9ZGXkdAwKM78Pfn4RWaiqVoOAaOhbBYovxePr0oTa7Xz/KnbWkxA7DME1EfIhHRmqFlW7CtvjA9/cnkQVop7Hhm9R2NDYSMziYGsbKSuPf5+VRsr1EAnTpwjV0LITFFtNseOMNahPY+80wDkR8iHt5kZdKLEAqlh0Qw4hqNa2bnAz861/1q68z9iU2lrxUR4/W/06tpuU+PiSoxP8J53OZDf8qTLNB7NgNHGj7EeFi2aBDh+hdNw2FYZo14kM8L49uvOBg7Yg4lYqS5AHKz+rWDZg1C0hK8ryyDa6OOPl2y5b0WZynUaHQVqrv3bv+/8Swhg5jFPZsMUwTyczUTmvGaQxMs0N3PkNjLmPxIX7hAiW/9+lDPZJr1yh5UpyCZ9QoElp80ziP+HjgpZco76KkRFtnq0sXEsPt2hnfjmvoNAqLLYZpApmZwOLFVNhanGWE0xiYZoO5PY34eLoZxHXbtaMRcaGhwIgRtM66dQ3XaWEcg5jkunSptoK8Wg0895x2LkpDuIZOo7DYYhgrEUe0G84ywmkMTLPA0p6GqcmWVSrghx/4JnE1YmO1E2qLc0xmZOg3dgDX0DETvroZxkrEEe3t2jU+FRDDeBSGPY2gIEqAF3sahYXU0zBMXrRn4iRjP8RQcHg4NXq6+VwnT3INHTPgX4ZhrMScsjQ3bnAaA+OBcE+j+SGGghMSqM5Wbi69DxjA+RJmwGFEhrES3bI0nMbANCvM6WlwwrTnYSoUzB6tRmGxxTBWIo5o5zQGxuMxHHHYogX3NJorjRVHZYziNmIrLS0NW7duxalTp+Dn54fBgwdjyZIl6CZOfMcwDkZ3RLsYUeGpgBiPw9iIw7g4qsd06RL3NBjGDNzmMbB3714888wz+PPPP5Geng6lUonRo0ej0tTUAgzjADiNgfFoMjKAF18EfvmFRqbFxtI0OkeOAFev0vQ7nDDNWEIzrQDtNp6tHTt26H3euHEjIiMjkZGRgaFDhzrJKobhNAbGQ8nIAJ54giaPlsuBoiIgJIRCSGJtkw4dSFidOsWTjjKNI3pJT5ygoqleXjRrwLPPUo/Vg3EbsWVI2T+Jl2FhYSbXqampQU1NjeazQqEAACiVSijFKSKcjGiHq9hjDu5ms6PsjY7W/q1S0csYajVw9iw5AYKCgJiY+sLME35jd7GdMUJmJjB3LgmtoCDAz488V4WFVOiyf3+KmxcWAv/+N13A3NNgGkKsy3b+POVaVFTQFEA5OcC+fcBrrwHjxzvbSrvhlmJLrVZj1qxZuPnmm9GrVy+T66WlpSE1NbXe8p07d8JfnCLCRUhPT3e2CRbjbja7or35+eRJN4Ur2twQuvZWVVU50RLGasQaWgUF5Kny9aWcLB8f8myVltJFO3Ag5XCVl9PfDGMK8Zo6f548WuJI1sBAEvEFBcCCBeQ19VAPl1uKrWeeeQbHjh3Dvn37Glxv3rx5mD17tuazQqFA+/btMXr0aAQZG0HjBJRKJdLT05GcnAwfsVqvi9NUm83x7NgSV/mNs7OBJUsoGhMVpS24feUK5Rq/9BLNlOFKNpuLMXtFTzLjRqjVQHo68Oef5KUqLKSHoUymXScggATX9es84pAxj9xcCh1WVZHQCgnRDqqQyYCICEp2XbkSWL/eIz2jbie2ZsyYge+//x6//vor2pmaFPMf5HI55Ebm2vLx8XG5B5gr2tQY1tjszEmbnfkbq9XAxx9TTrE4eEsQaPRi5870e3zyCfD22/rtjLtdF7r2upPdDLQ3559/0gUZGAjU1ADV1fQwFB+O3t4UArp8meY15BGHTGOUlZFHq6KCxLphIVwfH7quTp0iYeaBpSXcRj4KgoAZM2bg66+/xu7duxGtmyDDuAViyD4jgwY0delC7xkZtDwz09kW2g8uuM24NLo3Z3g41dHy/qcvXl1NYZ7aWuo1VFeTCIuI4BGHjHkEB1MyvFKpva50qaujQRgqlccWwnWbu+SZZ57BJ598gs2bN6NFixbIz89Hfn4+qqurnW0aYwbWTqXmKfDUPozLYnhzRkUBoaEkrsLD6eKUSEhgKRT0io4G3nyTRxwy5hEbS6MOlUoSVoZUVlLjGBrqsWFptxFbq1evRllZGYYPH442bdpoXp9//rmzTWPMoLl7dnSn9jEGF9xmnIbhzSmRUBjH15fUf0AAeR06dwZataLSDh984LGJzIwdkEqpvENYmNZLKggkvkpL6fry9wd69vTYsLTbiC1BEIy+pkyZ4mzTGDNo7p4dcWqfy5epjdFFLLjdo4fHtjOMK2Ps5gwPp/IO4eEU2qmoIM/WiBHAW2+x0GIsJyGByjuIyfBFRXTdBQWRR6tTJ48OS7tdgjzjnjT3SZt5ah/GZTF1c4aH0zDZK1coxJiaCiQn80XKWM/48eQ1ffddqq+lUpHQ6tnT4wvhsthiHAJP2qyd2kccjckFtxmXoKGbEyDP16BBLLQY25CQAGzY0PCUG4YTn3tAoVwWW4xDYM8OwVP7MC4H35yMo5FKTZd3cGZ9IDvCYotxGOzZIRpqZxjGKfDNybgCYgmSwkIS/WLl54wM6gwsXOi21yKLLcahsGeHYVwUvjkZZ2JYgkQMZ4v1gU6epPpAffu65TXJYotxOI7y7Ihh/5IS7WeGYRqA3a6Ms7CkPpAbXqMsthirUatpPlpX7ATrhv3VamDGDGD+fGDiRMu80B6Yp8kwDON6mFMf6MoVt60PxGKLsZr584Fjx1wvh9Ew7C+Wk8jKogmwzQ37W5unyQLNs1i1ahWWLl2K/Px89O3bFytXrkRiYqLRdTdu3IipU6fqLZPL5bhx44YjTGUY98VUCRJB0M6tqFLRVFJuCIstxmKys+k9KwuIjHStHEZjYX8vL/quWzfg6FHzwv7W5ml66ECaZsvnn3+O2bNnY82aNUhKSsKKFSuQkpKCnJwcREZGGt0mKCgIOTk5ms8Sw5CIM+AeAOPqGCtBUlhI4ZOSEqC8HAgJAVavBqZMcbsGle82xiLUamDzZvq7WzfXm+PQFtMCWTuPY3OeaNtTeeeddzBt2jRMnToVPXr0wJo1a+Dv74/169eb3EYikaB169aaV6tWrRxosREyM4HZs4GZM4EXXqD32bP5gmRcC7EESXg4NeIXLgCHDwP5+TR7QWgoNcBHjrhlg8qeLcYicnOp8O/Qoa6Zw2gs7C9Oj1NQQAKpurrhsL81eZoePpCmWVJbW4uMjAzMmzdPs0wqlWLUqFHYv3+/ye0qKirQsWNHqNVq9O/fH2+88QZ69uxpcv2amhrU1NRoPisUCgCAUqmEUqk0uZ34XUPrIDsbWLKEpkaJitK6aI8eBdLSgJdeAvr0Mb29AzDrPNwAPg8b0KsXsGAB8OmnwPbt1JgHB5NHKzaWZjQQBHoIffopzXHWQINqzrk46jxZbDEWUVZGnQxTODuH0TDsX1hIHaSUFODQIbp3ZTIgLw8YOND4PqzJ0/TwgTTNksLCQqhUqnqeqVatWuHUqVNGt+nWrRvWr1+PPn36oKysDMuWLcPgwYNx/PhxtGvXzug2aWlpSE1Nrbd8586d8Pf3b9TO9PT0hleYMMH0d5cv08sFaPQ83AQ+DxswbBi9TJGSQu87dpi1u4bOpaqqyhLLrIbFFmMRwcE0QbspnD3HoW7YPyKCPM2iZ6tFC5pPVyoFNm4EOnY0Hva3Zh5HDx9Iw5jJoEGDMGjQIM3nwYMHo3v37li7di1effVVo9vMmzcPs2fP1nxWKBRo3749Ro8ejSBjF+A/KJVKpKenIzk5GT4+PvVXyM0F5syh8Iux/SgUlAuzdKlT58lq9DzcBD4PG3LkCHm4OnfWJt2KCAJQWkqjnV54Abj7bpPeLXPORfQk2xsWW4xFxMZSrhagFTEirjDHoRj2P38eOHAAqK0F2rSh78rLgcBAElgFBabDetbM42iuQGvRwnXLZTD6hIeHw8vLC9euXdNbfu3aNbRu3dqsffj4+CA+Ph65DSQJyuVyyI30YHx8fMx62Jlcr6KCLvrWrYG6uvrfy2T0fUUF4ALiwNzzdXX4PGxAaCg1jGVl+g2qmDBfWEi92xUrgH37Gh2B1NC5OOocuZlnLEIqBR55hP7OyaHOcV0dvZ886RrTqMXH02AVb296lZfT8pYtgf79yePVUKK8YZ6mOecoCrTLl02L0PBwGkjDecrugUwmQ0JCAnbt2qVZplarsWvXLj3vVUOoVCocPXoUbUTF70hatKCh8hcvkifA8MJ0thuaYUxhrEEtLCSPV0EBNcht21JD7iYjkNizxVhMnz50D/TrR3W2XHEatbZtKUzYqpVWFCUm0rMHqB/WMxwZ37evZVPFNTaXr7c37ePiRY+b8sujmT17NiZPnowBAwYgMTERK1asQGVlpaaW1qOPPoq2bdsiLS0NALB48WLcdNNNiI2NRWlpKZYuXYoLFy7giSeecKzhmZnAhg0UalEoyKUbEUEJg+HhruGGZhhTGDaobdsCp06RF9bLi67nbt2osQ4KcosRSCy2GKt54w26F1wxJBYcDPj5kcgJC6NluuFA3U59Q7Wx3nnH/PJEpubyTUgArl8HLl3ikYruxkMPPYSCggIsXLgQ+fn56NevH3bs2KFJmr948SKkOv+0kpISTJs2Dfn5+QgNDUVCQgL++OMP9OjRw3FGi6UeTp+mOLpSCRQX00VcWEgXWmWla7ihGcYUug3q4cPaBjUigurqtGxJHtvaWmpMjx936RFILLYYq3HladR0865CQ/W/0+3Ul5cDr71mu0nmjc3lq1YDzz3HIxXdlRkzZmDGjBlGv/vll1/0Pi9fvhzLly93gFUmUKsp4T07m/KwQkLIC1BaSj2MwkLgr7+Ahx7iSruM6yM2qFu3Uu++c2dq0IuKgP376bquq6OHkUQCHDzoso0od2kYj0Q370os5m2YdzVxIvDxx5YXLzXn2F27UmmJrl1J0DU2UvHGDR6pyNiA06eB33+nizA0lASXnx+NEmnfni5CqRSYPp2FFuMeSKWUuxIRQaGKoiJt7pZMRg22tzf1kjdtctncLRZbjMcieqH79aPPZ89SNGXAAFreokXTq82bg+5IRWNwnjJjM44dox6FsSGxMhnF1Kuq6MJmGHdBDFVcukQdiupq8trKZCTGamspr6u62rlTmDQAhxEZjyY+nooM79hB4cLQUG3elVjk1N61sawpJcEwTcJw5CHDuDNiqOL4cXqJnQmlknqxvr6UMC+TuWxOBnu2GI9HzP/t35/uP/GzozxO1pSSYBir6NWLHkTl5cZrkJSX0/e9ejnHPoaxlvh4akj9/WlYuTidSXg4NeyCQA1rY/OxOQn2bDHNFkd6nEyNVHSlchmMB9C1K3DzzcBPP1HycEAA5bPU1VGvQq0GhgxxuV4/w5hFYiLQsyd5sGQyuqavXCFvlli4t7H52JwEiy2m2dJYbSxbe5yMjVR0pXIZjJsjFou74w7gzBnKb6mqop6DREKjP7p3p4q6fNEx7khsLOWFiPOxnT6tzQXx96ekXN352FzIg8tii2nWONrj5MrlMhg3JjOTHjAZGdq4eGQkvUul9CAaMIDLPTDujbH52MLCyKtVVlZ/PrY333S2xRpYbDEuhWEld0d4ftjjxLg12dnkrTpxgnJYAAodymRU7mHmTAq/8EXNeALifGyZmfTAUCjoeo+MpGKn4eGAXE73w9mzzrZWA4stxmVoqJK7vTvj7HFi3JbUVOCPP2hklph4KJVS2LCqCvjxR5rQlIUW4ynozsdWV0cdi+Bg7fUvDiVXKJxrpw4sthiXIDOT5hK1RSV3Q+9Yx472tZ1hnMpvv1E4RRRYAI3WUqvpffduym2Ji3OunQxjK4zNx6aLOJQ8KAjIz3e8fUZwq67Or7/+irvvvhtRUVGQSCTYtm2bs01ibIBaTR4tW1RyF6eFmzmTIiszZwLz51tv1+nTVI/r9GmXrJPHNGfEC7K2lnr03t70rvu3Wg2UlABHjzrXVoaxJeJQ8suXjZc4uXyZEuljYpxjnxHcyrNVWVmJvn374rHHHsP999/vbHMYG5Gba34l94ZCfaa8Y1lZwNChlNqSkGBeXpgzQ5oMYxa6+SiiR0sXqZQ8W3V1wNWrjrOLYeyNo4eS2wC3Elu33347br/9dmebwdiYsrKmV3I39I6Joi0oSDsR9ZYt9P7xxw2LKFuGNK1FdFocOaJf9Z5hNIj5KKIHS/Rq6SIIJMRat3a8fQxjT4wNJZfLabLqUaOo4XahcIRbiS3GM9Gt5G5sSjdzKrk35h0DKBz4118kskyJqL59TYu27t3pGB99ROvZS/xkZgKffELeuAUL6DjsVWPqIeajyGRUNVut1r8oVSp6DwmhiXwZxtPQHUp+8CCQnk6i64MPqFfdqxc1pC6AR4utmpoa1IhDoQEo/ukJKpVKKJVKZ5mlh2iHq9hjDra2Waw9l5VFXhzDSu7Xr9M91bEjDbgyRkkJPWuCg+tHVLy8aKPCQiUCAymUKB5DLqdj5uQAn35Kz63cXKBTJ8DHp/5xOnUC/v6b1rfHXIbZ2cCSJUB5uRJDhwLduilRVkYpN2lpwEsvueZz09g14U7XtFsSE0PJhAEB2ul5dHvygkAXcXIyD7VlPBeplHrN//sf1dcKCaHkeZWKeq66OSROxKPFVlpaGlJTU+st37lzJ/z9/Z1gkWnS09OdbYLF2NLmoUNNd0BSUuh9x46G9zFjRsPfv/GGaXvFY+TmAtOmNbwfgJ5xp083vp41TJig/Ts5Wd/my5fp5aroXhNVVVVOtKQZIHqxevSgkGJNDQks8SWTAb17c8V4xrMRc0jOn6fe+IULlKco1t4CKIckPt6p94FHi6158+Zh9uzZms8KhQLt27fH6NGjEWQsXuUElEol0tPTkZycDB9jrhQXxF42f/UV8J//kJhQKqlT3q4d8NxzwAMPNLytWk2jDrOyaPJ3Xe+YVKrEqFHpmD8/GUOG+MDbyFVfV0f5xk8+CWzYQMeWyehdt3yLQkFetKVLbe/Zys0F5swhT1toqBIjR6Zj165kqFQ+dj92UzF2TShcqMaNR/P00/R+9Cj18AUBaNECGD6chBbHnhlPJjeXqskXFJA3KyCAXnV1NH0PQDkkjY2wsjMeLbbkcjnkcnm95T4+PiZFgjMqmDdmk6tiS5szM4EvvqDOSL9+FApUqWgu3S++oBJBjT0zJk4kwXT0qP7glOvXKV8yPNwHCoWP0bwwhYL+z1IphfzPn6c8MW9v8kp37Qq0bEnLBwwgQWfr66KigqJBrVtr021UKh/U1dFvLJPR9xUVxkOcroDuNeFu17PbkZ1N7x9+SDlb0dF0sQ4fDtx9N1207NFiPJ2SEuDiRWo0xdFQADWSgYH0d14eredE3EpsVVRUIDc3V/P53LlzyMrKQlhYGDp06NDk/fNwf/Ow9Ug53ZGEPXroe6WiosxPSjc1z6H4vxswgHIodRPfAW1Zlg4daNuaGrpPRe9aQQHdpxERlLNlrxHFugMFjPQRzBoowDQTMjMpuW/CBLoJW7emC+fyZaomP2oUCy2meVBaSo22qeHsAD3QS0sdZZFR3EpsHT58GCNGjNB8FkOEkydPxsaNG5u0b1cY7u8O2GOknK3qbAHG5zns2JHyvR55hDxfxsqytGxJHufsbDqmIND9W1NDwqemhsL/CxbY7zoQ6/RlZOh30ACy59IlWqekhPLFmiJyneXBZWyAbu8EoIuztpb+kY4aMsswrkJICPVCa2ooMd6wJw3Q9yEhzrBOg1uJreHDh0MwrBZrAxqq0cRtlxZRkCoUJLY6d6aHdVMFqS3qbOliOM+hOCiuTx/jnq8BA2i+3jffpHszOJi8z0olnauPD52rnx+lwtgL3Tp9OTmUtF9XRzacOkXvKhXw4otN87qyB9fNEXNUKivp86FDdKGI8W5LeicM4+6EhlIDfukSea8CAij/o66OwusAzaVo2IN1MG4ltuyFLT0rnoquIO3dm5bpTqnTFEFqTZ0taz0zxjxfMTGU71VTQ9EYcT8yGRUiLi0lr1fLluYLPmsRQ6GffEKfz56lHC2Fgn6bTp2a5nVlD64HcPAg1R8R81FkMlLhBQV0sfTtSyra3hcrw7gCsbFAUhJd83V11GBXVZHgatWK1hk40OmjilhswfaeFU/EnoJUN3xmKp9qwADtvdJUz4yh5+v0aTqGv792AnldAgJIbIWGOiZfKj6ectd27CBhtGYNCVvdfDZrRC57cD0AtZoKN9bUaAvKFRZqEw3r6ugf2bEjJ/cxzQPdkEBBASXfiiOsRM/W+PFOb9S4SYW+Z8UYzSUxuaGJl80RpNZ2psV7JTycnhMKhTZ8dvKk/jRXomcmI4Mme+/Shd4zMmh5Zqblxy8ro323bKkdOa+LlxddAx06OK5zJLYLISFUJLx9+8ZFbmNYIpgZFyU3l25OQaAbDiCRJZWS4Kqupp5hVJTTe/IM4zDEkMCAAfTwKCuj9/796XsXqATNni1Y7lnxRBrzFtl7pJypkYQDBpDQio+3n2cmOJjysdq1o/MzDPsrFHTODz7o+M6RQmE7ryt7cD0AcZi7XK71bNXV0bu3N4ktb2/g1lud3pNnGIfS0OgoF4DFFtxyAnGbYk4eT9++DY+Ua0yQmpNjZexe0V3PXqFMXbEdH0/pMGLY38uLworDhwP33GP+Pm1FUFDT540UscUclIyTEYe5y2Ra1/ONGySypFJtJV4xV4VhmhOmRke5ACy2/sEcz4onYq636O23TY+Ua0yQWpJjZXiv6GIvz4xhyL9nT+25FReTiHPWjCcxMbbzurIH1wMICaELsbhYeyP4+9M/UKWiCzcw0OnD3BnGqYi9e7GQqW5OjJNgsaVDY54VT8QSb5GxkXJSacOC1Jaj3+zpmTEU26IovOUW54ptW3pdm7sH1yMwNtO6VErL1GoSXVIpuyeZ5otu716tpklz58+nIedO9Jqw2DKgIc+KJ2Kpt0h3pNxrrzVcQd7SHKvGQo329sy4qti2pde1uXpwPQqZjG5MX1/6rFSSV8vPTzsBNcM0Rwx792KnIyuLvANOrG3DYquZY423SBQf/fs3PEefuV6z06epaOdnn5FgkkrpuWEYanSEZ8ZVxbYthaCrikrGDMrL6ULXvaHCwylnq7aWEufDw2k9hmlOGOvdi17gbt1o0lwn1rZhsdXMMcdb1L8/cOYMsH8/Ff0cOtS8fZvjNTt9Gpg1i+ZZrKmhZS1bkpAyFmpszp4ZWwpBa/bFU/y4AMHBJKbCw+niB6i3IQg0eWfbttppEBimOeHi1clZbDVzGvMWVVdTyPDTT7UTM8fGAvPmNb7vxrxmeXk0w8Lly/R8aN2a8nsLC2mb+HhKWDfsjLBnxvHwFD8uQmws9Ub27tWGEaVS8mjFxtLNw6McmOaIi9e24ccTo/EWJSTQIKfcXHoPDATOnweuXqXrt1Urej97lrb76quG9yt6zUQxpYtaTV5diYTKAgUHa0euh4SQyMvNpY66sUKbomdm4EB6Z6FlP+xRSJaxkr/+ohvyxg0KGwL0ECkpAX79lW4mHuXANEdcvDo5e7YYAPW9RQEBwIQJ1J5HRWm9sv7+2lHl//kPMG4cte/GaMhr9vff5Cnr3JkEne4+JBI6fkkJeboaq0zvKeEt3fMQp71zNuYOcnjzTefa2SwQ/xl1dcCwYVTcFKAbRC6nHk1UFN3IDNPccPHaNhaJrffffx9bt25FWFgYnnzySYwcOVLzXWFhIRITE3FWdHswbofoLVKrgQ0byIPVooXx8DdA1+6uXVRzyxSmcqy6dKHrv1072k9dnX6yvbc3iTKFouHOSFPDW64i1AzPo0ULYNo0IDubPI7Owtw0CHvd9tzm6KD7zwgKorg7QO5diYRehYVOy0lhGKdirHcv5q/k5Di9to3ZR3333XcxZ84cxMXFQS6X44477kBaWprme5VKhQsXLtjFSMZxZGYCs2cDy5eTN7akBLh2TTsNmy5KJc3b1xjx8cA77wArVwLLltH7okV07Xt5kafM0PNbV0ffFRdTqQnDzohaDWzbBvzf/wH79lEJCkvDW+K5zpxJRUtnzACmTqU6YoZzQ9oTY2E6sUr/kiXODdOZOyemQmH7Y7tKm7Nq1Sp06tQJvr6+SEpKwsGDBxtc/8svv0RcXBx8fX3Ru3dv/PDDD7YxxPCfIarfiAi6iQICrJ+glGE8AcOcGLEjJi53hzpba9euxQcffIBHHnkEAPD0009jzJgxqK6uxuLFi+1mIOM4dEuUtGxJYkciofZbqSRxJObkAuSJEjvXjWE4+k2t1np8u3QBKiq0cxJ6edHDWyajzolhZyQzE9i4kcRWaSl5gWpqaP/h4UBcHK3z1lsk6qKjGz7Xdu0oR+zkSZqEe/t2sikpyf4J4Go1nculSzTZtFiTUuyQFRU5dbSy2aVBgoLME96W4Aptzueff47Zs2djzZo1SEpKwooVK5CSkoKcnBxERkbWW/+PP/7A+PHjkZaWhrvuugubN2/GmDFjcOTIEfTq1atpxuj+M1q00IqqsjJtfJ7nW2KaO7o5MSUl1DC9/rrxSX0diNnN97lz5zB48GDN58GDB2P37t1Yt24d5pkzNI1xaQxzc7p2pfZcHIGoUlGbXltLwgQgkaIT1bEI0eMbHk4jDrt1I69OZSXdGxIJzUe4aJG+2BFF0u+/ky1hYbSvq1eBAwcoF+zPP2mk444dwBNPUPHghs61tpbyjhUK2p+3N3WKDh9u2EOmVpMH7NAh6z1h335LojEvj+z+/XcqsVFURN9HRRkfIOAoGhrkIKZB9OhB0wrZGldoc9555x1MmzYNU6dORY8ePbBmzRr4+/tj/fr1Rtf/z3/+g9tuuw1z5sxB9+7d8eqrr6J///547733mm6M+M84dQr44w+64AF6/+MPWm7MDcwwzQ2xd9+/v/azkzHbsxUeHo5Lly6hU6dOmmW9evXC7t27ceutt+KKWPOFcUsMc3MkEuoc7N+vneNWoSAxJOZWRUTQiEJrPT+G+VwtW1L4rEMH4MEHaeJn3XtEVyS1b0+J9eKgLEEgMXj9OnlZgoJoWUAAFQ8eOlSb/6R7rgAJpRs39KeTq6oioXP1qnHPki1KIWRmAitWkHcuPJx+V7H0RVYWMH68NkznrMiQM6f4cXabU1tbi4yMDD1hJ5VKMWrUKOzfv9/oNvv378fs2bP1lqWkpGDbtm1NN0gqJXfrF1+QK7hNG+3yixdpVEVioks8WBiG0cdssTVkyBBs3boVt9xyi97yHj16YNeuXRgxYoTNjWMch7HcHLGDnJFBBanVahJhYpSiqoo8P00JhVtSM0tXJBUVkfATZyeRSEio1NWRXX5+JF5CQ8lbBQBbttDxdM9VodCGL0W8vWl5QQHZc/y4fs6xLeZ7FIVjRQV5EAE6Bx8fEn1ijlxlpfMjQ+YUklUqbX9cZ7c5hYWFUKlUaNWqld7yVq1a4dSpU0a3yc/PN7p+fgMx1pqaGtTU1Gg+K/5JgFMqlVDq/rBqNblbO3QA6uqg/OciUfr40MXp7U0X4f33u5XgEs9RaY+LyIHwebge5pyLo87TbLE1d+5cZGRkGP2uZ8+e2L17N75qrPAS47KYys3p3JnCeuI0On36UKQCsGwGhIZG/ZlbzVwUSf7+wLFj2vkUxXdBIMGiVlMYMCaGzkXMIz51imzQPdfaWhJooti6cYO2ramhY4gj6g8e1I7UtGS+R1OIwjE2lo5VUEBCS7fEBkD5nUlJzo8MOaOQbHNpc9LS0pCamlpv+c6dO+EvXggiQ4fWm8Ih/fXX9dfZscPWJjqE9PR0Z5tgE/g8XI+GzqWqqsohNpgttr788kssWrTI5PdBQUH4/fffbWIUY1+MCR9TJUoUClpPJqOoRbdu2gesuTMg2Kr6uCiS8vPJGxUaqs0jE0UWQOJIrdavDwaQqCkro1CieK5RUeQQqKujvLSCAtqfvz95xMSRdps2AT17kiizxYwQonAMDKT1dAcI6NYcCwx0nRqVjp430tltTnh4OLy8vHDt2jW95deuXUNrEyNDWrdubdH6ADBv3jy90KNCoUD79u0xevRoBOn2fI4cARYsoB6QlxeUXl5IHzkSybt2wUeloov47FmaIV7MVXEDlEol0tPTkZycDJ+GJlt1cfg8XA9zzkVhj6HURjBbbG3atAnbt2/HRx99VG9Uzdq1azFnzhzcfPPNNjeQaRxLakU1JHyM5eaUlFAIUSytYCgwGpsBwRYhNxFREP7yCz1XgoLIG1RWRnll4kg+f38SLIblCuRybaV68Vzz8mj90lJt/pdcTucrkZBAE0crfvQR8PDDls8IYez/o+tdCw+nZ+Pp02SHOKgMAP71L9eaDseRdcmc3ebIZDIkJCRg165dGDNmDABArVZj165dmDFjhtFtBg0ahF27dmHWrFmaZenp6Rg0aJDJ48jlcsiNjJTy8fHRf0CEhtKPXVam5372UangU1dHvQKplNZzw4dkvfN1U/g8XI+GzsVR52i22Dp27BhmzJiBAQMGYNGiRXjppZdw+fJlPPbYYzh06BCWLVuG6dOn29NWxgiWeI3MET6GuTkqFeUQxcWRKDCkqorESXExjcrTfQDbKuQmIoqk48fJXpmMcrPEZ5A43U9oKOUQyWS0nTiKLi5OG47TzUM6cIAS60WRI45wLC2l/XftSvs6cYKWmVMKQcyxMvX/mTRJ35MYHk4DBERPnegcuf32xn8XR9HQtdbUqgbGcIU2Z/bs2Zg8eTIGDBiAxMRErFixApWVlZg6dSoA4NFHH0Xbtm019b+ee+45DBs2DG+//TbuvPNOfPbZZzh8+DDWrVvXdGMM3c+6uECFbIZhTGO22AoKCsJHH32EsWPH4sknn8Tnn3+Oc+fOITExEdnZ2ejYsaM97WSMYInXyFzh8/bbVIBU9F60aAGsXk0RDDEnSkQQKA8KoO1qavQfwLYKuekSH09TwzzxBI1GrKmhTnybNkBkJHDmDJ1jmzbkZVIoSEilpNDoPl1Rp5uH9NlnwKpVVOOrtpZ+r8hI8uaFh5Mn7coVEp7mzgjR2P/nwQfrexKlUu36gGuED4HGz2XBAtsf0xXanIceeggFBQVYuHAh8vPz0a9fP+zYsUOTBH/x4kVIdf5JgwcPxubNm7FgwQLMnz8fXbp0wbZt25peYwuoPzRUHKWpUNDN4OQK2QzDmMbiuRFvuukm9O7dG7t27UJAQAAWLFjAQssJWOo1MnfaFVH46IqfKVNoZLnhDAhHjpAHplUr8soYPoDHjm085JaXRyUZLAlLJSQAH34IzJ1LOVbt2pEwqq4mD5uXF3mnzpwh8ScKzj596u9LzEN6+GGqRO/jQ14smYzsEX8r0WMVGmpeKQSg8f/PwYMkUj7+uP4ov4kTaX+ugDnX2pYtgMGgQZvh7DZnxowZJsOGv/zyS71l48aNw7hx4+xjjK5LViy+VlKiPzSUYRiXwyKxtWXLFsyYMQP9+vXDyZMn8d///hejR4/Gv/71L6SlpcFXt7w4Y1eMiSdBoE5ubW39kgXmTLtiKvfKcOh/YaH2u1attFOzAfoP4J9/phCjqZBbXh6JlhUrSCBZkjifkEAV4kWbRGE1fDgJFbHAdnAw0LFj4wO0YmNplKU5HiuptPFSCKdPmyduW7TQ9ySKglOlch2xZY5QP3XKPmKL2xwjiC7ZnBy60JYu1R+5wjCMy2G22Bo7dix++uknpKWlYebMmQCAt956C2PGjMHUqVPxww8/YOPGjQ0mgjK2w1A8FRZqE6zFeQV1SxaYO+2KqXpOxmZAaNlSv7SCiPgAvnKFwnlnz9YXMAUFlOPl60vrBgZanjhvbjkCc8qoWFq8s7FjWyJujY3yU6kat9lRmHMuugLcVnCb0wBSKV1wp087b/Z0hmHMxuw7ND8/H5mZmZpGT2Tw4MHIysrCbbfdhmHDhtncQMY4uuKpsJBCeoWF2hF3Xl4kFDZtonwbc6ddaSi31nAGhNrahh/ANTVAcjIJlZMnyetWV0cPb3GmkaQkrb2iV6ywkEKg5kx/I9o0cCC9N+WZYziHaW4uvQ8YYFz8NXRs3f+PMdxpGjtzzsUe045xm8MwjKdgtmfrt99+00sE1cXPzw//+c9/MHbsWJsZZopVq1Zh6dKlyM/PR9++fbFy5UokJiba/biuhiieDh/Wli0Qp5sRhPolC95+2/bTrjQUIhTFRGIi1acyHOHo7U0iJSJCfztrE+dtha2Kd5qqWwa438Axc84lKcn2x3WVNodhGDeirg7YtYvCLwazOTgTs8WWqUZPl6EGlY1tzeeff47Zs2djzZo1SEpKwooVK5CSkoKcnBxERkba9diOwJIaRrplEI4fJ8GjVtN1VllZv2RBbq55065YQrduFKY0J8dJV8BcuAC8+652xJ0hjdXusje2KN7pzDkFbY055zJ+vO1zzFyhzWEYxo3YsoWSeS9dovyRFi1omPlXX1Ej5UQsHo3oTN555x1MmzZNU+NmzZo12L59O9avX4+5c+c62bqmkZ2tHZVmbpX1+Hj6/uRJElkKBXmMjJUsEIWLLaddeeQRyscyR0zoCpjgYBKD1uaPuQu2FrfOpLFz6dXLdRL6GYZphmzZAjz/PE3HERpKoRfRC/Dyy/TuRMHlNmKrtrYWGRkZmDdvnmaZVCrFqFGjsH//fidaZhuWLAGuXrW8ynpiIuVaNVayQFe42GralT59rBMTrhxiM8e7aIkH0hlzCtqLhs7FA+asZRjGXamrI49WRYX+PG1iMmlFBY3aHTdOfz40B+I2YquwsBAqlUpTTFCkVatWOCVW1jSgpqYGNTU1ms/iHEhKpdJlZjSvrSU7ysuV6N1b/xoJDaXR3Z9+SoLK2AO6Y0egd28gK4v+NhQu16/TQ7JjR9s9EHVnUu/Vi4Ti2bPkWQsKogmgG3sAT5pE4uzMGbo3RK+YOIJx4kTK7bLFqDxzZ7HPzgY2b6bfvKaG/gfdupEHT6zRZc46xoiO1v5tznmZa7MzMHYuxux1RdsZhvFAdu2i0KE4z5ohoaFULHLXLqpw7QTcRmxZQ1paGlJTU+st37lzJ/z9/Z1gkWmeesr4rOTiddFQnaihQ+ll7fbWYmwm9fx8Go1uDhMmmP7u8mXbh6XMmcXe2G9paIs569gKc2x2JXTtraqqcqIlDMM0G/LzqXdvali0TEbf5+c71i4d3EZshYeHw8vLC9fESeP+4dq1a2jdurXRbebNm4fZs2drPisUCrRv3x6jR49GkLFkISdw+LAS16+nIz09GYJQf0LMujryGr32mrbkgjGMeVzi4ihE3ZDHxRpsOSu8Wm3cK2ZLGrNXrQbmzyfvYLdu9b2DOTlAv370919/UT7amTMUStOtaTZkCLBunW3st+Vv7AiM2St6khmGYexK69aUS1NTQ2ESQ2pr6XsTWsERuI3YkslkSEhIwK5duzBmzBgAgFqtxq5du0xOpSGXyyE3onRdaTbzkBAK9ZWV+cDfv75NCgU9vEND6VoxRUIChQsdmRtkq99RnFPXklwoazBl7+nTwLFjNLDAWHgvMlJbFywwkEZgikU+5XLt4IQff6TXP5enXW12VXTtdSe7GYZxY0aOBNq3p16wn1/9UGJJCdChA63nJNxGbAHA7NmzMXnyZAwYMACJiYlYsWIFKisrNaMT3ZGYGHrYX7kCdO7ctGRxWyW+O4PMTG2ivbmjMW2FORXSq6ro/1FSol/TDCARHBZGHurPPwfuucc9E+AZhmHcEm9v4MUXaTTilSva0YjV1fR9YCAwZ47TkuMBNxNbDz30EAoKCrBw4ULk5+ejX79+2LFjR72keXdCfCi3bOn+9ZisJTMTWLyYKsebOxrTXC+YuB5A78amkDNnKiN/f+1E18ZEWV0drXPpknOKsTaEvT2GDMMwTkcs6yDW2SotpTpbAPD661xny1JmzJhhMmzozrz0krbOljvXY7IUtZo8WoWF+mUgdCe0/ugjKjkgCgRzvWDierm5wLRp1LGJja2/nrmlKPLzaV+GgkwQSKiFh5ONzirGagxnegwZhmEchlpN+TTvv0+VvKVSGtpeUwM88ICzrXM/seWp9OkDvPNO8/FAiN6W7GyacqhdO9MTWutO3WOuF0x3vU6daH+hoca9ZeZUSBe/372bvFtBQeSR1q3Y364dCS9XKcZqjceQYRjG7cjMBDZupMZNDEUkJNCD1UWqLbPYciHcOecKMD9cpettKSgAzp2jXKi4OBI2uuhO3dOYF+zECeA//wGefBJYs4b2LRZ8FdeLi6PjL11KYkOcQNqcau99+wJffAHs3UudpaoqbcX+2Fg6nqvMd2iNx5BxYzhWzDRXMjOB2bMp+Vmt1i4/dw44dQr417+cZ5sOLLYYm6AroKqr6Zpv1w54+GHgrruovENZGZCXRx2QoiL6PiiIKudfv07ipX9/fcGlWwE/N1freTL0ghUVkdg5dYo6N1evUh5cUZF2tG9REc0jWVhIg1YuXiRxJIbUGqv2LpVSKFL0eIWFaT1ceXmulV/X0G/l7Mm+GRtjzVxfDOMJqNXUc87Opl51QIB+yOHYMe16TobFFtNkdMNVAQEUZisqAo4eBXbuBCIiSJj4+FD4qq4OSEoioSIIJFIKCkiknT5NIkkiqT8aMyPD+KjBwkLgyBHaXiKhcOG1aySYjhyhYwFUR6usjEJ+AHnNDENqjXkX4+OBRYv0PXOumF9nzghLZ072zdgQca6vtm2pdolCAezbB5w/Txerq1yUDGNrTp8Gfv+dGu6QEG3PUiajB05lJX3OzQV69nSamQCLLaaJ6IarIiJIeFVXa2tQXb1K01JVVlIIr7aWxFZmptaL1bUrrVNRQfspKaHOieFoTGOjBgWB7jfxmLW1NABFLqf7rapKOxqxpobux9pa2n9oKOVzmQqpmYrMuMN8h+aMsPSEyb6bNWJvvaiIbr7jx2kEllhp9+pVYNky8nq50sXJMLbi2DHqXBibpkcioZIPADXyLLYYd0YMV7VtS219dbW2g3HtGrXxEgkJnfPnSRyFhZFI+ftv8mKFh5PwOnWKvC25uZQHZegtMjZqsKyMni8BASQgwsNprsWLF8nrFBBAAg4gb45KRQIkMlI7YbexkFpjo/hcPb/OlSf7ZmzE2bP07u9PF6zoygwIIMFVVgakpwPffmvbSrsM42ooldSwSaXUy3ZBWGw1A+yZOyuGq1QqreiRSMh7JM6QUFdHnqaKCtpGpaL1Skpo+5AQEkm9e1MHZdYsGkRiaKexUYPV1STklEoKD4oJ76K3rLJSKzTUarLRzw/o0kW73DCk5gmj+MwZYekq+WWMlYjTIeXlcaVdpnkildID5to1CleIYkt07YsPHXGaEifCYsvDsXedJfGaFucJFKelUqvpJZXSy8eHRJa/vza0VVVFggygTkleHnlbevWi/eXm1hdchqMGi4vpOKGhtJ2YXC96y0QvM0DnHxlJQstUEr4njeIzZ4Ql48YEBZGYKinRT86rraULWaWinoUrVtplmKaSmUmNsa8vXfOCQA32jRvUA/f1pRfgEi58FlsejCM8NGK4at8+ShOpq6OOhRg+FD1OUil1PDp3pvBhcTF99vIiMXT5Mn2+fh147rmGhaFuzlRJCdWwO3OGQpK6tGxJqSxDhtDnNm0obK8rkgxDap42is8d8ssYKxHn+qqqIq/WjRv0T66pIaGlVmt7NzwSgvEkxF5xURFwyy3An3/SNa5SUaNeV0cN9oABtL4LNHjOt4CxC4YemqAgEjaih6awkDoFTR0RK4arxIKeCoW2Y6FW07KgIGrvxYT0+HgSZDIZiaviYpojFKBOeFgYeZ/CwkgYLl5MwtHwuF270kjDWbNIVJ08SccXJ4Y+eZKWi2VW2rWjvLCyMrpHz52j0YotW2pDauaM4hOfae6CVEoCKzhY6zF0gZHQTFMRHyByOd1I16/TjVZbS70cMUExJwf47Tfn2sowtkS3VxwRAQwaRJ2PoCBqvIODqWF/+mlnW6qBPVseiiM9NGI5hHnzgF9/JW+TVEriTiIhYRcSQvdCeTklrg8cCEyZQon1LVoAq1dTUrs1obvGwmW9epH36qWXgLffppHCYmgxKEj//D1xFB9P2ePhDBsGbN2qDR8KAt18YsKkUgmsWkXrJSQ421qGaTqGveLwcBJXZWV0H3h5UeejTRsKtbsALLbcDHOT3e1VZ6mhiZ19fclzVVlJx1ar6V0qJS9VSQkltBvmDJ0+TR6npgjDhsJlSqV2vaoq6gjFxWm9fZcukfds4ULahyeN4vOEZH+mEcaOBX78kYSViCCQe1kioRvz+nVg5Upg/XqXCKkwTJMw1iuWSLSDRBQKyl8R8xpdABZbboQlHgp7eGhMTew8aRKV8ikq0uZHiR0MHx9KfO/ShTy6oaH1BaKthGFj5Rg2byYb+/fXF1FBQVrv2dtve84oPk9K9mcaoE0boFUrutnr6rRD4EXvlpi/lZHhPsmGDNMQ5ta2EfMaXQAWW26CpR4KW9dZamhi5+PHaYRtp07a4+iOQhfrK4aGGm/nHRW6y8kxz3vmKaP4PC3ZnzFBUJC2eKOPj3YIvJcXLVOpyMt1/Tq5lxnG3XHD2jYsttwAazwUtrwWDY+vO7Fz9+7AwYP0XY8exrdvzDPlqAKcNTXaeRIbs1EMS54+rZ1eq1cv9xIlPGVPMyEmhir5HjlCyfLiDSoIdPOKUybU1VGhOYbxBMzpFevmkDgZFltugLUeClt5aMw5/uXLFBpv167+9o15phzVSZHLLfOe/fWXeyeWe2KyP2MEqRS47z5gxw7qUUilJLTEubHE+kNqNbmYGcZTcKPaNiy23ICmeChscS02dvzISBIyeXk0utAaz5QjQnfdupEXzhzvmScklvOUPc2Im26ixMizZ2kUijgqURDoe0Ggm3jGDBJc8+Y5116GsRWuPnfaP7DYcgOs9VAYjlxMSLBO8Dd2/OpqqpMVGNg0z5S9OymPPELPosZsrKsDVqygOlxdu1JpConE/RLL3TCtgbGW2Fjg1lvpn3n+PCVR6hZTE6dyqKoCXn+dbtgJE5xmLsM0N1hsuQHWeChsWVvJ8Pi6iMe/6SZg4kQaldgUz5Q9Oyl9+jTuPcvMJKG1fTvZUlREyf5du5I4cbfEck9J9mcaQVTWx4+T2BIbCW9vbRjR15feRcH10EP0PcMwdofvNDfAUg+FrUNghscXRyMqFNSui8ePj6eXvcPnTZlYuyHvmfi7nTunLdmiUlER1ooKKhkRHu5+ieVulNbANIX4eLpRMzPpwhX/wV5eNF2DKLy8vekC3rULSElxrs0M00xgseUmmOuhsFdtJd3ji0VNS0rqH9/e4XNbeOyM2aj7u3XrRh4ttZqeUT4+NIjr77+pSLE7Jpa7SVoD01QSE6nmVmkpXaRi+FBsCNRq7SSmLlLskWGaAyy23AhzPBT2rK0kHj8nh0oiLF1KwgSgz/bymoierIMHSRBVVwPt29s2aV33d2vRgrxa4jRDEgkdq6SEnmFXrzovsbwpXj3GMoqLizFz5kx89913kEqlGDt2LP7zn/8gUKxpZYThw4dj7969esuefPJJrFmzxt7mErGxdGOLhRzFWluAtgyEVEo9iMhIx9jEMAyLLXejMQ+FvWsriZManz5N7/YujyB6sk6coFdlJY14bNWKvHW2SlrX/d0kEvqNKypIXAUE0DOrpobOOzraOYnlPMehY5kwYQKuXr2K9PR0KJVKTJ06FdOnT8fmzZsb3G7atGlYvHix5rO/v7+9TdUilQLz5wO7d9MFLHq2lEq6gAFtKPHHH6nwHF88DGN3uE/sYeiOHDSGLUNg2dmU45SRQXMfdulC7xkZtDwzs2n7F3OoMjKoIy4IJK6Kiqh+Y2EhrWfosbMGw98tPFybo1VTQ14ttRro169hD5paTYLs0CF61x0Q1hR0fwt7/NaMPidPnsSOHTvw4YcfIikpCUOGDMHKlSvx2Wef4cqVKw1u6+/vj9atW2teQcaG8NqTgQOBuXPpgq6qoteNG9oyEHI5VegVLyq+eBjG7rDY8jDEkYOXL2vbVhFx5GCPHrYJgW3erM0NEyd1Fj1NhYXkabJWbBjmnslktMzXl0J7N26QmBHP0d+fllnrsTP2u4WHA4MGAYMHkzftrruADz80LbQyM4HZs4GZM4EXXqD32bOb/iwz/C1s/Vsz9dm/fz9CQkIwYMAAzbJRo0ZBKpXiwIEDDW776aefIjw8HL169cK8efNQVVVlb3PrM28esG4dXdi6IxNDQmjocIcOdIFfukQXF188DGNXOIzoYTiytpK5cw1ak5htmHsmDqaqq6O/AwIoxKdQkFeqqR67hn63K1codPjcc6ZHypszAtTaEYE8x6Hjyc/PR6RBTpO3tzfCwsKQ30Bi+SOPPIKOHTsiKioK2dnZeOmll5CTk4OtW7ea3KampgY1YogPgEKhAAAolUooG5huRPzO5DoPPkj1TqZPJ9dwaCjdJGfOkMiqq6P1fvwRGD4cuPNOk8eyJ42eh5vA5+F6mHMujjpPFlseiKNqK1ky16ClGOaeBQdTp7ygQDvXbmUl5fvaqhq6tb+bOSNAly0joXvqlOX5VjzHoe2YO3culixZ0uA6J0+etHr/06dP1/zdu3dvtGnTBiNHjsSZM2fQuXNno9ukpaUhNTW13vKdO3eale+Vnp7e8Apz5za6DwDADz+Yt56daPQ83AQ+D9ejoXNxlOeZxZaH4ojaSpbONWgJhlXrDZPW5XIKpdXWkpixlcfOmt+tMc9TQACQnk6Rmy5dLB9FyXMc2o7nn38eU6ZMaXCdmJgYtG7dGtevX9dbXldXh+LiYrQ21cMwQlJSEgAgNzfXpNiaN28eZs+erfmsUCjQvn17jB49usF8L6VSifT0dCQnJ8NHnHzakNxcYM4c6qmcOkUJj7oXSm0tvSIjKV7++usOH/lh1nm4AXweroc55yJ6ku0Niy0Pxt61lSyZa9BSjFXNF5PWT5+m/QcE0HPC1h47S3+3hjxPgkARm5oaKlchPjstGUXJcxzajoiICERERDS63qBBg1BaWoqMjAwkJCQAAHbv3g21Wq0RUOaQlZUFAGjTpo3JdeRyOeRyeb3lPj4+Zj3sGlyvWze6MPbto7pavr76oxJLS0lodegAHD1K6t9JsWhzz9fV4fNoAnaqbdPQuTjqHN1GbL3++uvYvn07srKyIJPJUFpa6myTmj3mzjVoDaZyqGQyugdbt6bvExOdX2uqIc+TQgEUF2tt18XcfCue49DxdO/eHbfddhumTZuGNWvWQKlUYsaMGXj44YcRFRUFAMjLy8PIkSPx0UcfITExEWfOnMHmzZtxxx13oGXLlsjOzsb//d//YejQoejTp49zTkS8eLKygPJy7UiTujq6YP38tO7Wq1c5Fs04Dw+vbeM2zXNtbS3GjRuHp59+2tmmMP8gzjWYkECCIjeX3gcMaFqBURExh8pw/wMHAm+9RXMxdu3qfJHR0AjQmhoSRS1bGg/zmTuK0tRvYavfmqnPp59+iri4OIwcORJ33HEHhgwZgnXr1mm+VyqVyMnJ0eR8yGQy/Pzzzxg9ejTi4uLw/PPPY+zYsfjuu++cdQpEfDwwaxaFEqurqQcghg7F+iYci2acSTOobeM2ni0xgXTjxo3ONYTRw965YU3dv+iVBui9Wzfbi7OGPE+XLlF+mbF8LsCyZxzPcehYwsLCGixg2qlTJwg66rp9+/b1qse7DPfcA+zZA/z+O8WzRRexRMKxaMa52GuOORfDbcSWNVg7pNqRuOMwW2M2R0drv1ep6GVLrNl/djbVAjt7VokpU4D585WIiaHwp62jOr16AQsW0PFycqjdkMuBYcNoBGVeHiX0G+ZbXb9OIqpjRyryLdLQdWHv39oajNnrTte0xyOVAlOmABcvauuTqFQci2acTzOpbePRYqupQ6odiTsOs3UHm4cOpRcATJlC9l6+TC97H88cUlLofccO49+7w2+si669TinmyZjGUTVhGMYSmkltG6eKLXNr3sTFxVm1f2uHVDsSdxxm6+o2q9U0PVxWFoUNvb2VGDkyHT//nIyiIh+cOQP07g2sXGm6SKmtEb1sOTmUxyWXA3FxwPjxxr1srv4bG2LMXkcNqWYsgGPRjKvRTGrbOFVsmVvzxlqaOqTakbiiTY3hqjafPg0cOwZERFBZITGtZv9+H1y75oOaGiqirVRS3rAjOvQJCXQcS59xrvobm0LXXneyu1lh75owDGMJzaS2jVPFlrk1bxjnYKeSJ3anrIzSUvLy6G9BACZMAM6f186tWFIC/PUXDXRx1Gg+fsYxDMMY0Exq27hNztbFixdRXFyMixcvQqVSaYoFxsbGIjAw0LnGeSANlTzp1cvZ1jVMXh6NAhQEoEULGu0OkHgsK6OOk68vhRivXHH8QBd3FbFMM4QvVsYRNIN8QrcRWwsXLsSmTZs0n+P/+fH37NmD4cOHO8kqz6SxSZUXLHC2haZRq2mEu48PDbaSSLSj/GQyEo7FxUBMDD07pFIa6HL6NP1t72dKRgbw7ruUu6VS0dzAPXp4TN0+xpPw8CKTjIvh4fmEbiO2Nm7cyDW2HIA5JU+2bAFuucW5dpoiN5emgOvdmwRNWRmdE0CeLvEVFUXn5u9PQuuVVyi0aM9nypYtJFSLi0kM+vhQfcmCAvPmSGQYh9FYj4svVsYeeHCuhWdIRsZmmFPy5NQp59hmDuIo4nbtqDh2WJg2QV6tJiEVGKgdZSyGHHNz7Vu4OCODhFZBAe0/NJRsKSsjkXf+PIUzRWHIME7DsMcVFERF4lq0ANq0oQbg3/8mlzBfsExjqNXUoz10iN6b6TXjNp4txn7opmVcuEA5Tg2VPCksdKx9lqA7ijg8nIqKHjhA34WH0ywltbXaKeKOHiUPU3y81ltt68LFajWFDouLaYSkOEeiTEbHLi2lXNDjx92+bh/jCRjrcRUW0oiSa9eodkluLnDkCDB8ODBnDnu5GONwKFoDi61mjuG9oFKRp8ffn6qaG1JVRTWiXBXDUcRSKeVEASQiKytpSjiAzl2pJA+YoZiyZeHi3FwKaXp7k7gyPE5AAFBRQR4uN6/bx3gChkUmCwuB/fuptyCRUANQV0cX7fbtVEfl/fepvgnDiHAoWg8OIzZjjM392a4dtaOHDlHISxex5ImVNWYdgjiKODycBKRCoa2FJ06X4+tLwqZrV5omrl074/syd5LoxigrIxErPqMM8fYm0efl5fZ1+xhPQNc9LAgU+ikro5tLLidXbV0duYirqqh68BNPUEPCMIDpULQYNigsbHZ5Eyy2mimm7oXgYCApidY5cIDa2Lo6Ei0nT5KIGT/eubY3hjiKOCGBOuNnz9LyMWOAdeuANWuoevzChXQ+lZXG92OrwsXBwZSjJXbsDKmrI7EVF2fbun2cKsFYhegevnyZGoCCAhJd3t50sVZXay9atZrejx8H/vUv2yU5Mu6NJfMdNhM4jNhMaeheiIgABg6k7y9fJhGmW/KkVy/t3IKuWoZHdxRxSQmQnw+89hqFSEVPlaMKF8fGUiizoIDSXUpLSXiJHq2CAgptzpxpu9+OUyUYq9EtMinOLyUO462upneJhBoGgNy2ajUlzi9bBnz8sWs0AozjER8IYti5obCBB8x3aAkstpopjc392bYteXaefZZyt3SFlFi3Kjub2lVXfaCLo4iVSuCHH2g04LFj+rYmJdm/cLHuswug/VdW0jOsro6E1quv2i7lhVMlmCYjuodXrKCLRqXSCi6JRJt8qPvZywvYt4/cqK6ca8DYB90igpWVJKYUCuqdh4frr+sh8x1aAoutZoo5c3/6+dEkyaaSw5csAa5etd8D3VZes+xses/KImFjaOuDD1LI1J6Fi3ULJJ84Qd42Ly96Js2caTuhZU6dNEdXzGfclPh44L//BR57DPjxR+ql1NZqPVoAXXASCTUWwcHktj12jMVWc0O3iKC3t3a4d14e9SoTErSCy4PmO7QEFlvNlKaE0MTcn6Ii+z3QbRUGU6uBzZuBoUNpeh6Vqr6tBw9S9OPsWfuGQx1RINmSVAkuMcE0irc38H//R/H3Q4f0KwOr1fQul9PFbHjBMc0D3SKCERHk5ayrI5FVU0M9v2PHgJtvpjC0B813aAnN50wZPYyN2jNMhDd1L4gJ52IVdhFBIBHh56dNyrYGY6MkrS00KpZdABoWH2fPkvgYOJDe7dUGiKFNex2nsfCwrUZYMs2I+HjgnXeoaJ2Xl3Y0okRCFYIjI0lwlZdTL8bVJ09lbIexIoJiWDkighocHx8SXEeP0noDBjTLXAb2bDVjrJ37U6Gg94AAbXX2wkISV6WllCNVUwOkpgIvvmi5J8qWYbCyMrLFFJ6Wp2lOeLiZpUowtiA+HvjmGxrSu3evdhi/nx8Jr9JSunmHDGGXqaejm99RXEwDI3x8yAtqSEgIebPCwoAZM4BBg1xnFJWDYbHVzLEmtBUURKP7Kiu1FeWPHNFWnhc7N3//TZ4oSzoxtg6DBQc3XITV08SHo0ZYMs0Qb28ayTF7NvWslErK4RJHJnbvDrzwQrN8kDYbDPM7amporjGARLc4PYaItzddIwEBJLSasRBnscVYPPdnTAy1tVeuaP+urqZOjERCndzwcBJYp05Z7olqLAxmiScqNpZytQCtF07EE8WH7shHe46wZJopYkhx40ZS9FVVdIENGOA6w5AZ21NXB6xdC6xfTwKrZ0+aKzM/n/I0lEpKiI2I0O/hKZW0ra2LCLohLLYYixEf1C1bUkensJDa27o68nb5+mrzkazxRNkyDCaVAo88QkIjJ4fSSzxdfFgbHmYYs2jKSA9jQ4wZ12bLFhp6npNDXipvbxJZffsCnTtTnaDz50lYlZRQHp9YANceRQTdFBZbjNW89BKwdClNjSYIFLYPDydRJY7ytcYTZeswWJ8+tF2/fjQopjmID0eMfGSaMZa6wwHTQ4wnTbKPjUzT2bIFeP55baKuvz+9KxTAn3/S3926UTijrEw7AkeppJetiwi6MSy2GKvp0wd45RUSMgEBNCVNUJC+QLLGE2WvMNgbb9B+m4v4sOZ5yDB2oaFKu1euABMmONtCxpC6OuCtt2jC8ZYtyUslTjDr5UWN8l9/AffdRz3XQ4dIXKnV9iki6Oaw2GKaRNeudJ9lZFCleVt4ouwVBmPxwTBOoLEhxuL8eIcP00Pd03tBrooY4i0poc979lB9tdBQCgtKJFohJZHQyKOKCipcGhxMeVzPP08jD5tDb9ZCWGwxTcJenigOgzGMh9DQEOOiIhJhAI1kDAx0rTm/mgu6IV61mso0rFpFyfBhYdTwyuUUIpRK6f8oldK6VVVUY23AACA5mRtpE7DYYpoMe6IYhjGJqSHGYs0YcZhwmzaU+MmTeDoWwxCvmPORn09iq7ychpoHB2vLfXh7a6dqKiykytOeNtLIxrDYYmyCJ3qibDU3I8M0a4wNMRYEbc2YsDBa5utLgown8XQcxkK84tyXAwdS7Z6iIu3/MDxcWylaFNAjRwJTprAwbgQWW4zN8CRPlK3mZmSYZo+xIcZlZTSCLSCABBdAQkylMl29mHs/ltPYb9ZQiNfLi8RuRgblhLRsSaHEwEDyboWGAvPnA7Nm8f/BDFhsMYwBDQ2c4ugGw1iIscTO6mryjiiVFKIC9B/2hjVjDHs/cjmFHZOTgcREFl7GMKfH2FgV6e7dKWFerSYPV2kphXq7dgXmzAHGj3fY6bg7LLYYRgdbz83IMAzqJ3YWF9PNFhpKBfAM0a0ZY9j7qa6mfRw6BGzfTvlCSUnsdtbF3B6jOVWkO3QAli+nkYn5+UDr1hQ6NDYXImMS/rUYRgdbz83IMMw/6CZ2lpQA779PFZHFnC0R3ZoxMTE0SlHs/RQVUW2nGzdou8pKEm6HD1MV8ylTqKJ5cw4zWtJjNLeKdFwc0KOHc87HQ2CxxTD/oFYD2dnA9evULglCfcFlaUV8hmF00E3slMnI+5KTA6SkUBFNhUK/ZszZs9reD0BJ9TduaEOPAHlf/PzI05WZSQX//Py01elbtPC8PK+GcrEs7TEahnhFD1dOjmfOZ+YkWGwxDLTpDWIHOT+//tRDgOUV8RmGMYEYWvzkE/p89iw91HVrxhw6pM0pUii0SfUi3t4kOI4fp/wvtRpo1YqW790LfPcdVTWXyTxnlEtjuViN5WEZ9hgNQ7xi3bP4eKrs786/lQvBYotp9mRkAHPn0mwUUVH0un6dPldUAP37k+CytiI+wzAmiI+n8NSOHcBrr1EOl66XRjenqLaWvF+6IkKpJGEhkVBYUaGgEY1qNYUqi4tJeA0ZQj0lVx7lolvBvbiY6ltJpUCvXtTrk0rNy8UyJw/LsMdoGOLNzwdef50GIjA2gcUW06ww9L6XlQHTp5M3y9eXUkLkchpwo1aT2Dp1Cujdm2alYK86w9gY8Wbq359uPF10c4qiokg41dXReoJA4koi0ZaN8Pam706coNGOEREkLioqKPRoy1EuuuKotFQrbE6f1oY29++nHlqHDhTSlMnqbxcSAly9StPjHDxIeWzl5bSeTEYi8uabaSqcjz9uPBdr6VLz8rAMe4xiiFepBH74gRs5G+MWYuv8+fN49dVXsXv3buTn5yMqKgoTJ07Eyy+/DJlM5mzzGDchO5vaKtH7XltLAqqkhNozX19qxysqqJ0JDKTO4ZUr1OEeOLBpFfEZ9+L111/H9u3bkZWVBZlMhtLS0ka3EQQBixYtwgcffIDS0lLcfPPNWL16Nbp06WJ/gz0R3bIReXkUAisr03p0xPbf25uElxjzF8ON4oTJtbW03FajXMRQ3oEDwMWLJOz8/YGVK4H77yexVFhIAhAgO+bOBR5+mP7W3U4qpYbHy4vWr6zU2qpW03n99BOJMKkU6NSp4Vyss2ftM4ca0yTc4tc+deoU1Go11q5di+PHj2P58uVYs2YN5s+f72zTGDdiyRLq7IWFUaeuuJhetbXaqb5kMupoqtXUMR08GIiOprp9b7/NQqs5UVtbi3HjxuHpp582e5u33noL7777LtasWYMDBw4gICAAKSkpuHHjhh0t9XDEnKIBA+jmraujGzc4mEKQPj702ddX65mpq9N6wby9taIMIOFx44b1o1zEUN7evVQOQaUiGxQK+r6ggDxVSqV2lI1USja+/z7w+efa7fz9qXdXUUHbl5XR+n5+dD7i9t7eJKIuXqRtjKF7XuJvlpBAx83NpfcBA1wzhNoMcAvP1m233YbbbrtN8zkmJgY5OTlYvXo1li1b5kTLGHdArab3oiL9AtZVVdR2X7tGHWFfX/pOIqFOcWkptV0REUCfPtwRbG6kpqYCADZu3GjW+oIgYMWKFViwYAHuvfdeAMBHH32EVq1aYdu2bXj44YftZarno5tTdPAgkJ5OgubGDRJSUimtI04n4+1NYqeykhLkdfOTmjLKRSyrUFBA+1epyO197Zq2gRCFtURCIkylItHk5UXbXL9O1dhbttT29EQvlngM0XPl7U3er8BAsruqivKpxNGZuhielyfOoebGuIXYMkZZWRnCDOuzMIwRzp6l96gobRsm5tqKuaTV1dQO6kYlqqoocjFiBCfEM41z7tw55OfnY9SoUZplwcHBSEpKwv79+02KrZqaGtTU1Gg+K/7xkCiVSiiVSpPHE79raB13wKLziI6m17hxdGMrFCS6Pv2UPDcBAZTDFBpKoiQ0VOv9Akj0XL9OQqRjR7rpLSE3l14RESSwwsI0Qkrp50fn8c+7xiMFaIWUmLMll9NLraYGCNAm+kulZK+XF+1bqSRPl1RK6xcWku2GuVimzis6Wvu3SqUNbZrAU64rwLxzcdR5uqXYys3NxcqVKxv1alnbiDkSd7yw3c3m0lKyMzhYCUGgZf7+1Fn08qKCyNevUwfS15faR7FzGhUFTJxoVhtlU9ztNzZmr7vYbivy8/MBAK1atdJb3qpVK813xkhLS9N40XTZuXMn/E2FjHRIT0+30FLXxOrzkEjoJtXlzjtNr5+SQu87dlh3vGnTGvw6ff166/bbVJp6XgZ4ynUFNHwuVVVVDrHBqWJr7ty5WLJkSYPrnDx5EnFxcZrPeXl5uO222zBu3DhMa+Sib2oj5kjc8cJ2N5uTk/XtHTfOvO0uX6aXM3C331jXXkc1YpZgTZtjb+bNm4fZs2drPisUCrRv3x6jR49GkLGh+/+gVCqRnp6O5ORk+BiO4nMjbHYearXW2xUURHlQn31GxTlrasiTFBdH8/n16WPdMXJzaU5AHx/g2DHyVAkCUFAApZ8f0leuRPJjj8Gnutq4Z0sQ6G8xjPjPtqiuJle7bm9QKiUPlTg82tsbGDOGkuxtfV46eMp1BZh3LqITxt44VWw9//zzmDJlSoPrxMTEaP6+cuUKRowYgcGDB2PdunWN7t/aRsyRuOOF7W4219Yq8fPP6VizJhnR0T4a73tREeW6lpSQdyspiTz0V65QO5iaanzaNkfgbr+xMXsd1YhZgqVtjiW0bt0aAHDt2jW0adNGs/zatWvo18CFJJfLITdSz8jHx8es/72567k6NjmP7t31Pyck2DZnqVs32sfhwyRyiopoRI1KReIOgE91tVZsiTlbuiFBqZREkhh1kcloP+KoRLWa1pNIaDu1mtbt04c8ePHxtj8vI3jKdQU0fC6OOkeniq2IiAhERESYtW5eXh5GjBiBhIQEbNiwAVIzLqymNmKOxBVtagx3s7lFCx8cPeqjGQktkdC7WCfx9GkKIw4e7DolHtztN9a11xXttqTNsZTo6Gi0bt0au3bt0ogrhUKBAwcOWDSikbEhutMD2Wp/YlmFykoSQyUlJLyqq2kdMQlUV1xJJNTQSKWUsK+7XU0NCSZB0OYqVFeTyJJKyUs3ZAjNESk2SrY+L8buuEXOVl5eHoYPH46OHTti2bJlKCgo0Hwn9iYZpjFeeklbZ+vKFWoThw+nzqInTp/GNI2LFy+iuLgYFy9ehEqlQlZWFgAgNjYWgYGBAIC4uDikpaXhvvvug0QiwaxZs/Daa6+hS5cuiI6Oxr///W9ERUVhzJgxzjsRxrboTm+jWy9LHAUYEUGNi1hnSzd0+NBD+nW2Kitp3fbtybU+fDjQpo3pCvKM2+IWYis9PR25ubnIzc1FO4Mhr4IY42aYRujTB3jnHR4JzZjHwoULsWnTJs3n+H+8Cnv27MHw4cMBADk5OSjTqdf04osvorKyEtOnT0dpaSmGDBmCHTt2wFccccZ4BobT24gV5K9fB7ZutbyCvOE0RYzH4RZia8qUKY3mWTCMObD3nTGXjRs3Nlpjy7CzJ5FIsHjxYixevNiOljEugWFjIk5z07WrttREr16Nb8c0C1hGMwzDMAzD2BEWWwzDMAzDMHbELcKItkJ0+bvSkHSlUomqqiooFAqXHL1lDHez2d3sBdzPZmP2ivcZ51VahrntlLtdI6bg83AtPOU8APPOxVHtVLMSW+Xl5QCA9u3bO9kShmk+lJeXI9iaeeiaKdxOMYzjsXc7JRGaUbdTrVbjypUraNGiBSS680o5EbHQ6qVLl1ym0GpjuJvN7mYv4H42G7NXEASUl5cjKirKrLp4DGFuO+Vu14gp+DxcC085D8C8c3FUO9WsPFtSqbRe6QhXISgoyO0ubHez2d3sBdzPZkN72aNlOZa2U+52jZiCz8O18JTzABo/F0e0U9zdZBiGYRiGsSMsthiGYRiGYewIiy0nI5fLsWjRIqNzOLoq7mazu9kLuJ/N7mavJ+Apvzmfh2vhKecBuNa5NKsEeYZhGIZhGEfDni2GYRiGYRg7wmKLYRiGYRjGjrDYYhiGYRiGsSMstlyI8+fP4/HHH0d0dDT8/PzQuXNnLFq0CLW1tc42zSSvv/46Bg8eDH9/f4SEhDjbHKOsWrUKnTp1gq+vL5KSknDw4EFnm2SSX3/9FXfffTeioqIgkUiwbds2Z5vUIGlpaRg4cCBatGiByMhIjBkzBjk5Oc42y2Ox5n4TBAELFy5EmzZt4Ofnh1GjRuHvv/+2r6GNUFxcjAkTJiAoKAghISF4/PHHUVFR0eA2w4cPh0Qi0Xs99dRTDrKYsLQt+fLLLxEXFwdfX1/07t0bP/zwg4MsbRhLzmPjxo31fndfX18HWmsca9rKX375Bf3794dcLkdsbCw2btxodztFWGy5EKdOnYJarcbatWtx/PhxLF++HGvWrMH8+fOdbZpJamtrMW7cODz99NPONsUon3/+OWbPno1FixbhyJEj6Nu3L1JSUnD9+nVnm2aUyspK9O3bF6tWrXK2KWaxd+9ePPPMM/jzzz+Rnp4OpVKJ0aNHo7Ky0tmmeSTW3G9vvfUW3n33XaxZswYHDhxAQEAAUlJScOPGDTta2jATJkzA8ePHkZ6eju+//x6//vorpk+f3uh206ZNw9WrVzWvt956ywHWEpa2JX/88QfGjx+Pxx9/HJmZmRgzZgzGjBmDY8eOOcxmY1jTJgYFBen97hcuXHCgxcaxtK08d+4c7rzzTowYMQJZWVmYNWsWnnjiCfz00092tvQfBMaleeutt4To6Ghnm9EoGzZsEIKDg51tRj0SExOFZ555RvNZpVIJUVFRQlpamhOtMg8Awtdff+1sMyzi+vXrAgBh7969zjbFozH3flOr1ULr1q2FpUuXapaVlpYKcrlc2LJlix0tNM2JEycEAMKhQ4c0y3788UdBIpEIeXl5JrcbNmyY8NxzzznAQuNY2pY8+OCDwp133qm3LCkpSXjyySftamdjWHoertq262JOW/niiy8KPXv21Fv20EMPCSkpKXa0TAt7tlycsrIyhIWFOdsMt6S2thYZGRkYNWqUZplUKsWoUaOwf/9+J1rmuZSVlQEAX7Muwrlz55Cfn693DwQHByMpKclp98D+/fsREhKCAQMGaJaNGjUKUqkUBw4caHDbTz/9FOHh4ejVqxfmzZuHqqoqe5sLwLq2ZP/+/XrrA0BKSopT2x5r28SKigp07NgR7du3x7333ovjx487wlyb4uz/R7OaG9HdyM3NxcqVK7Fs2TJnm+KWFBYWQqVSoVWrVnrLW7VqhVOnTjnJKs9FrVZj1qxZuPnmm9GrVy9nm8MAyM/PBwCj94D4naPJz89HZGSk3jJvb2+EhYU1aNMjjzyCjh07IioqCtnZ2XjppZeQk5ODrVu32ttkq9qS/Px8l/rdAevOo1u3bli/fj369OmDsrIyLFu2DIMHD8bx48dddq5hY5j6fygUClRXV8PPz8+ux2fPlgOYO3duvQRDw5fhhZ6Xl4fbbrsN48aNw7Rp01zeXoZ55plncOzYMXz22WfONsWt8JT7zd7nMX36dKSkpKB3796YMGECPvroI3z99dc4c+aMDc+CMWTQoEF49NFH0a9fPwwbNgxbt25FREQE1q5d62zT3Ar2bDmA559/HlOmTGlwnZiYGM3fV65cwYgRIzB48GCsW7fOztbVx1J7XZXw8HB4eXnh2rVresuvXbuG1q1bO8kqz2TGjBmaRGd36u26Ava838Tr/Nq1a2jTpo1m+bVr19CvXz+r9mkKc8+jdevW9ZKx6+rqUFxcbNF9mZSUBIAiAJ07d7bYXkuwpi1p3bq1y7U9tmgTfXx8EB8fj9zcXHuYaDdM/T+CgoLs7tUCWGw5hIiICERERJi1bl5eHkaMGIGEhARs2LABUqnjnY+W2OvKyGQyJCQkYNeuXRgzZgwACnXt2rULM2bMcK5xHoIgCJg5cya+/vpr/PLLL4iOjna2SW6HPe+36OhotG7dGrt27dKIK4VCgQMHDth8BLG55zFo0CCUlpYiIyMDCQkJAIDdu3dDrVZrBJQ5ZGVlAYCeiLQX1rQlgwYNwq5duzBr1izNsvT0dAwaNMju9prCFm2iSqXC0aNHcccdd9jRUtszaNCgeqU3HPr/cEgaPmMWly9fFmJjY4WRI0cKly9fFq5evap5uSoXLlwQMjMzhdTUVCEwMFDIzMwUMjMzhfLycmebJgiCIHz22WeCXC4XNm7cKJw4cUKYPn26EBISIuTn5zvbNKOUl5drfkMAwjvvvCNkZmYKFy5ccLZpRnn66aeF4OBg4ZdfftG7Xquqqpxtmkdizv3WrVs3YevWrZrPb775phASEiJ88803QnZ2tnDvvfcK0dHRQnV1tTNOQRAEQbjtttuE+Ph44cCBA8K+ffuELl26COPHj9d8f/nyZaFbt27CgQMHBEEQhNzcXGHx4sXC4cOHhXPnzgnffPONEBMTIwwdOtRhNjfWlkyaNEmYO3euZv3ff/9d8Pb2FpYtWyacPHlSWLRokeDj4yMcPXrUYTYbw9LzSE1NFX766SfhzJkzQkZGhvDwww8Lvr6+wvHjx511CoIgNN5Wzp07V5g0aZJm/bNnzwr+/v7CnDlzhJMnTwqrVq0SvLy8hB07djjEXhZbLsSGDRsEAEZfrsrkyZON2rtnzx5nm6Zh5cqVQocOHQSZTCYkJiYKf/75p7NNMsmePXuM/p6TJ092tmlGMXW9btiwwdmmeSTm3G+Gv79arRb+/e9/C61atRLkcrkwcuRIIScnx/HG61BUVCSMHz9eCAwMFIKCgoSpU6fqCcZz587pndfFixeFoUOHCmFhYYJcLhdiY2OFOXPmCGVlZQ61u6G2ZNiwYfXu0y+++ELo2rWrIJPJhJ49ewrbt293qL2msOQ8Zs2apVm3VatWwh133CEcOXLECVbr01hbOXnyZGHYsGH1tunXr58gk8mEmJgYh7ZTEkEQBPv6zhiGYRiGYZovPBqRYRiGYRjGjrDYYhiGYRiGsSMsthiGYRiGYewIiy2GYRiGYRg7wmKLYRiGYRjGjrDYYhiGYRiGsSMsthiGYRiGYewIiy2GYRiGYRg7wmKLYRiGYRjGjrDYYpyOSqXC4MGDcf/99+stLysrQ/v27fHyyy8DAJ599lkkJCRALpdrJtVlGIZxBNxOMU2BxRbjdLy8vLBx40bs2LEDn376qWb5zJkzERYWhkWLFmmWPfbYY3jooYecYSbDMM0YbqeYpuDtbAMYBgC6du2KN998EzNnzsStt96KgwcP4rPPPsOhQ4cgk8kAAO+++y4AoKCgANnZ2c40l2GYZgi3U4y1sNhiXIaZM2fi66+/xqRJk3D06FEsXLgQffv2dbZZDMMwGridYqyBxRbjMkgkEqxevRrdu3dH7969MXfuXGebxDAMowe3U4w1cM4W41KsX78e/v7+OHfuHC5fvuxscxiGYerB7RRjKSy2GJfhjz/+wPLly/H9998jMTERjz/+OARBcLZZDMMwGridYqyBxRbjElRVVWHKlCl4+umnMWLECPz3v//FwYMHsWbNGmebxjAMA4DbKcZ6WGwxLsG8efMgCALefPNNAECnTp2wbNkyvPjiizh//jwAIDc3F1lZWcjPz0d1dTWysrKQlZWF2tpaJ1rOMExzgdspxlokAvs/GSezd+9ejBw5Er/88guGDBmi911KSgrq6urw888/Y8SIEdi7d2+97c+dO4dOnTo5yFqGYZoj3E4xTYHFFsMwDMMwjB3hMCLDMAzDMIwdYbHFMAzDMAxjR1hsMQzDMAzD2BEWWwzDMAzDMHaExRbDMAzDMIwdYbHFMAzDMAxjR1hsMQzDMAzD2BEWWwzDMAzDMHaExRbDMAzDMIwdYbHFMAzDMAxjR1hsMQzDMAzD2BEWWwzDMAzDMHbk/wEra/cKFVhOZQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Generate Random Vectors\n",
    "np.random.seed(42)  # For reproducibility\n",
    "vectors = np.random.randn(100, 2)  # Generate 100 random 2D vectors\n",
    "\n",
    "# Step 2: Plot the Original Distribution\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(vectors[:, 0], vectors[:, 1], color='blue', alpha=0.6)\n",
    "plt.title('Original Vectors')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid(True)\n",
    "\n",
    "# Step 3: Apply L2 Norm\n",
    "norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "print(vectors[0], norms[0])\n",
    "\n",
    "def l2_norm(x):\n",
    "    norm = (x[0]**2 + x[1]**2)**0.5\n",
    "    return x[0] / norm, x[1] / norm\n",
    "\n",
    "a, b = l2_norm(vectors[0])\n",
    "print(a, b)\n",
    "\n",
    "normalized_vectors = vectors / norms  # Normalize each vector by its L2 norm\n",
    "\n",
    "# Step 4: Plot the Normalized Vectors\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(normalized_vectors[:, 0], normalized_vectors[:, 1], color='red', alpha=0.6)\n",
    "plt.title('Normalized Vectors (L2 Norm)')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show both plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E15 - Learning rate decay\n",
    "\n",
    "TODO - explain details\n",
    "\n",
    "<img src=\"images/dec.png\" alt=\"Alt text\" width=\"800\"/><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of training neural networks, particularly when configuring optimizers, it’s important to distinguish between different types of parameters based on their dimensionality. The distinction we make in the code:\n",
    "\n",
    "<code>\n",
    "decay_params = [p for n, p in param_dict.items() if p.dim() >= 2] <br>\n",
    "nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "</code>\n",
    "\n",
    " focuses on whether parameters are weight tensors (2D) or biases and normalization parameters (1D). Here’s a breakdown of why this distinction is important:\n",
    "\n",
    "### Understanding Parameter Types\n",
    "#### Weight Tensors (2D):\n",
    "These are the parameters that involve matrix operations, such as those used in fully connected (dense) layers and convolutional layers.\n",
    "They are typically represented as 2D tensors (matrices) where:\n",
    "The first dimension usually corresponds to the number of input features.\n",
    "The second dimension corresponds to the number of output features.\n",
    "#### Biases and LayerNorm Parameters (1D):\n",
    "Bias parameters are typically 1D tensors. Each neuron usually has a bias term that is a scalar (1D) added to the weighted sum of inputs.\n",
    "Layer normalization parameters are also 1D because they operate across a single dimension (the feature dimension)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E16 - Kernel Fusion\n",
    "Kernel fusion is a performance optimization technique commonly used in GPU programming, especially in frameworks like CUDA or machine learning libraries such as PyTorch and TensorFlow. It involves combining multiple computational operations (kernels) into a single kernel to reduce overhead and improve the efficiency of parallel processing on the GPU.\n",
    "\n",
    "### Why is Kernel Fusion Important?\n",
    "When multiple kernels are executed on a GPU, there are various costs associated with:\n",
    "\n",
    "- Kernel Launch Overhead: Every time a kernel is launched, there is some overhead, even if the kernel is relatively simple.\n",
    "- Memory Access: Separate kernels might require redundant reads and writes to memory, increasing memory bandwidth usage.\n",
    "- Global Synchronization: Each kernel needs to synchronize with the rest of the program, leading to performance losses.\n",
    "\n",
    "### By fusing kernels, we can:\n",
    "\n",
    "- Reduce the number of kernel launches.\n",
    "- Lower memory access overhead by eliminating redundant memory operations.\n",
    "- Keep data in shared memory or registers instead of writing it back to global memory between kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for kernel fusion\n",
    "\n",
    "# Instead of running two operations separately, we can combine them into a single operation.\n",
    "def k1(a, b):\n",
    "    c = a + b\n",
    "    return c\n",
    "\n",
    "def k2(c):\n",
    "    d = c*c\n",
    "\n",
    "for a in range(1000):\n",
    "    for b in range(1000):\n",
    "        k2(k1(a, b))\n",
    "\n",
    "# The kernel fusion is done by combining the two operations into a single operation.\n",
    "def kf(a, b):\n",
    "    d = (a + b)**2\n",
    "\n",
    "for a in range(1000):\n",
    "    for b in range(1000):\n",
    "        kf(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E17 - Gradient accumulation\n",
    "\n",
    "Gradient accumulation is a technique used in deep learning to simulate a larger batch size when memory limitations prevent using large batches. Instead of updating the model parameters after every mini-batch (which is standard in most training routines), gradient accumulation collects gradients over several mini-batches and updates the model only after a specified number of mini-batches (or steps).\n",
    "\n",
    "By using GA we can simulate every arbitrary batch size.\n",
    "\n",
    "### How its handled in Pytorch\n",
    "\n",
    "- Gradient Accumulation: In PyTorch, gradients are accumulated rather than overwritten when calling loss.backward(). This means if you call loss.backward() multiple times before updating the model's weights, the gradients from each call will be summed. This behavior is key when performing gradient accumulation in training (i.e., accumulating gradients over several mini-batches).\n",
    "\n",
    "- Optimizer Step: Once the gradients are accumulated, calling optimizer.step() updates the parameters of the model using the accumulated gradients. The optimizer updates the parameters based on the chosen optimization algorithm (e.g., SGD, Adam, etc.).\n",
    "\n",
    "- Clearing Gradients: After the optimizer.step() call, you typically call optimizer.zero_grad() to clear the gradients in each parameter. This ensures that the gradients from the next batch of data are calculated independently and not added to the previously accumulated gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D1\n",
    "\n",
    "Every head captures a different dimension. We split the embedding by the number of heads:\n",
    "\n",
    "1) `k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)`\n",
    "\n",
    "We need to do so as we concatonate the final attention matrices afterwards:\n",
    "\n",
    "2) `y = y.transpose(1, 2).contiguous().view(B, T, C)`\n",
    "\n",
    "We see in #2 that we transpose the tensor `y` from four to three dimensions. The transformation to three dimensions merges the `h` Attention matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/trans_head.jpg\" alt=\"Alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  6 83]\n",
      " [ 8  9 83]\n",
      " [13 15 72]\n",
      " [ 9 11 80]\n",
      " [17  9 74]\n",
      " [13 14 73]\n",
      " [10 13 77]\n",
      " [12  9 79]\n",
      " [12 12 76]\n",
      " [ 9  8 83]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "# Parameters:\n",
    "n = 100  # number of trials\n",
    "p = [0.1, 0.1, 0.8]  # probabilities of each outcome\n",
    "\n",
    "# Generate random samples\n",
    "samples = multinomial.rvs(n, p, size=10)  # Generate 10 samples\n",
    "\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "data = text[:1000]\n",
    "print(data[:100]) # Get the first 1k characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "We need to generate training examples: sentences.\n",
    "As we are using self-supervised learning we only need to create batches of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n",
      "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]])\n"
     ]
    }
   ],
   "source": [
    "# gpt2 tokenizer has a compression of 3:1\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])\n",
    "\n",
    "# We can convert the tokens to a tensor and reshape it to a 4x6 tensor.\n",
    "# Here we get for example 4 sequences with 6 tokens each.\n",
    "# The problem though is that the lost token has no 'label'\n",
    "buf = torch.tensor(tokens[:24])\n",
    "x = buf.view(4,6)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
      "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
      "          461,    11,  2740,    13,   198])\n",
      "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]])\n",
      "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
      "        [  597,  2252,    11,  3285,   502,  2740],\n",
      "        [   13,   198,   198,  3237,    25,   198],\n",
      "        [ 5248,   461,    11,  2740,    13,   198]])\n"
     ]
    }
   ],
   "source": [
    "buf = torch.tensor(tokens[:24+1])\n",
    "print(buf)\n",
    "x = buf[:-1].view(4,6)\n",
    "y = buf[1:].view(4,6) # y is basically x shifted by one token to the right\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `buf = torch.tensor(tokens[:B*T+1])`\n",
    "\n",
    "This only works with texts who are larger than B*T+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  198,   464,  3290,   357,  6090,   271,  5385,   271,   393,  1680,\n",
      "          271,   300,   929,   385,  5385,   271,     8,   318,   257, 26026,\n",
      "         3474, 45923,   286,   262, 17481,    13,   220,   198,  7583,  1444,\n",
      "          262,  5928,  3290,    11,   340,   373, 26026,  3474,   422,   281,\n",
      "        28881,  3265,   286, 18063,   396, 34973, 23214,   625,  1478,    11,\n",
      "          830]) \n",
      "\n",
      "tensor([[  198,   464,  3290,   357,  6090,   271,  5385,   271,   393,  1680],\n",
      "        [  271,   300,   929,   385,  5385,   271,     8,   318,   257, 26026],\n",
      "        [ 3474, 45923,   286,   262, 17481,    13,   220,   198,  7583,  1444],\n",
      "        [  262,  5928,  3290,    11,   340,   373, 26026,  3474,   422,   281],\n",
      "        [28881,  3265,   286, 18063,   396, 34973, 23214,   625,  1478,    11]]) \n",
      "\n",
      "tensor([[  464,  3290,   357,  6090,   271,  5385,   271,   393,  1680,   271],\n",
      "        [  300,   929,   385,  5385,   271,     8,   318,   257, 26026,  3474],\n",
      "        [45923,   286,   262, 17481,    13,   220,   198,  7583,  1444,   262],\n",
      "        [ 5928,  3290,    11,   340,   373, 26026,  3474,   422,   281, 28881],\n",
      "        [ 3265,   286, 18063,   396, 34973, 23214,   625,  1478,    11,   830]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "B = 5 # Number of batches\n",
    "T = 10 # Maximum sequence length\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "sentence = \"\"\"\n",
    "The dog (Canis familiaris or Canis lupus familiaris) is a domesticated descendant of the wolf.\n",
    "Also called the domestic dog, it was domesticated from an extinct population of Pleistocene wolves over 14,000 years ago.\n",
    "The dog was the first species to be domesticated by humans.\n",
    "Experts estimate that hunter-gatherers domesticated dogs more than 15,000 years ago, which was before the development of agriculture.\n",
    "Due to their long association with humans, dogs have expanded to a large number of domestic individuals and gained the ability to thrive on a starch-rich diet that would be inadequate for other canids.\n",
    "\"\"\"\n",
    "tokens = enc.encode(sentence)\n",
    "buf = torch.tensor(tokens[:B*T+1])\n",
    "x = buf[:-1].view(B,T)\n",
    "y = buf[1:].view(B,T)\n",
    "print(buf,\"\\n\")\n",
    "print(x, \"\\n\")\n",
    "print(y, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MathOperations object at 0x111c6f250>\n"
     ]
    }
   ],
   "source": [
    "class MathOperations:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        print(self)\n",
    "\n",
    "    def add(self, c):\n",
    "        return self.a + self.b + c\n",
    "\n",
    "\n",
    "\n",
    "# Calling the static method without an instance\n",
    "test = MathOperations(1,2)  # Output: 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ\n",
    "\n",
    "### F1 - Block size / sequence length\n",
    "- Sequence length (T) is the length of the input sequence, which can be less than or equal to the block size.\n",
    "- The block size is the maximum length of input sequences that the model can process.\n",
    "\n",
    "### F2 - Nice numbers\n",
    "- Always use numbers to the power of two e.g. batch size of 16, 24, 32 etc.\n",
    "- This is most efficient for the GPU\n",
    "- Always max out the maximum batch size that fits on your GPU\n",
    "\n",
    "### F3 - Powers of 2\n",
    "- Same as for batch optimization, CUDA prefers numbers to the power of 2\n",
    "\n",
    "### F4\n",
    "- We increase the vocab size as we want to work with a 'nicer' number. By doing so we are adding 'fake tokens'. While we are doing more flops it still runs more efficient due to the power of 'nice numbers'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomy\n",
    "\n",
    "### Flops\n",
    "- FLOP stands for Floating Point Operation. It's a single mathematical calculation, like adding or multiplying numbers, especially numbers that have decimal points (floating point numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 42.0545\n",
      "Epoch [2/100], Loss: 27.8467\n",
      "Epoch [3/100], Loss: 19.6823\n",
      "Epoch [4/100], Loss: 12.6528\n",
      "Epoch [5/100], Loss: 6.6512\n",
      "Epoch [6/100], Loss: 2.6205\n",
      "Epoch [7/100], Loss: 0.7446\n",
      "Epoch [8/100], Loss: 0.1650\n",
      "Epoch [9/100], Loss: 0.0393\n",
      "Epoch [10/100], Loss: 0.0175\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
