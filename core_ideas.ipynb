{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1 - Linear layers\n",
    "\n",
    "A **linear layer** (or **fully connected layer**) in a neural network is a layer where each input neuron is connected to each output neuron with a certain weight. This layer performs a linear transformation on the input vector.\n",
    "\n",
    "Given an input vector $\\mathbf{x} \\in \\mathbb{R}^n$ and a weight matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, and a bias vector $\\mathbf{b} \\in \\mathbb{R}^m$, the output $\\mathbf{y}$ of a linear layer can be computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W}$: $\\begin{bmatrix} w_{11} & w_{12} & \\cdots & w_{1n} \\\\ w_{21} & w_{22} & \\cdots & w_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{m1} & w_{m2} & \\cdots & w_{mn} \\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](/Users/soeren/code/gpt2/images/q_k_v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create the following:\n",
    "\n",
    "`nn.Linear(3, 4)`\n",
    "\n",
    "We get a weight matrix according to the dimensionality of of the output neurons $y$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W} = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23} \\\\\n",
    "w_{31} & w_{32} & w_{33} \\\\\n",
    "w_{41} & w_{42} & w_{43}\n",
    "\\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyLinearLayer' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     18\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Print of the state_dict\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/code/gpt2/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyLinearLayer' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "class MyLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    # Define flow\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input_size = 3\n",
    "output_size = 9\n",
    "\n",
    "x = torch.randn(1, input_size)  # Random input tensor\n",
    "\n",
    "model = MyLinearLayer(input_size, output_size)\n",
    "output = model(x)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Print of the state_dict\n",
    "print(state_dict.keys())\n",
    "\n",
    "# Access weights of a specific layer\n",
    "print(state_dict['linear.weight'])\n",
    "\n",
    "# When we look at the weight matrix, it's had exactly the dimension we expectec based on the math above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2 - Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rand tensor \n",
      "\n",
      " tensor([[[-1.1030e+00, -1.5641e+00, -2.4768e-01, -3.3742e-02,  2.4014e+00],\n",
      "         [-9.6585e-01, -1.5485e+00, -1.6011e+00,  2.0549e-01, -3.6359e-01],\n",
      "         [ 6.9281e-01, -1.1421e+00, -1.4178e+00, -1.3838e+00,  1.5762e+00]],\n",
      "\n",
      "        [[-2.8017e-01,  5.7541e-01, -2.3022e-01,  8.5519e-01,  2.9211e-02],\n",
      "         [-1.0060e+00, -7.4851e-02,  1.3897e+00,  9.9862e-01, -1.9336e-02],\n",
      "         [ 1.9113e+00, -5.3601e-01, -8.4473e-01, -2.5183e-01, -2.7436e-01]],\n",
      "\n",
      "        [[ 7.8483e-02, -5.7096e-01, -2.8123e-01,  2.7012e-01, -8.8670e-01],\n",
      "         [ 4.0694e-04, -2.6898e-01,  1.0925e+00,  2.1568e-01,  3.5610e-02],\n",
      "         [ 7.5496e-01,  4.7470e-01, -1.0231e-01,  6.8636e-02,  9.7441e-01]],\n",
      "\n",
      "        [[ 4.6341e-01, -8.9418e-02, -2.9654e-01, -7.6041e-02, -5.3760e-01],\n",
      "         [-4.5800e-01,  4.5955e-01,  8.8806e-01, -1.4692e+00, -1.0137e+00],\n",
      "         [ 8.3687e-01, -9.5285e-01, -1.0146e+00, -2.6947e-01,  1.1127e+00]]])\n",
      "\n",
      "\n",
      "Weights linear layer \n",
      "\n",
      " Parameter containing:\n",
      "tensor([[-0.3384,  0.0427,  0.3315, -0.2314, -0.2123],\n",
      "        [-0.1761,  0.2091,  0.4455, -0.3397, -0.1282],\n",
      "        [ 0.1548, -0.2786,  0.3146,  0.2231, -0.3114],\n",
      "        [-0.2153, -0.1094,  0.0792, -0.2073,  0.0028],\n",
      "        [-0.2084,  0.4075,  0.1133, -0.3105, -0.0688],\n",
      "        [-0.1694,  0.0305, -0.1248,  0.0767, -0.0215],\n",
      "        [ 0.0960, -0.2173,  0.3591, -0.1589, -0.1669],\n",
      "        [-0.3121,  0.1431,  0.2806, -0.2296, -0.1797],\n",
      "        [-0.0274, -0.0332, -0.2513, -0.0930,  0.3732],\n",
      "        [-0.0384,  0.3510,  0.3610, -0.4142,  0.0398],\n",
      "        [-0.3575,  0.2760, -0.1712,  0.1927,  0.0968],\n",
      "        [ 0.0688, -0.2417, -0.1448, -0.0607, -0.3378],\n",
      "        [-0.2426,  0.3623,  0.2699,  0.2461,  0.3481],\n",
      "        [-0.3543, -0.1650, -0.0636,  0.2336, -0.3564],\n",
      "        [ 0.4342, -0.0955, -0.2106,  0.0369, -0.0500]], requires_grad=True)\n",
      "\n",
      "\n",
      "qkv tensor \n",
      "\n",
      " tensor([[[ 0.0308, -0.9606, -0.1970,  0.5622, -0.4546, -0.2475, -0.2184,\n",
      "          -0.0242,  1.0634, -0.3952,  0.5429, -0.2905,  0.8081, -0.0185,\n",
      "          -0.0134],\n",
      "         [ 0.0681, -1.3113,  0.3086,  0.3664, -0.5142, -0.0235, -0.2712,\n",
      "          -0.0025,  0.3450, -1.0928,  0.5084,  0.8305, -0.4886,  1.0577,\n",
      "           0.4767],\n",
      "         [-0.4590, -1.1454, -0.4488,  0.3142, -0.3134, -0.4788, -0.2057,\n",
      "          -0.3942,  1.1117, -0.2122, -0.1223,  0.2612, -0.4102, -0.6713,\n",
      "           0.9638]],\n",
      "\n",
      "        [[ 0.1476, -0.6481,  0.2769, -0.0386,  0.1350, -0.2046, -0.3435,\n",
      "           0.2523, -0.0026, -0.1321,  0.7780, -0.0062,  0.7813,  0.3889,\n",
      "           0.2873],\n",
      "         [ 0.8795,  0.0229,  0.9023,  0.2873,  0.1636, -0.2916,  0.2952,\n",
      "           0.8161, -0.3996,  0.1911,  0.6037, -0.1258,  1.1775,  0.7011,\n",
      "          -0.2991],\n",
      "         [-0.5245, -1.1252,  0.5801, -0.2090, -0.4796, -0.6116,  0.1143,\n",
      "          -0.4544,  0.1183, -0.3818, -0.4497,  0.6718, -0.6971, -0.3156,\n",
      "           1.4486]],\n",
      "\n",
      "        [[ 0.2902, -0.6576,  0.7904,  0.1243, -0.1680, -0.3192,  0.1676,\n",
      "           0.2610, -0.2489, -0.3608,  0.1407,  0.6478, -0.1977,  0.6439,\n",
      "           0.5874],\n",
      "         [ 0.6016, -0.0684,  0.8270,  0.2307,  0.0804, -0.4922,  0.4425,\n",
      "           0.5608, -0.2528,  0.3034,  0.0956,  0.0624,  0.6092,  0.1929,\n",
      "           0.1873],\n",
      "         [-0.1833, -0.6484,  0.0356, -0.0747,  0.0719, -0.4797, -0.2091,\n",
      "          -0.0386,  0.3662,  0.2024,  0.2982, -0.2007,  0.6637, -0.4900,\n",
      "           0.6431]],\n",
      "\n",
      "        [[ 0.1814, -0.5586,  0.5251,  0.0602,  0.0297, -0.4019,  0.0911,\n",
      "           0.2222, -0.1092, -0.0548,  0.1057,  0.4632, -0.0844,  0.2238,\n",
      "           0.6816],\n",
      "         [ 1.3328,  0.7805,  0.4397,  0.5798,  1.0449, -0.4735,  0.6095,\n",
      "           1.3262, -0.4480,  1.1590,  0.0694,  0.3411,  0.1492,  0.2286,\n",
      "          -0.0479],\n",
      "         [-0.5255, -1.2707,  0.0405,  0.0621, -0.5347, -0.4522, -0.1879,\n",
      "          -0.4716,  0.7236, -0.4857, -0.0207,  0.2558, -0.1548, -0.3537,\n",
      "           0.9877]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # Number of examples in the batch\n",
    "sequence_length = 3  # Length of the sequence for each example (e.g. \"I love dogs\"), assuming \"I love dogs\" would result in three tokens: \"I\", \"love\", \"dogs\"\n",
    "embedding_dimension = 5  # Dimension of the embedding space\n",
    "\n",
    "# Create a sample tensor with random values\n",
    "rand_tensor = torch.randn(batch_size, sequence_length, embedding_dimension)\n",
    "print(\"\\n\\nRand tensor \\n\\n\", rand_tensor)\n",
    "\n",
    "B, T, C = rand_tensor.size()\n",
    "\n",
    "# In a second step we are going to use a linear layer with three times the embedding dimension.\n",
    "lin_layer = nn.Linear(embedding_dimension, 3 * embedding_dimension)\n",
    "qkv = lin_layer(rand_tensor)\n",
    "print(\"\\n\\nWeights linear layer \\n\\n\", lin_layer.weight)\n",
    "\n",
    "print(\"\\n\\nqkv tensor \\n\\n\", qkv)\n",
    "q, k, v = qkv.split(embedding_dimension, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2475, -0.2184, -0.0242,  1.0634, -0.3952],\n",
       "          [-0.0235, -0.2712, -0.0025,  0.3450, -1.0928],\n",
       "          [-0.4788, -0.2057, -0.3942,  1.1117, -0.2122]]],\n",
       "\n",
       "\n",
       "        [[[-0.2046, -0.3435,  0.2523, -0.0026, -0.1321],\n",
       "          [-0.2916,  0.2952,  0.8161, -0.3996,  0.1911],\n",
       "          [-0.6116,  0.1143, -0.4544,  0.1183, -0.3818]]],\n",
       "\n",
       "\n",
       "        [[[-0.3192,  0.1676,  0.2610, -0.2489, -0.3608],\n",
       "          [-0.4922,  0.4425,  0.5608, -0.2528,  0.3034],\n",
       "          [-0.4797, -0.2091, -0.0386,  0.3662,  0.2024]]],\n",
       "\n",
       "\n",
       "        [[[-0.4019,  0.0911,  0.2222, -0.1092, -0.0548],\n",
       "          [-0.4735,  0.6095,  1.3262, -0.4480,  1.1590],\n",
       "          [-0.4522, -0.1879, -0.4716,  0.7236, -0.4857]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = 1\n",
    "# B = batch\n",
    "# T = sequence length\n",
    "# head = number of heads\n",
    "# C = embedding dimension --> The tensor k is being split into head smaller chunks. This is a common step in multi-head attention, where the input is projected into multiple subspaces corresponding to different heads.\n",
    "k = k.view(B, T, head, C // head).transpose(1, 2) # transpose starts from zero. Hence, we are transposing T and head.\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E3 - Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), \n",
      "\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      " tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 10  # Example block size\n",
    "\n",
    "ones_matrix = torch.ones(block_size, block_size)\n",
    "lower_triangular_matrix = torch.tril(ones_matrix)\n",
    "reshaped_tensor = lower_triangular_matrix.view(1, 1, block_size, block_size)\n",
    "\n",
    "print(f\"\"\"{ones_matrix}, \\n\\n {lower_triangular_matrix} \\n\\n {reshaped_tensor}\"\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_triangular_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
