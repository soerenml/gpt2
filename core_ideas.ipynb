{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E0 - Fetching the origial model\n",
    "\n",
    "Let's print the weights of the original model:\n",
    "<br><br><br>\n",
    "**Examples**:\n",
    "**Token embeddings**: \n",
    "`transformer.wte.weight torch.Size([50257, 768])`: Basically a lookup table for the vocabulary used, `50257` tokens, with an embedding dimension of `768` for each token.\n",
    "<br><br>\n",
    "**Positional embeddings**\n",
    "`transformer.wpe.weight torch.Size([1024, 768])`: Basically a lookup table for each of the `1024` position with an embedding dimension of `768`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2') #124M\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "# Print key and values of the tensor\n",
    "for k in sd_hf:\n",
    "    print(k, sd_hf[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/trans_arch.jpg\" alt=\"Alt text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1 - Linear layers\n",
    "\n",
    "A **linear layer** (or **fully connected layer**) in a neural network is a layer where each input neuron is connected to each output neuron with a certain weight. This layer performs a linear transformation on the input vector.\n",
    "\n",
    "Given an input vector $\\mathbf{x} \\in \\mathbb{R}^n$ and a weight matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, and a bias vector $\\mathbf{b} \\in \\mathbb{R}^m$, the output $\\mathbf{y}$ of a linear layer can be computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W}$: $\\begin{bmatrix} w_{11} & w_{21} & \\cdots & w_{n1} \\\\ w_{12} & w_{22} & \\cdots & w_{n2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{1m} & w_{2m} & \\cdots & w_{nm} \\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/q_k_v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create the following:\n",
    "\n",
    "`nn.Linear(3, 4)`\n",
    "\n",
    "We get a weight matrix according to the dimensionality of of the output neurons $y$\n",
    "\n",
    "- **Input Vector** $\\mathbf{x}$: $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "- **Weight Matrix** $\\mathbf{W} = \\begin{bmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32} \\\\\n",
    "w_{13} & w_{23} & w_{33} \\\\\n",
    "w_{14} & w_{24} & w_{34}\n",
    "\\end{bmatrix}$\n",
    "- **Bias Vector** $\\mathbf{b}$: $\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix}$\n",
    "- **Output Vector** $\\mathbf{y}$: $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['linear.weight', 'linear.bias'])\n",
      "tensor([[-0.3509, -0.1814, -0.2050],\n",
      "        [ 0.3712,  0.2231,  0.5085],\n",
      "        [-0.5417,  0.5079, -0.1018],\n",
      "        [ 0.5199,  0.3205, -0.4828],\n",
      "        [-0.0970,  0.5154,  0.3764],\n",
      "        [ 0.3046,  0.5113, -0.1598],\n",
      "        [-0.4367, -0.1647,  0.3665],\n",
      "        [-0.1620,  0.2772,  0.1710],\n",
      "        [-0.1128, -0.3102, -0.1087]])\n"
     ]
    }
   ],
   "source": [
    "class MyLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    # Define flow\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input_size = 3\n",
    "output_size = 9\n",
    "\n",
    "x = torch.randn(1, input_size)  # Random input tensor\n",
    "\n",
    "model = MyLinearLayer(input_size, output_size)\n",
    "output = model(x)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Print of the state_dict\n",
    "print(state_dict.keys())\n",
    "\n",
    "# Access weights of a specific layer\n",
    "print(state_dict['linear.weight'])\n",
    "\n",
    "# When we look at the weight matrix, it's had exactly the dimension we expectec based on the math above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2 - Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), \n",
      "\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      " tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 10  # The maximum length of the input sequence\n",
    "\n",
    "ones_matrix = torch.ones(block_size, block_size)\n",
    "lower_triangular_matrix = torch.tril(ones_matrix)\n",
    "mask = lower_triangular_matrix.view(1, 1, block_size, block_size)\n",
    "\n",
    "print(f\"\"\"{ones_matrix}, \\n\\n {lower_triangular_matrix} \\n\\n {mask}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E3 - Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input tensor \n",
      "\n",
      " tensor([[[ 0.8794,  0.0162, -0.0192,  0.2003, -0.7611],\n",
      "         [ 1.0562, -0.5795, -0.5137,  1.1017,  1.6426],\n",
      "         [ 0.6179, -0.3312, -0.2809,  1.2661, -0.7663]],\n",
      "\n",
      "        [[ 2.1343,  0.2184,  0.4019,  0.7011, -0.7549],\n",
      "         [ 0.0954, -0.9134, -0.2705,  0.0145,  1.8395],\n",
      "         [ 0.1218, -0.3264,  0.8258, -0.9379,  1.2374]],\n",
      "\n",
      "        [[-0.8918, -0.0756,  0.2246, -0.9340,  0.6094],\n",
      "         [ 0.6002, -1.3578,  0.9987, -0.9037,  0.3855],\n",
      "         [-0.4717, -1.1441,  0.0813,  0.0183,  0.6153]],\n",
      "\n",
      "        [[ 1.0018,  2.1894, -0.4847,  0.9124,  1.0803],\n",
      "         [-1.0512,  0.0516,  0.9709, -0.0821, -0.7628],\n",
      "         [-1.0259, -1.8742, -0.3618, -0.0867, -1.1570]]])\n",
      "\n",
      "\n",
      "Weights linear layer \n",
      "\n",
      " Parameter containing:\n",
      "tensor([[ 0.1323, -0.4311, -0.3484, -0.3857, -0.2718],\n",
      "        [-0.2823, -0.4313, -0.2143,  0.0630, -0.2421],\n",
      "        [ 0.0306, -0.1347,  0.4031,  0.0497,  0.1724],\n",
      "        [ 0.0738,  0.0929, -0.2537, -0.3381,  0.0239],\n",
      "        [-0.2294, -0.3157,  0.1039, -0.0898,  0.0561],\n",
      "        [-0.2480, -0.2124, -0.2918,  0.3381, -0.3456],\n",
      "        [-0.3464, -0.0233, -0.2880,  0.3966,  0.3284],\n",
      "        [-0.2238, -0.4415, -0.3083,  0.2162,  0.2018],\n",
      "        [ 0.2484, -0.2777, -0.4087,  0.3304,  0.2563],\n",
      "        [ 0.2166,  0.2253,  0.3157,  0.4382, -0.4094],\n",
      "        [ 0.2178, -0.4416,  0.0459, -0.0010, -0.3850],\n",
      "        [ 0.2625, -0.0398,  0.0874, -0.0344,  0.0478],\n",
      "        [-0.3914,  0.1948,  0.3009, -0.1443,  0.3758],\n",
      "        [-0.3381, -0.0573, -0.2979, -0.3809, -0.1226],\n",
      "        [ 0.0883,  0.3488,  0.2408, -0.1695,  0.1864]], requires_grad=True)\n",
      "\n",
      "\n",
      "qkv tensor \n",
      "\n",
      " tensor([[[ 0.5620, -0.4771, -0.1193, -0.1770, -0.0912, -0.1881, -0.9082,\n",
      "          -0.3982, -0.1748,  0.4831,  0.0430,  0.4818, -0.7781, -0.1399,\n",
      "           0.3219],\n",
      "         [ 0.0133, -0.6892,  0.2262, -0.3412,  0.0587, -0.4870,  0.3338,\n",
      "           0.6576,  1.1507, -0.3581, -0.6044,  0.5926, -0.3389, -0.6563,\n",
      "           0.3061],\n",
      "         [ 0.3586, -0.1289, -0.1339, -0.5227, -0.0447,  0.3891, -0.3131,\n",
      "           0.1238,  0.3145,  0.7347,  0.1284,  0.3672, -0.9779, -0.3589,\n",
      "          -0.0669]],\n",
      "\n",
      "        [[ 0.2992, -0.9787,  0.0876, -0.3416, -0.4438, -0.4980, -1.2683,\n",
      "          -0.7886,  0.0757,  1.1504,  0.2435,  0.8231, -1.1731, -0.8928,\n",
      "           0.5210],\n",
      "         [ 0.3113, -0.4422,  0.3197, -0.1326,  0.5185, -0.6843,  0.2379,\n",
      "           0.7497,  0.5966, -1.1216, -0.7297,  0.4218,  0.2762,  0.0052,\n",
      "           0.3843],\n",
      "         [ 0.2108, -0.8520,  0.5322, -0.0466,  0.4928, -1.2493, -0.6761,\n",
      "          -0.1807, -0.4770, -0.8084, -0.7001,  0.5052,  0.6213,  0.0726,\n",
      "           0.9045]],\n",
      "\n",
      "        [[ 0.3473, -0.3930,  0.1170,  0.0381,  0.5481, -0.6575, -0.3624,\n",
      "          -0.0052, -0.7125, -0.9024, -0.8175,  0.1464,  0.6494,  0.6556,\n",
      "           0.6400],\n",
      "         [ 0.8769, -0.3709,  0.6103, -0.1829,  0.6759, -0.8934, -1.1339,\n",
      "          -0.0502, -0.3495, -0.5188,  0.1954,  0.6451, -0.0400,  0.0099,\n",
      "           0.4641],\n",
      "         [ 0.5444,  0.0385,  0.2644, -0.3156,  0.6890, -0.1730, -0.0621,\n",
      "           0.6239,  0.0634, -0.6826, -0.2640,  0.2542,  0.0985,  0.2540,\n",
      "           0.1097]],\n",
      "\n",
      "        [[-0.9717, -1.7501, -0.2431, -0.0448, -0.8145, -0.9399,  0.0199,\n",
      "          -0.7161,  0.1496,  0.4105, -1.6209,  0.4503,  0.0465, -0.6641,\n",
      "           1.2012],\n",
      "         [ 0.0558, -0.1769,  0.2017, -0.4720,  0.4686, -0.1005, -0.6379,\n",
      "          -0.3483, -1.1627,  0.2624, -0.3467,  0.0698,  0.3225,  0.3236,\n",
      "           0.4498],\n",
      "         [ 1.4625,  1.0273, -0.1435, -0.3189,  0.9108,  0.8259, -0.3492,\n",
      "           0.8266, -0.1793, -0.4273,  0.5998,  0.0179, -0.6111,  0.8724,\n",
      "          -0.6133]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # Number of examples in the batch\n",
    "sequence_length = 3  # Length of the sequence for each example (e.g. \"I love dogs\"), assuming \"I love dogs\" would result in three tokens: \"I\", \"love\", \"dogs\"\n",
    "embedding_dimension = 5  # Dimension of the embedding space\n",
    "\n",
    "# Create a sample tensor with random values\n",
    "input_tensor = torch.randn(batch_size, sequence_length, embedding_dimension)\n",
    "print(\"\\n\\nInput tensor \\n\\n\", input_tensor)\n",
    "\n",
    "B, T, C = input_tensor.size()\n",
    "\n",
    "# In a second step we are going to use a linear layer with three times the embedding dimension.\n",
    "lin_layer = nn.Linear(embedding_dimension, 3 * embedding_dimension)\n",
    "qkv = lin_layer(input_tensor)\n",
    "print(\"\\n\\nWeights linear layer \\n\\n\", lin_layer.weight)\n",
    "\n",
    "print(\"\\n\\nqkv tensor \\n\\n\", qkv)\n",
    "q, k, v = qkv.split(embedding_dimension, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1881, -0.9082, -0.3982, -0.1748,  0.4831],\n",
       "          [-0.4870,  0.3338,  0.6576,  1.1507, -0.3581],\n",
       "          [ 0.3891, -0.3131,  0.1238,  0.3145,  0.7347]]],\n",
       "\n",
       "\n",
       "        [[[-0.4980, -1.2683, -0.7886,  0.0757,  1.1504],\n",
       "          [-0.6843,  0.2379,  0.7497,  0.5966, -1.1216],\n",
       "          [-1.2493, -0.6761, -0.1807, -0.4770, -0.8084]]],\n",
       "\n",
       "\n",
       "        [[[-0.6575, -0.3624, -0.0052, -0.7125, -0.9024],\n",
       "          [-0.8934, -1.1339, -0.0502, -0.3495, -0.5188],\n",
       "          [-0.1730, -0.0621,  0.6239,  0.0634, -0.6826]]],\n",
       "\n",
       "\n",
       "        [[[-0.9399,  0.0199, -0.7161,  0.1496,  0.4105],\n",
       "          [-0.1005, -0.6379, -0.3483, -1.1627,  0.2624],\n",
       "          [ 0.8259, -0.3492,  0.8266, -0.1793, -0.4273]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_head = 1\n",
    "# B = batch\n",
    "# T = sequence length\n",
    "# head = number of heads\n",
    "# C = embedding dimension --> The tensor k is being split into head smaller chunks. This is a common step in multi-head attention, where the input is projected into multiple subspaces corresponding to different heads.\n",
    "k = k.view(B, T, n_head, C // n_head).transpose(1, 2) # transpose starts from zero. Hence, we are transposing T and head.\n",
    "q = q.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#E4 - Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3, 5]) torch.Size([4, 1, 3, 5]) torch.Size([4, 1, 3, 5])\n",
      "tensor([[[[ 0.5620, -0.4771, -0.1193, -0.1770, -0.0912],\n",
      "          [ 0.0133, -0.6892,  0.2262, -0.3412,  0.0587],\n",
      "          [ 0.3586, -0.1289, -0.1339, -0.5227, -0.0447]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2992, -0.9787,  0.0876, -0.3416, -0.4438],\n",
      "          [ 0.3113, -0.4422,  0.3197, -0.1326,  0.5185],\n",
      "          [ 0.2108, -0.8520,  0.5322, -0.0466,  0.4928]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3473, -0.3930,  0.1170,  0.0381,  0.5481],\n",
      "          [ 0.8769, -0.3709,  0.6103, -0.1829,  0.6759],\n",
      "          [ 0.5444,  0.0385,  0.2644, -0.3156,  0.6890]]],\n",
      "\n",
      "\n",
      "        [[[-0.9717, -1.7501, -0.2431, -0.0448, -0.8145],\n",
      "          [ 0.0558, -0.1769,  0.2017, -0.4720,  0.4686],\n",
      "          [ 1.4625,  1.0273, -0.1435, -0.3189,  0.9108]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "tensor([[[[-0.1881, -0.4870,  0.3891],\n",
      "          [-0.9082,  0.3338, -0.3131],\n",
      "          [-0.3982,  0.6576,  0.1238],\n",
      "          [-0.1748,  1.1507,  0.3145],\n",
      "          [ 0.4831, -0.3581,  0.7347]]],\n",
      "\n",
      "\n",
      "        [[[-0.4980, -0.6843, -1.2493],\n",
      "          [-1.2683,  0.2379, -0.6761],\n",
      "          [-0.7886,  0.7497, -0.1807],\n",
      "          [ 0.0757,  0.5966, -0.4770],\n",
      "          [ 1.1504, -1.1216, -0.8084]]],\n",
      "\n",
      "\n",
      "        [[[-0.6575, -0.8934, -0.1730],\n",
      "          [-0.3624, -1.1339, -0.0621],\n",
      "          [-0.0052, -0.0502,  0.6239],\n",
      "          [-0.7125, -0.3495,  0.0634],\n",
      "          [-0.9024, -0.5188, -0.6826]]],\n",
      "\n",
      "\n",
      "        [[[-0.9399, -0.1005,  0.8259],\n",
      "          [ 0.0199, -0.6379, -0.3492],\n",
      "          [-0.7161, -0.3483,  0.8266],\n",
      "          [ 0.1496, -1.1627, -0.1793],\n",
      "          [ 0.4105,  0.2624, -0.4273]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the tensors: batch, sequence length, number of heads, embedding dimension (i.e. the embedding)\n",
    "print(q.shape, k.shape, v.shape)\n",
    "\n",
    "\n",
    "print(q)\n",
    "print(\"\\n\\n\")\n",
    "print(k.transpose(-2, -1))\n",
    "\n",
    "# What happens is that the transformation allows us to multiply the q and k tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1881, -0.9082, -0.3982, -0.1748,  0.4831],\n",
      "         [-0.4870,  0.3338,  0.6576,  1.1507, -0.3581],\n",
      "         [ 0.3891, -0.3131,  0.1238,  0.3145,  0.7347]]],\n",
      "       grad_fn=<SelectBackward0>), \n",
      "\n",
      " tensor([[[-0.1881, -0.4870,  0.3891],\n",
      "         [-0.9082,  0.3338, -0.3131],\n",
      "         [-0.3982,  0.6576,  0.1238],\n",
      "         [-0.1748,  1.1507,  0.3145],\n",
      "         [ 0.4831, -0.3581,  0.7347]]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check batches 0\n",
    "# This transformation allows us to multiply the q and k tensors.\n",
    "print(f\"{k[0]}, \\n\\n {k.transpose(-2, -1)[0]}\")\n",
    "\n",
    "# The @ operator is used for matrix multiplication.\n",
    "# k.size(-1) is the length of the embeddings.\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E5 - Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.1619,    -inf,    -inf],\n",
      "          [ 0.2779, -0.2243,    -inf],\n",
      "          [ 0.0772, -0.3985, -0.0152]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2177,    -inf,    -inf],\n",
      "          [ 0.3310, -0.3306,    -inf],\n",
      "          [ 0.5006, -0.2364, -0.0714]]],\n",
      "\n",
      "\n",
      "        [[[-0.2720,    -inf,    -inf],\n",
      "          [-0.4136, -0.3042,    -inf],\n",
      "          [-0.3444, -0.3535, -0.1887]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3181,    -inf,    -inf],\n",
      "          [-0.0352,  0.3169,    -inf],\n",
      "          [-0.4137, -0.0638,  0.1782]]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We now apply the mask and set.\n",
    "# We replace values with '-inf' where the mask is zero.\n",
    "# While mask has the dimensions of the the sequence length we define in the overall setting (e.g. 1024)\n",
    "# It automatically adapts to the sequence length of the current batch (e.g. attn)\n",
    "att = att.masked_fill(mask=mask[:, :, :T, :T] == 0, value=float('-inf'))\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E6 - Merging\n",
    "\n",
    "### Step 1 - Check for contigiousy\n",
    "Contiguous Memory:  `[   Book 1   ] [   Book 2   ] [   Book 3   ] [   Book 4   ] [   Book 5   ]`\n",
    "\n",
    "Non-Contiguous Memory:  `[         ] [   Book 3   ] [         ] [   Book 1   ] [   Book 2   ]`\n",
    "\n",
    "Think of `.contiguous()` as a librarian who reorganizes the books on the shelf:\n",
    "`[   Book 1   ] [   Book 2   ] [   Book 3   ] [   Book 4   ] [   Book 5   ]`\n",
    "\n",
    "### Step 2 - Merging the `heads`\n",
    "We started out with `n_head`. In reality we would use several heads to capture different different \"perspectives\".\n",
    "In order to merge the 'heads' we apply .view(B, T, C) to our originally four-dimensional tensor (B, T, n_heads, C).\n",
    "We are essentially flattening (combining) the elements in the fourth dimension into the existing dimensions (B, T, C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0430,  0.4818, -0.7781, -0.1399,  0.3219],\n",
      "         [-0.2011,  0.5236, -0.6125, -0.3346,  0.3160],\n",
      "         [-0.0851,  0.4677, -0.7423, -0.3454,  0.1781]],\n",
      "\n",
      "        [[ 0.2435,  0.8231, -1.1731, -0.8928,  0.5210],\n",
      "         [-0.0878,  0.6865, -0.6798, -0.5871,  0.4745],\n",
      "         [-0.2452,  0.6413, -0.3379, -0.4157,  0.5949]],\n",
      "\n",
      "        [[-0.8175,  0.1464,  0.6494,  0.6556,  0.6400],\n",
      "         [-0.2834,  0.4094,  0.2859,  0.3151,  0.5472],\n",
      "         [-0.2951,  0.3427,  0.2294,  0.3045,  0.3887]],\n",
      "\n",
      "        [[-1.6209,  0.4503,  0.0465, -0.6641,  1.2012],\n",
      "         [-0.8728,  0.2269,  0.2086, -0.0842,  0.7600],\n",
      "         [-0.2434,  0.1376, -0.1420,  0.3246,  0.1729]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.0402,  0.5025,  0.7070, -0.5162,  0.6892],\n",
      "         [-0.1152,  0.4472,  0.7358, -0.3647,  0.6219],\n",
      "         [-0.0133,  0.4823,  0.7441, -0.4807,  0.5555]],\n",
      "\n",
      "        [[ 0.1551,  1.1552,  0.9167, -0.3060,  0.7705],\n",
      "         [-0.0968,  0.6976,  0.7757, -0.2331,  0.7158],\n",
      "         [-0.2705,  0.4810,  0.6702, -0.1059,  0.7661]],\n",
      "\n",
      "        [[-0.7204, -0.5923,  0.2955, -0.0808,  0.7469],\n",
      "         [-0.4351, -0.1187,  0.3447, -0.1709,  0.8159],\n",
      "         [-0.3989, -0.1624,  0.3594, -0.2582,  0.7164]],\n",
      "\n",
      "        [[-0.9550,  0.1387,  0.9260,  0.5125,  0.6835],\n",
      "         [-0.6582, -0.0911,  0.5612,  0.0895,  0.6544],\n",
      "         [-0.2177, -0.1294,  0.4563, -0.5051,  0.5528]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "att = F.softmax(att, dim=-1)\n",
    "y = att @ v # matrix multiplication attention * values\n",
    "y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "print(y)\n",
    "c_proj = nn.Linear(embedding_dimension, embedding_dimension)\n",
    "y = c_proj(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 5]) torch.Size([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# When we compare the shape of the output tensor with the input tensor, we see that the dimensions are the same.\n",
    "# batch_size=4, sequence_length=3, embedding_dimension=5\n",
    "print(input_tensor.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E7 - Stacking blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of 'test' the origial code uses the class Block(config)\n",
    "# This leads to a list with n.layers of blocks. E.g. n.layer = 3 --> 3 blocks stacked on each other\n",
    "blocks = ['test' for _ in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E8 - ModuleDict\n",
    "\n",
    "`ModuleDict` allows to create a dict with layers as items.\n",
    "It therefore allows us to store a collection of modules in a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'lin1': nn.Linear(5, 20),  # Input size is 5, output size is 20\n",
    "            'lin2': nn.Linear(20, 100),  # Input size is 20, output size is 3\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['lin1'](x)\n",
    "        x = self.layers['lin2'](x)\n",
    "        return x\n",
    "\n",
    "# Dummy tensor\n",
    "dummy_input = torch.randn(3, 5)  # Batch size of 3, input size of 5\n",
    "\n",
    "model = MyModel()\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E9 - Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) [-0.28834486 -0.42112446 -0.90688705 -1.8311492  -1.5700469 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "block_size = 200 # The maximum length of the input sequence\n",
    "n_embd = 5\n",
    "T = 100 # Sequence length\n",
    "\n",
    "# Arrange returns a 1D tensor with values from the start (0 in this case) to the end (T), excluding T.\n",
    "# it's similar to Python’s built-in range function but returns a tensor instead of a list.\n",
    "pos = torch.arange(start=0, end=T, step=1, dtype=torch.long, device='cpu') # Shape (T)\n",
    "wpe = nn.Embedding(block_size, n_embd)\n",
    "pos_emd = wpe(pos)\n",
    "\n",
    "numpy_array = pos_emd.detach().numpy()\n",
    "\n",
    "# The the number '1' is embedded into a 5-dimensional vector.\n",
    "print(pos[1], numpy_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E10 - Regular Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 588, 6844, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7135e-01, -2.1596e+00,  3.2757e-01,  9.1524e-01,  9.1762e-01],\n",
       "        [ 2.0205e-03, -7.0155e-01, -7.5294e-01, -5.6002e-01, -7.3883e-01],\n",
       "        [-2.8199e-01, -1.6643e+00,  6.9581e-01, -4.5845e-01,  3.5994e-01],\n",
       "        [-1.1862e+00,  3.1415e-01, -8.7480e-01, -1.6340e+00,  4.5589e-01]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"I like dogs!\")\n",
    "print(tokens) # Check tokens created here: https://tiktokenizer.vercel.app/?model=gpt2\n",
    "\n",
    "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # Shape: (1, 8)\n",
    "\n",
    "n_embd = 64  # Adjust embedding dimension as needed\n",
    "wte = nn.Embedding(50257, 5) # 50257 is the number of tokens in the GPT-2 vocabulary. This needs to match.\n",
    "embeddings = wte(tokens)  # Shape: (1, 8, n_embd)\n",
    "\n",
    "embeddings[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11 - Multinomal distribution\n",
    "\n",
    "I case we have a set of probabilities a multinomal distribution can help to introduce some randomness.\n",
    "We see, for example, that p=0.8 is not picked 80% of the time. In context of a LLM we want to introduce some randomness:\n",
    "If we only choose the most probable token at each step this would leading to repetitive and predictable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 10 81]\n",
      " [ 7  8 85]\n",
      " [14  7 79]\n",
      " [13 11 76]\n",
      " [11  3 86]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "# Parameters:\n",
    "n = 100  # number of trials\n",
    "p = [0.1, 0.1, 0.8]  # probabilities of each outcome\n",
    "\n",
    "# Generate random samples\n",
    "samples = multinomial.rvs(n, p, size=5)  # Generate 10 samples\n",
    "\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E12 - weight sharing\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E13 - Residual stream\n",
    "\n",
    "The residual stream adds the input of an layer to its output.<br>\n",
    "We are not talking about weights here. We are talking about the acutal- in and outputs.\n",
    "<br>\n",
    "\n",
    "Example:<br>\n",
    "`attn_output, _ = self.attention(x, x, x)`<br>\n",
    "`x = x + attn_output # Residual connection: add the input 'x' back to the output`<br>\n",
    "`x = self.norm1(x)`<br>\n",
    "\n",
    "<br>\n",
    "The problem with this approch is an explosion of the deviation of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/res.png\" alt=\"Alt text\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1456)\n",
      "tensor(0.9557)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(30)\n",
    "n = 100\n",
    "for i in range(n):\n",
    "    # We create random values with mean 0 and standard deviation 1.\n",
    "    x += torch.randn(30)\n",
    "# Just after 100 interations the std moved to >10!\n",
    "print(x.std())\n",
    "\n",
    "# We can fix this by dividing the sum by the square root of the number of iterations.\n",
    "x = torch.zeros(30)\n",
    "for i in range(n):\n",
    "    x += n**-0.5 * torch.randn(30)\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E14 - Quantization\n",
    "\n",
    "Quantization is used to speed up training time and work more memory efficient.\n",
    "It works as follows:\n",
    "\n",
    "<img src=\"images/prec2.png\" alt=\"Alt text\" width=\"400\"/>\n",
    "\n",
    "Different procedures are available with `TF32` being the gold standard for training.\n",
    "\n",
    "<img src=\"images/prec1.png\" alt=\"Alt text\" width=\"400\"/>\n",
    "\n",
    "What is missing here is sign (0 for positive and 1 for negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.unsqueeze()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6])\n",
      "torch.Size([1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2009, 2003, 1037, 3722,  102],\n",
       "        [ 101, 2009, 2003, 1037, 3722,  102],\n",
       "        [ 101, 2009, 2003, 1037, 3722,  102],\n",
       "        [ 101, 2009, 2003, 1037, 3722,  102],\n",
       "        [ 101, 2009, 2003, 1037, 3722,  102]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adds a new dimension to the tensor at the specified position. In this case, 0 indicates that the new dimension will be added at the beginning.\n",
    "Adding this extra dimension is often necessary for batching in PyTorch. Transformer models expect input tensors to have a specific shape, typically [batch_size, sequence_length].\n",
    "By unsqueezing the tensor, we create a batch dimension, even if there's only one sequence in the batch.\n",
    "\"\"\"\n",
    "\n",
    "tokens = [101, 2009, 2003, 1037, 3722, 102]\n",
    "tens = torch.tensor(tokens)\n",
    "\n",
    "# The shape of the tensor is (6,) because it's a 1D tensor with 6 elements.\n",
    "# For our model we need a batch dimension, so we add a dimension at the beginning.\n",
    "print(tens.shape)\n",
    "\n",
    "tens2 = tens.unsqueeze(0)\n",
    "print(tens2.shape)\n",
    "\n",
    "# Repeat the tensor 5 times along the first dimension\n",
    "tens.unsqueeze(0).repeat(5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `@classmethod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof!\n",
      "Miau!\n",
      "Can't call method without instance\n"
     ]
    }
   ],
   "source": [
    "class Dog():\n",
    "    def bark(self):\n",
    "        print(\"Woof!\")\n",
    "\n",
    "\n",
    "class Cat():\n",
    "    @classmethod\n",
    "    def miau(self):\n",
    "        print(\"Miau!\")\n",
    "\n",
    "# Using the Dog class to create an instance of a dog and call the bark method\n",
    "doggi = Dog()\n",
    "doggi.bark()\n",
    "\n",
    "# While the Dog class has an instance method, the Cat class has a class method.\n",
    "# This means that the Cat class method can be called without creating an instance of the class.\n",
    "Cat.miau()\n",
    "try:\n",
    "    Dog.bark()\n",
    "except:\n",
    "    print(\"Can't call method without instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `state_dict()`\n",
    "\n",
    "`state_dict()` returns the different model layers.\n",
    "\n",
    "h.# stand for the individual heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model information: GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Number of heads: 12\n",
      "Number of layers: 12\n",
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # Load the GPT-2 model\n",
    "sd_hf = model_hf.state_dict() # State dict are the raw tensors\n",
    "\n",
    "print(\"Model information:\", model_hf.config)\n",
    "print(\"Number of heads:\", model_hf.config.num_attention_heads)\n",
    "print(\"Number of layers:\", model_hf.config.num_hidden_layers)\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.8821e-02, -1.9742e-01,  4.0267e-03,  1.1347e-02,  6.3824e-02,\n",
       "        -1.0501e-01,  3.6937e-02, -1.6803e-01, -4.9111e-02, -5.6461e-02,\n",
       "        -2.4560e-03,  1.3503e-02, -4.1711e-03,  1.5115e-02,  1.6595e-02,\n",
       "        -1.3808e-01, -6.3314e-03, -4.6150e-02,  2.6675e-02, -2.0417e-01,\n",
       "         1.3454e-02, -3.6267e-02,  1.9301e-02, -2.5931e-02,  8.0243e-03,\n",
       "         8.4712e-03, -1.9906e-02,  6.6802e-02,  7.1151e-03, -2.6618e-02,\n",
       "         2.0829e-02, -3.3732e-02, -8.2898e-03,  9.8622e-03, -2.7369e-02,\n",
       "        -9.9118e-02, -7.5254e-01,  2.3550e-02, -3.0513e-02,  7.7456e-02,\n",
       "         3.4301e-03,  7.1132e-03,  2.6479e-02, -1.2113e-03,  1.1219e-01,\n",
       "        -2.0606e-03, -2.2458e-02, -2.2287e-02,  2.3570e-02,  3.9777e-01,\n",
       "         1.8856e-02,  2.0280e-02,  6.3043e-01,  2.3146e-02, -4.6894e-02,\n",
       "         4.0653e+00, -1.7403e-02, -5.1683e-02,  7.2271e-02, -7.9312e-02,\n",
       "         4.0248e-02,  1.9908e-02, -4.6380e-02, -2.8380e-02,  7.2535e-03,\n",
       "         2.6772e-02,  1.4972e-03, -2.9892e-01, -1.1627e-01,  9.1860e-03,\n",
       "        -2.3379e-02,  7.8674e-02,  3.6326e-02, -4.1279e-02,  3.5415e-02,\n",
       "         2.6852e-02,  3.7368e-04,  4.5535e-03,  2.5672e-02, -3.8062e-02,\n",
       "         1.5198e-02, -1.1324e-03, -3.6556e-02, -9.6938e-03, -7.7945e-02,\n",
       "        -5.0139e-02, -1.0231e+00, -1.2559e-02, -1.8484e-03,  6.6347e-02,\n",
       "         1.1819e-03, -3.3319e-02, -1.1513e-01,  2.1627e-02, -4.3646e-02,\n",
       "        -1.1303e-02, -7.7890e-03,  1.6647e-02, -7.2352e-03,  1.0707e-02,\n",
       "         1.3787e-02, -3.7777e-02, -1.2361e-01, -1.7224e-01, -1.8045e-02,\n",
       "         6.3456e-03, -4.8349e-02,  4.1328e-01, -1.0673e-03, -1.1704e-02,\n",
       "         1.6934e-02,  3.2545e-03, -2.3711e-02,  5.9829e-02, -5.9910e-02,\n",
       "        -1.4971e-02, -8.3736e-01, -3.8427e-02, -7.6987e-03,  4.9164e-02,\n",
       "         2.3555e-02, -5.3212e-02,  1.8699e-03, -1.5544e-01,  6.5622e-02,\n",
       "         5.2014e-03, -1.3533e-02,  2.8135e-02, -1.0581e-01,  5.5449e-02,\n",
       "         3.4420e-02,  1.4542e-02,  1.7542e-02, -2.5723e-03, -1.6590e-03,\n",
       "         1.6827e-02, -3.0037e-02, -1.1984e-01,  8.1251e-02, -4.5297e-01,\n",
       "         1.6045e-02,  4.2054e-03,  3.4240e-02, -2.0139e-02, -3.2975e-02,\n",
       "        -5.1996e-02, -3.6392e-02, -1.1540e-02,  2.2939e-02,  7.3400e-02,\n",
       "        -4.6437e-02, -8.2206e-02, -1.3095e-02,  1.5196e-02,  1.6183e-01,\n",
       "        -5.1890e-02,  4.9133e-02,  5.4630e-03, -3.1565e-02, -2.2381e-02,\n",
       "        -1.6066e-01,  2.7186e-02,  3.3445e-02, -2.9198e-02,  3.6843e-02,\n",
       "         1.0586e-03,  2.1095e-02,  6.4639e-02, -2.7470e-02,  8.8352e-02,\n",
       "         5.5776e-02, -2.3872e-02, -1.2993e-02, -2.6942e-04,  1.0038e-01,\n",
       "        -9.2795e-02, -7.9185e-01, -4.2195e-02,  2.4330e-02, -1.9673e-02,\n",
       "        -2.2679e-02,  1.3850e-02, -2.0604e-02,  9.8258e-02,  3.8011e-02,\n",
       "         2.3985e-02,  2.6789e-02,  3.1317e-02, -1.0934e-02, -7.4405e-02,\n",
       "        -4.1879e-02, -7.8305e-03, -9.1414e-02,  1.4571e-01, -2.2372e-02,\n",
       "        -7.4952e-02,  3.6658e-02,  4.8322e-03,  1.1366e-02,  7.7893e-02,\n",
       "        -5.3751e-02, -1.1380e-02, -1.0407e-02, -1.5257e-02,  3.0442e-02,\n",
       "        -5.0440e-03,  2.6727e-02,  5.8141e-03, -3.8137e-02,  1.3994e-02,\n",
       "        -2.9547e-03,  4.7238e-02,  5.7311e-02,  6.8433e-02, -4.4144e-02,\n",
       "         4.0254e-03,  3.9776e-02, -3.2052e-02,  2.8832e-02, -2.5070e-02,\n",
       "        -3.2514e-03, -7.3319e-02,  5.0036e-02, -7.9418e-02, -1.7448e-02,\n",
       "        -6.5178e-03,  8.9161e-03,  1.6941e-02,  2.8391e-02,  1.0850e-02,\n",
       "         1.5500e-02, -1.5806e-02, -1.5287e-02, -2.9555e-02, -4.0786e-02,\n",
       "        -5.6020e-02, -3.3106e-02,  3.6305e-02, -5.9147e-03, -5.6259e-03,\n",
       "        -3.0364e-03,  1.1340e-01,  2.6560e-02,  1.6906e-03,  1.5023e-02,\n",
       "        -3.3559e-02,  1.6866e-02, -3.8061e-02,  7.5497e-03,  1.0009e-02,\n",
       "         2.3764e-02,  3.2676e-02, -8.4955e-03,  8.1304e-03, -6.6838e-02,\n",
       "         6.7926e-02, -3.7586e-02,  4.8547e-02,  8.3520e-02,  2.8259e-02,\n",
       "         6.7701e-03, -9.8969e-02,  1.6984e-02,  1.0438e-02, -4.1560e-02,\n",
       "         9.8130e-03, -3.7107e-03,  1.7649e-02, -2.9944e-02,  1.1517e-02,\n",
       "         2.1170e+00,  8.0320e-01, -2.7883e-02, -2.5582e-02, -2.4898e-02,\n",
       "         2.4827e-02, -3.8706e-01,  3.0922e-02,  5.3630e-02, -2.1325e-02,\n",
       "        -1.7633e-02,  1.4735e-02, -1.3573e-02,  9.3698e-02,  6.3691e-02,\n",
       "        -8.5184e-03, -3.6612e-02,  1.6177e-02, -4.6694e-01,  6.3364e-01,\n",
       "        -6.5773e-02,  5.9070e-02,  4.0830e-02,  3.7846e-02,  2.1491e-02,\n",
       "        -1.2280e-02,  2.3577e-02,  6.6729e-03,  2.7811e-02, -6.1744e-02,\n",
       "         3.0685e+00, -1.8870e-02,  1.8418e-02, -5.8950e-04, -3.9127e-02,\n",
       "        -1.2742e-02, -3.3234e-02,  4.1388e-03, -2.1280e-01,  3.9684e-02,\n",
       "         2.4262e-02,  1.7842e-01, -4.1642e-04, -5.5655e-02, -3.9652e-02,\n",
       "         3.6324e-02, -3.6439e-04,  3.3898e-02,  3.8444e-02,  3.1045e-02,\n",
       "         1.6215e-01,  1.9416e-02,  3.9075e-02, -7.9726e-02, -3.5166e-02,\n",
       "        -6.1327e-02, -9.5163e-02, -1.3732e-02, -2.8533e-02, -3.2208e-02,\n",
       "         5.1764e-02,  5.1271e-02,  1.3170e-02, -2.8410e-02,  1.9225e-02,\n",
       "        -6.1447e-02,  9.4512e-02,  1.7898e-02,  3.3249e-02, -8.4298e-02,\n",
       "        -1.6876e-03,  2.2678e-02,  3.8959e-02, -2.6315e-02,  2.6722e-02,\n",
       "        -4.1036e-02, -4.3836e-02,  7.2811e-03,  2.3958e-02,  2.5368e-02,\n",
       "         7.1948e-02, -2.8513e-01,  6.8697e-03, -3.6890e-02, -2.6570e-03,\n",
       "        -2.4861e-02, -2.1709e+00, -7.5174e-02, -3.7429e-02,  4.7887e-02,\n",
       "         2.3458e-02, -9.0910e-01,  2.8369e-01, -7.7521e-03,  2.6658e-02,\n",
       "        -5.5254e-01, -2.5945e-02, -2.3110e-02,  3.5670e-02, -3.8938e-02,\n",
       "         6.3535e-02,  2.6753e-02,  3.8857e-02, -7.1311e-03, -5.0512e-01,\n",
       "        -8.4332e-02, -8.9942e-03, -9.6385e-01, -1.9003e-01, -1.6461e-02,\n",
       "         1.5758e-02, -2.9520e-02, -3.7219e-02,  2.6768e-02,  7.8288e-02,\n",
       "         3.6225e-02, -2.4597e-02, -9.8554e-03,  1.7192e-02,  2.0096e-01,\n",
       "         4.9101e-02,  1.9576e-02, -4.3461e-03,  9.6674e-02, -2.8922e-02,\n",
       "         1.3446e-02, -1.8345e-02,  5.6624e-02,  1.7636e-02,  8.8476e-03,\n",
       "        -2.3983e-02, -4.1040e-02, -3.5934e-01,  1.3422e-02, -1.9759e-02,\n",
       "         5.2325e-02, -2.6986e-02,  5.4985e-03, -1.2450e-01, -1.7049e-03,\n",
       "         3.3860e-04, -4.5541e-02, -2.4598e-02, -1.8265e-02, -7.3527e-03,\n",
       "         5.3070e-02,  2.5912e-02, -4.8607e-02,  3.5768e-02, -2.2845e-02,\n",
       "        -3.4739e-01, -3.7481e-02, -8.0708e-03,  7.6596e-02, -4.2530e-02,\n",
       "         4.2420e-02,  4.8602e-02,  2.1992e-02,  1.1385e-01, -2.4760e-02,\n",
       "        -9.0741e-02,  1.1689e-02,  9.7670e-02,  6.2420e-02,  8.3420e-04,\n",
       "         5.7120e-02, -3.3638e-03,  4.3362e-02,  6.4554e-03,  4.3409e-01,\n",
       "        -2.8289e-02,  8.4534e-03,  1.6225e-01,  2.1655e-02,  5.4778e-02,\n",
       "         3.3466e-02,  1.0512e-02,  6.3881e-03, -1.7619e-02, -3.7032e-02,\n",
       "        -5.3927e-02, -2.1898e-02,  2.0860e-02,  4.2955e-01,  2.8156e-02,\n",
       "        -3.0496e-01, -5.0267e-02,  3.3228e-02, -1.9458e-03,  4.3604e-01,\n",
       "         1.2594e-02,  1.6785e-03,  3.2018e-02, -1.1435e-02,  3.0255e-02,\n",
       "        -2.8112e-02,  4.1541e-03, -2.9137e-02, -5.2214e-02, -7.5817e-02,\n",
       "        -5.0438e-03,  3.8585e-03,  9.9854e-02, -2.6922e-02,  9.8854e-03,\n",
       "        -7.0485e-03, -5.8754e-03,  2.7603e-02,  9.0747e-02, -8.6773e-01,\n",
       "        -2.5373e-02,  9.3951e-03,  4.5241e-03,  2.1289e-02, -1.7936e-02,\n",
       "        -3.6137e-02, -2.0088e-02, -1.9245e-02,  1.3789e-01,  3.2350e-02,\n",
       "         4.9784e-02,  7.1033e-03,  2.4016e-02, -4.6873e-04,  1.0043e-02,\n",
       "        -1.8570e-02, -1.9425e-01,  2.0531e-01,  4.4722e-02, -4.8765e-02,\n",
       "         3.0511e-02,  1.5151e-04,  1.1109e-02, -1.5149e+00,  2.5688e-02,\n",
       "         7.3238e-03,  4.4182e-02, -1.5863e-02, -6.9036e-02,  9.6258e-03,\n",
       "         2.1722e-02,  3.8190e-01,  1.3655e-02, -3.3429e-02,  3.3138e-02,\n",
       "        -1.7596e-02,  1.2354e-02, -9.8003e-02,  1.7169e-02,  3.9591e-02,\n",
       "         4.0094e-02, -3.8441e-02,  7.1837e-02, -3.8313e-03, -3.9395e-02,\n",
       "        -2.5580e-01, -4.1298e-02, -2.0359e-01, -9.5320e-01,  2.5642e-02,\n",
       "         9.2723e-03,  3.7339e-02,  3.6342e-02, -7.0528e-03, -6.5394e-02,\n",
       "         1.2265e-02,  1.0122e-02,  1.1549e-02, -1.8140e-02,  6.1482e-03,\n",
       "         1.7135e+00, -3.5519e-02, -5.0767e-02,  7.6745e-03,  2.9331e-02,\n",
       "         3.0430e-03, -3.1479e-02, -1.2568e-02, -2.1320e-02,  2.1707e-02,\n",
       "         1.8987e-02, -8.8883e-01,  1.2125e-02,  2.6438e-02,  2.0943e-01,\n",
       "         3.2970e-02,  6.6763e-02,  3.3256e-02, -3.2825e-02, -1.2005e-01,\n",
       "        -4.9361e-02,  5.1311e-02, -3.3480e-02,  9.5489e-03,  6.4813e-02,\n",
       "        -2.4486e-02, -2.7666e-01,  4.9428e-04,  4.4311e-02, -1.4237e-02,\n",
       "        -1.6418e-02,  1.8140e-02,  8.0819e-02,  1.3806e-02,  2.2750e-03,\n",
       "        -2.3704e-02,  3.5261e-02,  7.6599e-03,  3.8602e-02,  6.6556e-03,\n",
       "        -1.7804e-02,  1.3549e+00,  1.9752e-02, -3.4907e-02,  6.4030e-03,\n",
       "        -6.0518e-02,  6.0842e-02, -1.7444e-02,  1.0880e-01, -1.8750e-02,\n",
       "        -1.0402e-02, -6.6880e-03, -7.7606e-03, -2.2134e-02,  1.2712e-02,\n",
       "        -1.2865e-02, -3.5302e-02, -1.6899e-03,  6.8769e-03,  2.6735e-02,\n",
       "         3.6005e-02,  2.0541e-03, -4.8858e-02, -5.8962e-02, -4.0852e-02,\n",
       "        -1.1628e-02,  1.2221e-02, -1.7475e-02,  2.4019e-01, -2.5707e-02,\n",
       "         1.1639e-02, -7.5646e-03, -2.8997e-03, -1.0732e-02, -1.1266e-01,\n",
       "         1.8306e-01, -9.5816e-03,  9.0851e-02, -4.1114e-02, -4.6769e-02,\n",
       "        -1.4158e-02,  2.6344e-02,  6.5998e-01, -5.0159e-01,  6.4231e-02,\n",
       "         6.2835e-03, -3.3330e-02, -1.6098e+00,  3.8070e-02,  1.1026e-02,\n",
       "        -2.5894e-02,  6.1403e-03,  4.6618e-02,  2.7380e-02, -3.2654e-02,\n",
       "         2.3865e-02, -2.8636e-02,  2.9206e-03, -3.3496e-03,  2.7162e-02,\n",
       "         1.9196e-01, -6.2142e-02,  2.1860e-02,  2.7283e-01,  3.2564e-02,\n",
       "         9.9084e-03, -3.1970e-02,  3.6526e-02,  2.2737e-02, -1.8527e-01,\n",
       "        -8.3985e-03, -3.7638e-02,  2.2447e-02,  2.3149e-02,  4.9008e-03,\n",
       "        -4.9940e-01, -5.8577e-02,  1.3585e-02, -5.3179e-02,  8.9382e-03,\n",
       "        -2.9503e-02,  1.1848e-02, -6.9815e-03,  1.5438e-02, -6.4257e-02,\n",
       "        -8.2343e-02, -8.6457e-02, -7.3714e-02,  1.6764e-02,  6.2733e-02,\n",
       "         3.0075e+00,  2.3481e-03, -8.1105e-03,  3.6314e-02, -4.5381e+00,\n",
       "         3.0046e-02, -4.3590e-03,  1.9305e-03, -1.9914e-02, -1.3940e+00,\n",
       "         8.5825e-02,  1.5078e-03, -3.5813e-02, -2.5510e-02,  8.1946e-02,\n",
       "        -3.9880e-02, -4.3724e-03,  4.6706e-02, -7.5673e-02, -5.7956e-02,\n",
       "        -4.0163e-02,  8.6380e-03, -1.8892e-02,  1.2066e-02,  7.1348e-02,\n",
       "         3.0804e-02,  3.5332e-02, -3.0011e-02, -4.6266e-03, -3.0416e-01,\n",
       "         7.3280e-03, -3.6848e-01,  1.8192e-02,  2.1566e-02,  2.4931e-02,\n",
       "        -3.1073e-02,  9.5428e-05, -1.1928e-01, -1.5507e-03, -2.4268e-01,\n",
       "        -8.4555e-03, -1.3348e-02,  1.5365e-03,  6.2889e-03,  1.3201e-02,\n",
       "         1.5080e-02, -3.1683e-03,  6.8163e-02, -7.9179e-01, -4.6202e-03,\n",
       "         9.1074e-01, -1.6140e-01,  1.3297e-02,  6.4707e-03,  1.4513e+00,\n",
       "         5.0443e-02,  1.4084e-01,  1.5878e-02,  3.4399e-01, -8.5022e-03,\n",
       "        -5.8426e-03, -1.0584e-02,  5.1128e-02,  3.6588e-02,  3.8108e-02,\n",
       "        -4.0882e-02, -3.5466e-02, -1.0839e-02,  2.4481e-03, -2.0107e-01,\n",
       "         6.0284e-03,  4.9156e-02, -2.1477e-02, -2.4268e-03, -9.1521e-03,\n",
       "         3.4912e-01, -1.5139e-01, -7.5483e-02,  2.2206e-02, -6.1442e-03,\n",
       "        -6.3152e-03,  9.7740e-02,  1.3022e-02,  1.3445e-02,  2.6431e-02,\n",
       "        -3.6497e-02, -1.5883e+00, -6.3216e-02, -4.6221e-02, -2.8667e-02,\n",
       "         5.4453e-02, -5.2860e-02,  2.5602e-03, -8.4281e-03,  3.3671e-03,\n",
       "        -4.3044e-02,  2.8267e-02,  5.4490e-02])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a singular layer\n",
    "\n",
    "# We get a context length `n_ctx` of 1024 tokens with am embedding dimension of 768 per token.\n",
    "print(sd_hf[\"transformer.wpe.weight\"].shape)\n",
    "\n",
    "# The first element of the tensor is the embedding for the first token.\n",
    "sd_hf[\"transformer.wpe.weight\"][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash attention (Fa) [E14]\n",
    "\n",
    "Fa is a kernel fusion algorithm. Fa needs more flops than the regular procedure but it's way more mindful of the memory (reads and writes).<br>\n",
    "\n",
    "<img src=\"images/flash.png\" alt=\"Alt text\" width=\"800\"/><br><br>\n",
    "\n",
    "\n",
    "Our original code looks like this:\n",
    "\n",
    "<code>\n",
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))<br>\n",
    "att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf')<br>\n",
    "att = F.softmax(att, dim=-1)<br>\n",
    "y = att @ v\n",
    "</code>\n",
    "\n",
    "<br>\n",
    "\n",
    "Instead we are using:\n",
    "\n",
    "`F.scaled_dot_product_attention(q, k, v, is_causal=True)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D1\n",
    "\n",
    "Every head captures a different dimension. We split the embedding by the number of heads:\n",
    "\n",
    "1) `k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)`\n",
    "\n",
    "We need to do so as we concatonate the final attention matrices afterwards:\n",
    "\n",
    "2) `y = y.transpose(1, 2).contiguous().view(B, T, C)`\n",
    "\n",
    "We see in #2 that we transpose the tensor `y` from four to three dimensions. The transformation to three dimensions merges the `h` Attention matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/trans_head.jpg\" alt=\"Alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  6 83]\n",
      " [ 8  9 83]\n",
      " [13 15 72]\n",
      " [ 9 11 80]\n",
      " [17  9 74]\n",
      " [13 14 73]\n",
      " [10 13 77]\n",
      " [12  9 79]\n",
      " [12 12 76]\n",
      " [ 9  8 83]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "# Parameters:\n",
    "n = 100  # number of trials\n",
    "p = [0.1, 0.1, 0.8]  # probabilities of each outcome\n",
    "\n",
    "# Generate random samples\n",
    "samples = multinomial.rvs(n, p, size=10)  # Generate 10 samples\n",
    "\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "data = text[:1000]\n",
    "print(data[:100]) # Get the first 1k characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "We need to generate training examples: sentences.\n",
    "As we are using self-supervised learning we only need to create batches of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n",
      "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]])\n"
     ]
    }
   ],
   "source": [
    "# gpt2 tokenizer has a compression of 3:1\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])\n",
    "\n",
    "# We can convert the tokens to a tensor and reshape it to a 4x6 tensor.\n",
    "# Here we get for example 4 sequences with 6 tokens each.\n",
    "# The problem though is that the lost token has no 'label'\n",
    "buf = torch.tensor(tokens[:24])\n",
    "x = buf.view(4,6)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
      "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
      "          461,    11,  2740,    13,   198])\n",
      "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]])\n",
      "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
      "        [  597,  2252,    11,  3285,   502,  2740],\n",
      "        [   13,   198,   198,  3237,    25,   198],\n",
      "        [ 5248,   461,    11,  2740,    13,   198]])\n"
     ]
    }
   ],
   "source": [
    "buf = torch.tensor(tokens[:24+1])\n",
    "print(buf)\n",
    "x = buf[:-1].view(4,6)\n",
    "y = buf[1:].view(4,6) # y is basically x shifted by one token to the right\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `buf = torch.tensor(tokens[:B*T+1])`\n",
    "\n",
    "This only works with texts who are larger than B*T+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  198,   464,  3290,   357,  6090,   271,  5385,   271,   393,  1680,\n",
      "          271,   300,   929,   385,  5385,   271,     8,   318,   257, 26026,\n",
      "         3474, 45923,   286,   262, 17481,    13,   220,   198,  7583,  1444,\n",
      "          262,  5928,  3290,    11,   340,   373, 26026,  3474,   422,   281,\n",
      "        28881,  3265,   286, 18063,   396, 34973, 23214,   625,  1478,    11,\n",
      "          830]) \n",
      "\n",
      "tensor([[  198,   464,  3290,   357,  6090,   271,  5385,   271,   393,  1680],\n",
      "        [  271,   300,   929,   385,  5385,   271,     8,   318,   257, 26026],\n",
      "        [ 3474, 45923,   286,   262, 17481,    13,   220,   198,  7583,  1444],\n",
      "        [  262,  5928,  3290,    11,   340,   373, 26026,  3474,   422,   281],\n",
      "        [28881,  3265,   286, 18063,   396, 34973, 23214,   625,  1478,    11]]) \n",
      "\n",
      "tensor([[  464,  3290,   357,  6090,   271,  5385,   271,   393,  1680,   271],\n",
      "        [  300,   929,   385,  5385,   271,     8,   318,   257, 26026,  3474],\n",
      "        [45923,   286,   262, 17481,    13,   220,   198,  7583,  1444,   262],\n",
      "        [ 5928,  3290,    11,   340,   373, 26026,  3474,   422,   281, 28881],\n",
      "        [ 3265,   286, 18063,   396, 34973, 23214,   625,  1478,    11,   830]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "B = 5 # Number of batches\n",
    "T = 10 # Maximum sequence length\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "sentence = \"\"\"\n",
    "The dog (Canis familiaris or Canis lupus familiaris) is a domesticated descendant of the wolf.\n",
    "Also called the domestic dog, it was domesticated from an extinct population of Pleistocene wolves over 14,000 years ago.\n",
    "The dog was the first species to be domesticated by humans.\n",
    "Experts estimate that hunter-gatherers domesticated dogs more than 15,000 years ago, which was before the development of agriculture.\n",
    "Due to their long association with humans, dogs have expanded to a large number of domestic individuals and gained the ability to thrive on a starch-rich diet that would be inadequate for other canids.\n",
    "\"\"\"\n",
    "tokens = enc.encode(sentence)\n",
    "buf = torch.tensor(tokens[:B*T+1])\n",
    "x = buf[:-1].view(B,T)\n",
    "y = buf[1:].view(B,T)\n",
    "print(buf,\"\\n\")\n",
    "print(x, \"\\n\")\n",
    "print(y, \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "\n",
    "a = b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ\n",
    "\n",
    "### F1 - Block size / sequence length\n",
    "- Sequence length (T) is the length of the input sequence, which can be less than or equal to the block size.\n",
    "- The block size is the maximum length of input sequences that the model can process.\n",
    "\n",
    "### F2 - Batch optimization\n",
    "- Always use numbers to the power of two e.g. batch size of 16, 24, 32 etc.\n",
    "- This is most efficient for the GPU\n",
    "- Always max out the maximum batch size that fits on your GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
