{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e00414",
   "metadata": {},
   "source": [
    "# Run the exact GPT2\n",
    "\n",
    "Run GPT2 with the weights provided bz hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6834acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.skeleton_gpt2 import GPT\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ca6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soeren/code/gpt2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/soeren/code/gpt2/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "------\n",
      "\n",
      "This is the converted hugging face model:\n",
      "\n",
      "------ GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CasualSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='tanh')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_return_sequences = 1\n",
    "max_length = 100\n",
    "\n",
    "model = GPT.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88b0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(\"My name is Johann Sebastian Bach and I am a\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = tokens.to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c290fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n",
      "torch.Size([1, 11, 50257])\n",
      "torch.Size([1, 12, 50257])\n",
      "torch.Size([1, 13, 50257])\n",
      "torch.Size([1, 14, 50257])\n",
      "torch.Size([1, 15, 50257])\n",
      "torch.Size([1, 16, 50257])\n",
      "torch.Size([1, 17, 50257])\n",
      "torch.Size([1, 18, 50257])\n",
      "torch.Size([1, 19, 50257])\n",
      "torch.Size([1, 20, 50257])\n",
      "torch.Size([1, 21, 50257])\n",
      "torch.Size([1, 22, 50257])\n",
      "torch.Size([1, 23, 50257])\n",
      "torch.Size([1, 24, 50257])\n",
      "torch.Size([1, 25, 50257])\n",
      "torch.Size([1, 26, 50257])\n",
      "torch.Size([1, 27, 50257])\n",
      "torch.Size([1, 28, 50257])\n",
      "torch.Size([1, 29, 50257])\n",
      "torch.Size([1, 30, 50257])\n",
      "torch.Size([1, 31, 50257])\n",
      "torch.Size([1, 32, 50257])\n",
      "torch.Size([1, 33, 50257])\n",
      "torch.Size([1, 34, 50257])\n",
      "torch.Size([1, 35, 50257])\n",
      "torch.Size([1, 36, 50257])\n",
      "torch.Size([1, 37, 50257])\n",
      "torch.Size([1, 38, 50257])\n",
      "torch.Size([1, 39, 50257])\n",
      "torch.Size([1, 40, 50257])\n",
      "torch.Size([1, 41, 50257])\n",
      "torch.Size([1, 42, 50257])\n",
      "torch.Size([1, 43, 50257])\n",
      "torch.Size([1, 44, 50257])\n",
      "torch.Size([1, 45, 50257])\n",
      "torch.Size([1, 46, 50257])\n",
      "torch.Size([1, 47, 50257])\n",
      "torch.Size([1, 48, 50257])\n",
      "torch.Size([1, 49, 50257])\n",
      "torch.Size([1, 50, 50257])\n",
      "torch.Size([1, 51, 50257])\n",
      "torch.Size([1, 52, 50257])\n",
      "torch.Size([1, 53, 50257])\n",
      "torch.Size([1, 54, 50257])\n",
      "torch.Size([1, 55, 50257])\n",
      "torch.Size([1, 56, 50257])\n",
      "torch.Size([1, 57, 50257])\n",
      "torch.Size([1, 58, 50257])\n",
      "torch.Size([1, 59, 50257])\n",
      "torch.Size([1, 60, 50257])\n",
      "torch.Size([1, 61, 50257])\n",
      "torch.Size([1, 62, 50257])\n",
      "torch.Size([1, 63, 50257])\n",
      "torch.Size([1, 64, 50257])\n",
      "torch.Size([1, 65, 50257])\n",
      "torch.Size([1, 66, 50257])\n",
      "torch.Size([1, 67, 50257])\n",
      "torch.Size([1, 68, 50257])\n",
      "torch.Size([1, 69, 50257])\n",
      "torch.Size([1, 70, 50257])\n",
      "torch.Size([1, 71, 50257])\n",
      "torch.Size([1, 72, 50257])\n",
      "torch.Size([1, 73, 50257])\n",
      "torch.Size([1, 74, 50257])\n",
      "torch.Size([1, 75, 50257])\n",
      "torch.Size([1, 76, 50257])\n",
      "torch.Size([1, 77, 50257])\n",
      "torch.Size([1, 78, 50257])\n",
      "torch.Size([1, 79, 50257])\n",
      "torch.Size([1, 80, 50257])\n",
      "torch.Size([1, 81, 50257])\n",
      "torch.Size([1, 82, 50257])\n",
      "torch.Size([1, 83, 50257])\n",
      "torch.Size([1, 84, 50257])\n",
      "torch.Size([1, 85, 50257])\n",
      "torch.Size([1, 86, 50257])\n",
      "torch.Size([1, 87, 50257])\n",
      "torch.Size([1, 88, 50257])\n",
      "torch.Size([1, 89, 50257])\n",
      "torch.Size([1, 90, 50257])\n",
      "torch.Size([1, 91, 50257])\n",
      "torch.Size([1, 92, 50257])\n",
      "torch.Size([1, 93, 50257])\n",
      "torch.Size([1, 94, 50257])\n",
      "torch.Size([1, 95, 50257])\n",
      "torch.Size([1, 96, 50257])\n",
      "torch.Size([1, 97, 50257])\n",
      "torch.Size([1, 98, 50257])\n",
      "torch.Size([1, 99, 50257])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad(): # we are not training the model (no gradient calculation), only using it to generate text\n",
    "        # We pass the current sequence x (tensor of token IDs) to the model\n",
    "        # The model outputs the logits, which represent the unnormalized probabilities of the next token\n",
    "        logits, loss = model(x)\n",
    "        # We start out with a shape of 8 as our input tensor x contains 8 token IDs\n",
    "        # After each iteration, we add a new token to the sequence, so the shape of x grows by 1\n",
    "        print(logits.shape)\n",
    "        # We extract the logits for the last token in the sequence (-1)\n",
    "        # This is because we only care about predicting the next token based on the current context\n",
    "        # The model's output logits has a shape of (batch_size, sequence_length, vocab_size).\n",
    "        logits = logits[:, -1, :]\n",
    "        # We apply a softmax function to the logits of the last token\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # This is a key step for sampling-based generation. Instead of simply picking the single most probable token (which can lead to repetitive and uninteresting text), the code uses torch.topk to get the 50 most likely next tokens and their corresponding probabilities.\n",
    "        # This reduces the sampling space to a manageable size and focuses on the most relevant options.\n",
    "        # along the last dimension (vocabulary dimension).\n",
    "        # this is an ordered tensor with the highest probabilities at the beginning\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # E[11]\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        # We use torch.gather to extract the actual token ID from the top 50 indices (topk_indices) based on the sampled index (ix).\n",
    "        # xcol becomes a one-dimensional tensor containing the sampled token ID.\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        # We concatenate the sampled token ID to the current sequence x.\n",
    "        x = torch.cat((x, xcol), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de9048f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> My name is Johann Sebastian Bach and I am a composer, writer, educator, performer. I specialize in composing the most important works of music and have been practicing my piano since 2004.\n",
      "\n",
      "I am currently in the process of mastering my first piece \"Beethoven's Symphony in Three Decades\" since it became a part of the \"Moral Piano\". For this I have been teaching myself new techniques and ideas.\n",
      "\n",
      "In my research I've found that \"The Piano Concerto\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    # Here we decode the token IDs back to text.\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff96494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
