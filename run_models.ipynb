{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e00414",
   "metadata": {},
   "source": [
    "# Run the exact GPT2\n",
    "\n",
    "Run GPT2 with the weights provided bz hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6834acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.skeleton_gpt2 import GPT\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86ca6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Using device: mps ---\n",
      "\n",
      "\n",
      "loading weights from pretrained gpt: gpt2\n",
      "------\n",
      "\n",
      "This is the converted hugging face model:\n",
      "\n",
      "------ GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CasualSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='tanh')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CasualSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper_functions import device_info\n",
    "device = device_info()\n",
    "\n",
    "num_return_sequences = 1\n",
    "max_length = 100\n",
    "\n",
    "model = GPT.from_pretrained(\"gpt2\")\n",
    "# Run this cell to run the untrained model.\n",
    "# from model.config import GPTConfig\n",
    "# model = GPT(GPTConfig())\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a88b0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(\"My name is Johann Sebastian Bach and I am a\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c290fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n",
      "torch.Size([1, 11, 50257])\n",
      "torch.Size([1, 12, 50257])\n",
      "torch.Size([1, 13, 50257])\n",
      "torch.Size([1, 14, 50257])\n",
      "torch.Size([1, 15, 50257])\n",
      "torch.Size([1, 16, 50257])\n",
      "torch.Size([1, 17, 50257])\n",
      "torch.Size([1, 18, 50257])\n",
      "torch.Size([1, 19, 50257])\n",
      "torch.Size([1, 20, 50257])\n",
      "torch.Size([1, 21, 50257])\n",
      "torch.Size([1, 22, 50257])\n",
      "torch.Size([1, 23, 50257])\n",
      "torch.Size([1, 24, 50257])\n",
      "torch.Size([1, 25, 50257])\n",
      "torch.Size([1, 26, 50257])\n",
      "torch.Size([1, 27, 50257])\n",
      "torch.Size([1, 28, 50257])\n",
      "torch.Size([1, 29, 50257])\n",
      "torch.Size([1, 30, 50257])\n",
      "torch.Size([1, 31, 50257])\n",
      "torch.Size([1, 32, 50257])\n",
      "torch.Size([1, 33, 50257])\n",
      "torch.Size([1, 34, 50257])\n",
      "torch.Size([1, 35, 50257])\n",
      "torch.Size([1, 36, 50257])\n",
      "torch.Size([1, 37, 50257])\n",
      "torch.Size([1, 38, 50257])\n",
      "torch.Size([1, 39, 50257])\n",
      "torch.Size([1, 40, 50257])\n",
      "torch.Size([1, 41, 50257])\n",
      "torch.Size([1, 42, 50257])\n",
      "torch.Size([1, 43, 50257])\n",
      "torch.Size([1, 44, 50257])\n",
      "torch.Size([1, 45, 50257])\n",
      "torch.Size([1, 46, 50257])\n",
      "torch.Size([1, 47, 50257])\n",
      "torch.Size([1, 48, 50257])\n",
      "torch.Size([1, 49, 50257])\n",
      "torch.Size([1, 50, 50257])\n",
      "torch.Size([1, 51, 50257])\n",
      "torch.Size([1, 52, 50257])\n",
      "torch.Size([1, 53, 50257])\n",
      "torch.Size([1, 54, 50257])\n",
      "torch.Size([1, 55, 50257])\n",
      "torch.Size([1, 56, 50257])\n",
      "torch.Size([1, 57, 50257])\n",
      "torch.Size([1, 58, 50257])\n",
      "torch.Size([1, 59, 50257])\n",
      "torch.Size([1, 60, 50257])\n",
      "torch.Size([1, 61, 50257])\n",
      "torch.Size([1, 62, 50257])\n",
      "torch.Size([1, 63, 50257])\n",
      "torch.Size([1, 64, 50257])\n",
      "torch.Size([1, 65, 50257])\n",
      "torch.Size([1, 66, 50257])\n",
      "torch.Size([1, 67, 50257])\n",
      "torch.Size([1, 68, 50257])\n",
      "torch.Size([1, 69, 50257])\n",
      "torch.Size([1, 70, 50257])\n",
      "torch.Size([1, 71, 50257])\n",
      "torch.Size([1, 72, 50257])\n",
      "torch.Size([1, 73, 50257])\n",
      "torch.Size([1, 74, 50257])\n",
      "torch.Size([1, 75, 50257])\n",
      "torch.Size([1, 76, 50257])\n",
      "torch.Size([1, 77, 50257])\n",
      "torch.Size([1, 78, 50257])\n",
      "torch.Size([1, 79, 50257])\n",
      "torch.Size([1, 80, 50257])\n",
      "torch.Size([1, 81, 50257])\n",
      "torch.Size([1, 82, 50257])\n",
      "torch.Size([1, 83, 50257])\n",
      "torch.Size([1, 84, 50257])\n",
      "torch.Size([1, 85, 50257])\n",
      "torch.Size([1, 86, 50257])\n",
      "torch.Size([1, 87, 50257])\n",
      "torch.Size([1, 88, 50257])\n",
      "torch.Size([1, 89, 50257])\n",
      "torch.Size([1, 90, 50257])\n",
      "torch.Size([1, 91, 50257])\n",
      "torch.Size([1, 92, 50257])\n",
      "torch.Size([1, 93, 50257])\n",
      "torch.Size([1, 94, 50257])\n",
      "torch.Size([1, 95, 50257])\n",
      "torch.Size([1, 96, 50257])\n",
      "torch.Size([1, 97, 50257])\n",
      "torch.Size([1, 98, 50257])\n",
      "torch.Size([1, 99, 50257])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad(): # we are not training the model (no gradient calculation), only using it to generate text\n",
    "        # We pass the current sequence x (tensor of token IDs) to the model\n",
    "        # The model outputs the logits, which represent the unnormalized probabilities of the next token\n",
    "        logits, loss = model(x)\n",
    "        # We start out with a shape of 8 as our input tensor x contains 8 token IDs\n",
    "        # After each iteration, we add a new token to the sequence, so the shape of x grows by 1\n",
    "        print(logits.shape)\n",
    "        # We extract the logits for the last token in the sequence (-1)\n",
    "        # This is because we only care about predicting the next token based on the current context\n",
    "        # The model's output logits has a shape of (batch_size, sequence_length, vocab_size).\n",
    "        logits = logits[:, -1, :]\n",
    "        # We apply a softmax function to the logits of the last token\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # This is a key step for sampling-based generation. Instead of simply picking the single most probable token (which can lead to repetitive and uninteresting text), the code uses torch.topk to get the 50 most likely next tokens and their corresponding probabilities.\n",
    "        # This reduces the sampling space to a manageable size and focuses on the most relevant options.\n",
    "        # along the last dimension (vocabulary dimension).\n",
    "        # this is an ordered tensor with the highest probabilities at the beginning\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # E[11]\n",
    "        ix = torch.multinomial(topk_probs, 1)\n",
    "        # We use torch.gather to extract the actual token ID from the top 50 indices (topk_indices) based on the sampled index (ix).\n",
    "        # xcol becomes a one-dimensional tensor containing the sampled token ID.\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        # We concatenate the sampled token ID to the current sequence x.\n",
    "        x = torch.cat((x, xcol), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de9048f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> My name is Johann Sebastian Bach and I am a music teacher and music writer. I have spent my life learning how to make music more and more interesting. My goal is to give these two great German composers a place in the modern music world. The next project is The World's First International Music Symposium.\" https://www.youtube.com/watch?v=HXnWdVXzZg4\n",
      "\n",
      "SafÃ© Music Symposium\n",
      "\n",
      "October 4-7\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    # Here we decode the token IDs back to text.\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc3f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from 'input.txt':\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soeren/code/gpt2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens from 'input.txt'\n",
      "Dataset has 1320 batches per epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<model.dataloader.DataloaderLite at 0x1058c2d00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.dataloader import DataloaderLite\n",
    "\n",
    "data_loader = DataloaderLite(B=32, T=8, print_data=True)\n",
    "data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783212e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
